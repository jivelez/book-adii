[["rlm.html", "Capítulo 3 Regresión Lineal Múltiple 3.1 Formulación básica del modelo de RLM 3.2 Propiedades de los estimadores de \\({\\mathbf{\\beta}}\\) 3.3 Estimación de \\(\\sigma^2\\) 3.4 Inferencia para \\(\\mathbf{\\beta}\\) 3.5 Inferencia para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) 3.6 Análisis de Residuales 3.7 Análisis de Multicolinealidad 3.8 Selección de Modelos", " Capítulo 3 Regresión Lineal Múltiple Como se mencionó en el Capítulo 2, los modelos de regresión lineal pueden utilizarse para predecir futuros valores de una variable respuesta continua a partir de valores específicos de las variables controlables del proceso. En la práctica, pueden existir múltiples variables controlables en un proceso de producción o de servicios. Por ejemplo, en un proceso de pintura electrostática, puede ser de interés determinar el espesor de la capa de pintura (variable respuesta \\(y\\), en micrones) con la que se recubre una lámina de área determinada, a partir de valores conocidos de la presión de aire (variable \\(x_1\\) en psi) y la velocidad de la banda transportadora (variable \\(x_2\\) en m/s) en la que se desplaza dicha lámina. En este caso, el interés es: Determinar la magnitud de la influencia de las variables \\(x_1\\) y \\(x_2\\) sobre el espesor de capa esperado; construir una función \\(f(x_1,x_2)\\) que permita predecir el espesor de capa esperado; y construir intervalos de confianza y predicción para dicho valor. Si la variable respuesta \\(y\\) es continua y aproximadamente simétrica, podemos desarrollar 1, 2 y 3 a partir de la estimación de un modelo de regresión lineal. Puesto que el número de variables controlables es \\(k&gt;1\\), una posibilidad es utilizar el modelo de Regresión Lineal Múltiple (RLM). El modelo de RLM es una extensión del modelo de RLS cuando se tiene más de una variable controlable. 3.1 Formulación básica del modelo de RLM Matemáticamente, el modelo de RLM puede expresarse como: \\[\\begin{align} \\label{mod1} y_i &amp;= \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_1X_{ki} + \\epsilon_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] Este modelo es equivalente a \\[\\begin{align} \\label{mod2} \\mathbf{y} &amp;= \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon} \\end{align}\\] donde \\(\\mathbf{y} = (y_1,y_2,\\ldots,y_n)\\) es el vector respuesta, \\(\\mathbf{X} = (\\mathbf{1}, \\mathbf{x}_1, \\mathbf{x}_2,\\ldots,\\mathbf{x}_k)_{n\\times p}\\) es la matriz de diseño y \\(\\mathbf{\\epsilon} = (\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n)\\) es el error aleatorio. 3.1.1 Estimación Similar a RLS, la estimación del modelo de RLM se realiza utilizando el método de mínimos cuadrados ordinarios (MCO). A partir de una muestra aleatoria de tamaño \\(n\\) del proceso de producción, los datos se registran en una estructura rectangular similar a: Figura 3.1: Estructura de datos en RLM. De esta forma, se tienen \\(n\\) unidades experimentales para cada una de estas se determina el valor de la variable respuesta \\(y_i\\) para condiciones fijas \\(\\mathbf{X}_i\\). Por ejemplo, para la quinta unidad experimental, se obtuvo el valor \\(y_6\\) cuando las variables controlables tomaron los valores fijos \\((x_{1,6}, x_{2,6}, \\ldots, x_{k,6})\\). Al igual que en RLS, la estimación del modelo de RLM realiza utilizando minimos cuadrados La idea fundamental consiste en minimizar \\[\\begin{eqnarray*}\\label{L} L &amp;=&amp;\\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_{1,i} -\\beta_2X_{2,i} - \\ldots -\\beta_kX_{k,i} )^2. \\end{eqnarray*}\\] Los estimadores de mínimos cuadrados deben satisfacer las siguientes dos condiciones fundamentales: \\[\\begin{eqnarray*} \\frac{\\partial L}{\\partial \\beta_0} | _{\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots \\hat{\\beta}_k} &amp;=&amp; -2\\sum_{i=1}^n \\left(y_i-\\hat{\\beta}_0-\\sum_{j=1}^k{\\hat{\\beta}_jx_{ij}}\\right) = 0 \\\\ \\frac{\\partial L}{\\partial \\beta_j} | _{\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots \\hat{\\beta}_k} &amp;=&amp; -2\\sum_{i=1}^n \\left(y_i-\\hat{\\beta}_0-\\sum_{j=1}^k{\\hat{\\beta}_jx_{ij}}\\right)x_{ij} = 0 \\end{eqnarray*}\\] La solución al sistema de ecuaciones de condiciones fundamentales da origen al sistema de ecuaciones normales de mínimos cuadrados dado por Figura 3.2: Ecuaciones normales de mínimos cuadrados. Las solución de estas ecuaciones permite determinar \\(\\hat{\\mathbf{\\beta}}\\). Es fácil llegar a que el vector de coeficientes estimado para el modelo de RLM puede obtenerse como \\[ \\hat{\\mathbf{\\beta}} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime \\mathbf{y} \\] Finalmente, el modelo estimado es \\[\\hat{y}_i = \\hat{\\beta}_0+\\sum_{j=1}^k\\hat{\\beta}_jx_{ij},\\] que, matricialmente, puede representarse como \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\mathbf{\\beta}}\\] A partir del modelo ajustado, un valor específico \\(y_i\\) puede calculase como: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2+\\cdots+\\hat{\\beta}_kx_k\\] Como ilustración, consideremos los siguientes datos provenientes de un proceso de pintura electrostática: ## lectura de datos url &lt;- &#39;https://www.dropbox.com/s/st5xk1prkxg1pj4/datalab2.txt?dl=1&#39; datos &lt;- read.table(url, header = TRUE) head(datos) ## y x1 x2 ## 1 91.88308 18.722091 6 ## 2 94.37544 19.056131 3 ## 3 74.63026 9.292093 3 ## 4 93.66348 17.456714 4 ## 5 76.84981 14.626183 9 ## 6 76.05198 12.786439 9 En este caso, la variable respuesta \\(y\\) representa el espesor de la capa de pintura en micrones, \\(x_1\\) es la presión de aire en psi y \\(x_2\\) es la velocidad de la banda transportadora. Una forma de comenzar a establecer si existe relación lineal entre \\(y\\) y las variables \\(x_1\\) y \\(x_2\\) es a través de una red, como se muestra a continuación: ## matriz de correlación require(IsingSampler) require(qgraph) corMat &lt;- cor(datos) par(mfrow = c(1, 1), mar = c(0.1, 0.1, 0.1, 0.1)) qgraph(corMat, graph = &quot;cor&quot;, layout = &quot;spring&quot;, sampleSize = nrow(datos), legend.cex = 1, alpha = 0.05) Figura 3.3: Red de correlación para \\(x_1\\), \\(x_2\\) y \\(y\\). Correlaciones positivas se muestran en verde, y las negativas en rojo. O utilizando una matriz de dispersión: ## pairs plot panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;) on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- cor(x, y) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste0(prefix, txt) text(0.5, 0.5, txt, cex = 1.5) } pairs(datos, lower.panel = panel.smooth, upper.panel = panel.cor, las = 1) Figura 3.4: Matriz de dispersión para \\(x_1\\), \\(x_2\\) y \\(y\\). Este último gráfico indica que \\(x_1\\) e \\(y\\) están linealmente relacionados y la correlación es \\(\\hat\\rho = 0.89\\), mientras la correlación de \\(y\\) y \\(x_2\\) es \\(\\hat\\rho = -0.32\\). Para otras posibilidades relacionadas con correlogramas, se recomienda revisar las alternativas en R Graph Gallery. Para estimar el modelo de RLM, utilizamos la función lm como se muestra a continuación: ## ajuste del modelo de RLM fit &lt;- lm(y ~ x1 + x2, data = datos) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0571 -1.6610 -0.1362 1.5409 8.7008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.28483 1.10350 60.068 &lt; 2e-16 *** ## x1 1.48275 0.06144 24.132 &lt; 2e-16 *** ## x2 -1.05521 0.13660 -7.725 1.03e-11 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.767 on 97 degrees of freedom ## Multiple R-squared: 0.8716, Adjusted R-squared: 0.8689 ## F-statistic: 329.1 on 2 and 97 DF, p-value: &lt; 2.2e-16 A partir de estos resultados podemos realizar las pruebas de significancia global y marginales tal y como se mostró en el Capítulo 2. 3.2 Propiedades de los estimadores de \\({\\mathbf{\\beta}}\\) Cuando estimamos \\(\\hat{\\mathbf{\\beta}}\\), los cálculos están basados en los resultados obtenidos al tomar una muestra aleatoria de tamaño \\(n\\). Como consecuencia, el valor de los estimadores \\(\\mathbf{\\hat{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k)\\) cambian si cambiamos la muestra. Desde el punto de vista formal, los estimadores \\(\\mathbf{\\hat{\\beta}}\\) cumplen con las siguientes propiedades: Los estimadores \\(\\mathbf{\\hat\\beta}\\) son insesgados. Esta propiedad implica que, al aumentar \\(n\\), el valor de los estimadores de \\(\\mathbf{\\beta}\\) se aproximan a los verdaderos valores de los parámetros. Matemáticamente se tiene que: \\[\\begin{eqnarray} E[\\mathbf{\\hat\\beta}] &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{y}]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime(\\mathbf{X\\beta} + \\mathbf{\\epsilon})]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{X\\beta} + (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{\\epsilon}]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{X\\beta}] + E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{\\epsilon}]\\\\\\nonumber &amp;=&amp; E[\\mathbf\\beta] + \\mathbf{0} = \\mathbf{\\beta}\\nonumber \\end{eqnarray}\\] La varianza de \\(\\hat{\\beta}_j\\) y la covarianza entre \\(\\hat\\beta_{i}\\) y \\(\\hat\\beta_{j}\\) están dadas por: \\[\\begin{eqnarray*} V(\\hat\\beta_{j}) &amp;=&amp; \\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj} \\hspace{0.5cm} 0,1,2,\\ldots,p;\\\\ \\text{cov}(\\hat\\beta_{i}, \\hat\\beta_{j}) &amp;=&amp; \\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{ij} \\hspace{0.5cm} i\\neq j. \\end{eqnarray*}\\] Ahora, a partir de \\(E[\\hat{\\beta}_j]\\) y \\(V(\\hat\\beta_{j})\\), es posible hacer inferencia para el parámetro \\(\\beta_j\\), \\(j=1,2,\\ldots,k.\\) Sin embargo, observe que \\(V(\\hat\\beta_{j})\\) depende de \\(\\sigma^2\\), la varianza del modelo de RLM, que se estima a través del MSE. Los coeficientes del modelo de RLM ajustado para los datos provenientes del proceso de pintura electrostática se obtienen haciendo: ## coeficientes estimados coefficients(fit) ## (Intercept) x1 x2 ## 66.284831 1.482753 -1.055206 A partir de estos coeficientes, el modelo ajustado será \\[\\hat{y} = 66.285 + 1.483x_1 - 1.055x_2\\] Note que esta es la ecuación de un plano en el espacio \\((x_1, x_2, y)\\). Los coeficientes estimados del modelo de RLM se interpretan en términos de una derivada parcial. En particular, si incrementamos \\(x_1\\) en una unidad y mantenemos constante \\(x_2\\), esperaríamos que el espesor de la capa de pintura aumentara, en promedio, \\(\\hat{\\beta}_1 = 1.483\\) micrones. Por otro lado, si incrementamos \\(x_2\\) en una unidad y mantenemos constante \\(x_1\\), se espera que el espesor de la capa de pintura disminuya, en promedio, \\(\\hat{\\beta}_2 = -1.055\\) micrones. 3.3 Estimación de \\(\\sigma^2\\) Similar a como se observó en RLS, en RLM, también se cumple la misma relación que en RLS en cuanto que \\[SST = SSR + SSE \\] Adicionalmente, \\(\\mathbf{\\hat{\\beta}}\\), se tiene que \\[\\begin{eqnarray*}\\label{L2} L &amp;=&amp;\\sum_{i=1}^n\\hat{\\epsilon}_i^2 = \\sum_{i=1}^n(Y_i-\\hat{Y}_i^2)^2 \\\\ &amp;=&amp;\\sum_{i=1}^n(Y_i-\\hat{\\beta}_0-\\hat{\\beta}_1X_{1,i} -\\hat{\\beta}_2X_{2,i} - \\cdots -\\hat{\\beta}_kX_{k,i})^2 \\\\ &amp;=&amp; SSE \\end{eqnarray*}\\] Por lo tanto, \\[\\hat{\\sigma}^2 = \\frac{SSE}{n-p} = MSE\\] donde \\(p = k+1\\) es el número de coeficientes del modelo ajustado. Este resulado indica que la varianza de los errores, también conocida como la varianza del modelo, puede estimarse utilizando el MSE. El MSE se obtiene de la tabla ANOVA que tiene la siguiente forma: Figura 3.5: Tabla ANOVA en RLM. En el caso del modelo de RLM para el proceso de pintura electrostática, podemos obtener el valor de \\(\\hat{\\sigma}^2\\) haciendo: ## estimación de sigma^2 summary(fit)$sigma^2 ## [1] 7.65454 Por lo tanto, el modelo estimado será: \\[ \\begin{eqnarray} \\hat{y} &amp;=&amp; 66.285 + 1.483x_1 - 1.055x_2 \\\\\\nonumber \\epsilon &amp;\\sim&amp; N(0, \\sigma^2) \\\\\\nonumber \\hat{\\sigma}^2 &amp;=&amp; 7.655.\\nonumber \\end{eqnarray} \\] Recordemos que, adicional al MSE, a partir de la tabla ANOVA es posible calcular el porcentaje de variabilidad de la respuesta explicado por el modelo de RLM, también conocido como coeficiente de determinación o, simplemente, como \\(R^2\\): \\[R^2 = SSR/SST = 1 - SSE/SST\\] Puesto que \\(R^2\\) incrementa a medida que el número de variables aumenta, se recomienda utilizar el \\(R^2\\) ajustado en RLM: \\[R^2_{\\text{ajustado}} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\\] La interpretación de esta medida de desempeño es similar a la interpretación de \\(R^2\\) discutida en el Capítulo 2. El ejemplo de la pintura electrostática, \\(R^2_{\\text{adj}}=0.8689\\), por lo que podemos afirmar que incluir las variables \\(x_1\\) y \\(x_2\\) en un modelo de regresión permite explica cerca del 87% de la variabilidad de la respuesta. 3.4 Inferencia para \\(\\mathbf{\\beta}\\) Uno de los propósitos de la inferencia estadística es determinar el valor de los verdaderos parámetros de una población a partir de los resultados obtenidos en una muestra. En este caso, los parámetros poblacionales son \\(\\mathbf{{\\beta}} = (\\beta_0, \\beta_1, \\ldots, \\beta_k)\\), además de \\(\\sigma^2\\). Con los valores muestrales, podemos construir pruebas de hipótesis de dos tipos para los parámetros del modelo de RLM: la prueba de significancia total y las pruebas de significancia marginal. 3.4.1 Prueba de significancia global Esta prueba se utiliza para determinar la significancia total del modelo, es decir, para detemrinar si incluir las variables controlables en el modelo de regresión es mejor que no incluirlas para explicar la respuesta \\(Y\\). La idea fundamental es determinar si, en la población, \\[\\begin{eqnarray} H_0&amp;:&amp; \\beta_1=\\beta_2=\\cdots\\beta_k=0 \\\\\\nonumber H_1&amp;:&amp; \\text{Al menos un $\\beta_j \\neq 0$}\\nonumber \\end{eqnarray}\\] Este procedimiento de prueba de hipótesis se realiza a través de la tabla de ANOVA utilizando el estadístico \\(F\\) dado por \\[F_0 = \\frac{SSR/k}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F_{k, n-p}\\] Rechazamos \\(H_0: \\beta_1=\\beta_2=\\cdots\\beta_k=0\\) si \\(F_0 &gt; F_{\\alpha,k,n-p}\\), donde \\(\\alpha\\in(0,1)\\) es un nivel de significancia predeterminado. Cuando esto ocurre, concluimos que al menos un \\(\\beta_j\\) es estad'isticamente significativo al \\(100(1-\\alpha)\\%\\). Observe que en la prueba de significancia global del modelo, \\(F_{\\text{calc}} = 329.1\\) y \\(p &lt; 2.2\\times 10^{-16}\\). Esto indica que tener este modelo de RLM para explicar el espesor de la capa de pintura es mejor que no tenerlo. 3.4.2 Prueba de significancia marginal Esta prueba se realiza si rechazamos la prueba de significancia global. Lo que intentamos hacer es determinar si, a nivel poblacional, los coeficientes asociados a cada \\(x_j\\) son o diferentes de cero. Esto es equivalente a probar: \\[\\begin{eqnarray*} H_0&amp;:&amp; \\beta_j=0 \\\\\\nonumber H_1&amp;:&amp; \\beta_j \\neq 0\\nonumber \\end{eqnarray*}\\] Para \\(j\\) fijo, el estadístico de prueba es \\[t_j = \\frac{\\hat{\\beta}_j - 0}{\\text{s.e.}(\\hat\\beta_j)} = \\frac{\\hat{\\beta}_j}{\\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}}}\\sim t_{n-p}\\] Por lo tanto, rechazamos \\(H_0\\) con un nivel de significancia de \\(100\\times(1-\\alpha)\\%\\) si \\(|t_j| &gt; t_{\\alpha/2, n-p}\\). Para el problema de la pintura electrostática, tendríamos: ## ajuste del modelo de RLM summary(fit)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.284831 1.10349995 60.067815 1.529216e-78 ## x1 1.482753 0.06144364 24.131927 8.750958e-43 ## x2 -1.055206 0.13659758 -7.724922 1.026117e-11 Según estos resultados, todas las pruebas marginales del tipo \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\), el valor \\(p\\) es \\(&lt; 0.05\\). Por lo tanto, la magnitud de la influencia de cada variable controlable sobre \\(E[y|x_1, x_2]\\) es estadísticamente diferente de cero a nivel poblacional. En otras palabras, controlando las variables \\(x_1\\) y \\(x_2\\) en el proceso, permitiría modificar satisfactoriamente el espesor de la capa de pintura. 3.4.3 Intervalos de confianza para \\(\\beta_j\\) Otra forma de realizar inferencia para \\(\\mathbf{\\beta}\\) es a través de la construcción de intervalos de confianza del \\(100\\times(1-\\alpha)100\\%\\). Es fácil mostrar que, para \\(j\\) fijo, \\[\\begin{equation}\\label{eq:icbeta} \\beta_j\\in \\left( \\hat{\\beta}_j - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}}, \\hat{\\beta}_j + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}} \\right) \\end{equation}\\] Otra alternativa para construir intervalos de confianza es vía bootstrap o likelihood profiling. Finalmente concluimos, con un nivel de confianza del \\(100\\times(1-\\alpha)\\%\\), que \\(\\beta_j\\) está en el intervalo anterior. Uan forma de construir los intervalos de confianza para los coeficientes del modelo es utilizando la función confint.default: ## intervalos de confianza del 95% para los coeficientes confint.default(fit) ## 2.5 % 97.5 % ## (Intercept) 64.122011 68.4476513 ## x1 1.362326 1.6031807 ## x2 -1.322932 -0.7874793 Por lo tanto, con una confianza del 95%, \\[\\begin{align} \\beta_0 &amp;\\in(64.12, 68.45) \\\\ \\beta_1 &amp;\\in(1.36, 1.60) \\\\ \\beta_2 &amp;\\in(-1.323, -0.787). \\end{align}\\] 3.5 Inferencia para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) El modelo de RLM ajustado puede utilizarse para predecir \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) sólo si se validan todos los supuestos. Para más detalles ver Análisis de Residuales. 3.5.1 Intervalos de confianza para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) A partir del modelo ajustado y para valores fijos de las variables controlables, digamos \\(\\mathbf{x}_0\\), se tiene que \\[\\begin{eqnarray} \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0} &amp;=&amp; \\hat{E[\\mathbf{Y} | \\mathbf{x}_0]} = \\mathbf{x}_0^\\prime\\hat{\\mathbf{\\beta}} \\\\\\nonumber V[\\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}] &amp;=&amp; \\hat{\\sigma}^2\\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0 \\end{eqnarray}\\] Finalmente, el intervalo de confianza del \\(100\\times(1-\\alpha)\\%\\) puede calcularse como \\[\\begin{eqnarray} \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0} \\pm t_{\\alpha/2,n-p}\\sqrt{\\hat{\\sigma}^2\\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0} \\end{eqnarray}\\] Supongamos que queremos determinar el espesor de la capa de pintura para las condiciones \\(\\mathbf{x}_0 = (10, 8)\\), es decir, cuando \\(x_1 = 10\\) y \\(x_2 = 8\\). En R procedemos de la siguiente manera: ## cálculo de E[y|x_1 = 10, x_2 = 8] predict(fit, newdata = data.frame(x1 = 10, x2 = 8)) ## 1 ## 72.67072 Si trabajamos bajo las condiciones \\(\\mathbf{x}_0\\), se espera que, en promedio, el espesor de la capa de pintura sea 72.67 micrones. Para otros argumentos y opciones, se sugiere al lector escribir ?predict.lm en la consola de R. Ahora, si es de interés calcular un intervalo de confianza del 95%, agregamos el argumento interval = 'confidence' a la instrucción anterior: ## cálculo de E[y|x_1 = 10, x_2 = 8] predict(fit, newdata = data.frame(x1 = 10, x2 = 8), interval = &#39;confidence&#39;) ## fit lwr upr ## 1 72.67072 71.64637 73.69507 Por lo tanto, se espera que, si continuamos trabajando bajo las condiciones \\(x_1 = 10\\) y \\(x_2 = 8\\), el espesor de capa promedio sea 72.67 micrones. A nivel poblacional, dicho promedio se encontrará en el intervalo \\((71.646, 73.695)\\) con una confianza del 95%. 3.5.2 Intervalos de predicción para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) Sea \\(\\hat{y}_0 = \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}\\), donde \\(\\mathbf{x}_0\\) es el vector de covariables futuro. Un intervalo de predicción del \\(100\\times(1-\\alpha)\\%\\) para \\(Y_0\\) está dado por: \\[\\begin{eqnarray} \\hat{y}_0 \\pm t_{\\alpha/2,n-p}\\sqrt{\\hat{\\sigma}^2(1 + \\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0)} \\end{eqnarray}\\] Otra posibilidad para construir dicho intervalo es vía bootstrap. Observe que lo único que cambia en este intervalo en relación con el intervalo de confianza es la varianza de \\(Y_0\\) \\(-\\) existe más incertidumbre. Observe que en el intervalo de predicción estamos interesados en \\(Y_0 | \\mathbf{x}_0\\) y no \\(\\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}\\). Para calcular un intervalo de predicción del 95%, agregamos el argumento interval = 'prediction' a la instrucción anterior: ## intervalo de predicción para E[y|x_1 = 10, x_2 = 8] predict(fit, newdata = data.frame(x1 = 10, x2 = 8), interval = &#39;prediction&#39;) ## fit lwr upr ## 1 72.67072 67.08489 78.25655 Este resultado indica que, bajo las condiciones \\(x_1 = 10\\) y \\(x_2 = 8\\), el valor del espesor de la capa de pintura para la próxima unidad experimental será 72.67 micrones. A nivel poblacional, dicho valor se encontrará en el intervalo \\((67.085, 78.257)\\) con una confianza del 95%. 3.6 Análisis de Residuales El análisis de residuales en RLM es fundamental para: Validar los supuestos del error; identicar observaciones outlier; e identificar observaciones influenciales. 3.6.1 Validación de supuestos La validación de los supestos del error en el modelo de RLM se realiza de manera similar a como se mostró para el modelo de RLS. Para más detalles, ver Análisis de Residuales. 3.6.2 Identificación de outliers Los outliers son también conocidos como observaciones atípicas en los datos. A partir del modelo ajustado, podemos calcular: Residuales crudos \\[\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\] Residuales estandarizados \\[d_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\hat{\\sigma}^2}} = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{MSE}}} \\] Residuales estudentizados \\[r_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii})}} = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{MSE}(1-h_{ii})}}\\] donde \\(h_{ii} = \\mathbf{X}(\\mathbf{X^\\prime\\mathbf{X}})^{-1}\\mathbf{X}^\\prime_{ii}\\) y \\(\\mathbf{X}(\\mathbf{X^\\prime\\mathbf{X}})^{-1}\\mathbf{X}\\) denominada la matriz hat. La mejor manera de identificar outliers es a partir del cálculo de los residuales estudentizados. Decimos que la \\(i\\)-ésima observación es un outlier si \\(r_i\\notin (-3, 3)\\). En R las funciones clave son: ## residuales crudos r_crudo &lt;- residuals(fit) ## residuales estudentizados r_est &lt;- rstudent(fit) ## residuales estandarizados r_normal &lt;- rstandard(fit) donde fit es el objeto R que contiene en modelo de RLM estimado. Para más detalles, se sugiere consultar la ayuda de cada función: ?residuals, ?rstudent y ?rstandard. En nuestro caso, ## residuales estudentizados r &lt;- rstudent(fit) which(r &lt; -3 | r &gt; 3) ## 18 ## 18 la observación 18 podría considerarse un outlier. En la práctica, el siguiente paso es evaluar la trazabilidad de esa observación y determinar si existen o no causas asignables para que esta sea un outlier. En caso de que exista una causa asignable, dicha observación debería removerse de la base de datos y, con los datos reducidos, estimar nuevamente el modelo de RLM. Otra forma de detectar outliers es a través de la prueba de Bonferroni. Esta prueba está implementada en la función outlierTest del paquete car. En nuestro ejemplo, procedemos de la siguiente manera: ## prueba Bonferroni para outliers require(car) outlierTest(fit, n.max = 5) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 18 3.360964 0.001116 0.1116 En la parte superior vemos que la observación 18 tiene el mayor valor del residual estudentizado. Sin embargo, el valor \\(p\\) es superior a 0.05, por lo que no tenemos evidencia suficiente para concluir que dicha observación representa un outlier. Gráficamente es posible identificar gráficamente cuáles son las observaciones influenciales utilizando la función influenceIndexPlot del paquete car: ## gráfico de variables influenciales influenceIndexPlot(fit, vars = &quot;Bonf&quot;, las = 1) Figura 3.6: Gráfico de observaciones influenciales. El eje \\(y\\) corresponde al valor \\(p\\) de la prueba de Bonferroni. Para más detalles, se recomienda escribir ?viainfluenceIndexPlot en la consola del R. 3.6.3 Identificación de observaciones influenciales En ciertas ocasiones encontramos observaciones que lucen algo anormales y es importante determinar si estas son influenciales o no. A diferencia de los outliers, las observaciones influenciales controlan el modelo y por ello es importante determinar si el modelo ajustado es consistente cuando estas se remueven. La identificación de este tipo de observaciones se realiza utilizando, principalmente, la Distancia de Cook. Para la \\(i\\)-ésima observación, esta distancia se calcula como \\[D_i = \\frac{r_i^2}{p}\\frac{h_{ii}}{(1-h_{ii})} = \\frac{\\hat{\\epsilon}_i^2 \\,h_{ii}}{p\\,\\hat{\\sigma}^2(1-h_{ii})^2}\\] donde \\(p\\) es el número de variables controlables incluídas en el modelo. Usualmente, cuando \\(D_i&gt;\\frac{4}{n-p-2}\\) decimos que la \\(i\\)-ésima observacion es influencial. Este criterio es el utilizado por la función cooks.distance() del R. 3.7 Análisis de Multicolinealidad Cuando se utiliza el modelo de RLM, se asume que las variables controlables \\(X_1, X_2,\\ldots,X_k\\) son independientes. Desde el punto de vista práctico, esto tiene consideraciones importantes puesto que permite evaluar la magnitud del efecto de sobre \\(\\widehat{E[Y]}\\) cuando modificamos, en una unidad, digamos \\(x_j\\), mientras se mantienen el resto de ellas constantes. Este efecto corresponde, sin duda, a \\(\\hat{\\beta}_j\\). Sin embargo, cuando estas variables controlables no son independientes, este efecto no puede calcularse de la misma forma. Buscamos modelos en los que las covariables estén altamente correlacionadas con la respuesta, pero mínimamente entre ellas. Desde el punto de vista teórico, la existencia de no independencia en las variables controlables tiene consecuencias importantes sobre los estimadores de los parámetros del modelo dados por \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k)\\). Cuando se usa el método de mínimos cuadrados, los estimadores de \\(\\mathbf{\\beta}\\) están dados por: \\[ \\hat{\\mathbf{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime \\mathbf{y} \\] La existencia de multicolinealidad es sinónimo de que no existe independencia en las variables controlables del modelo. Si esto es cierto, las columnas de la matriz de diseño \\(\\mathbf{X}\\) no son independientes, es decir, que la columna \\(x_j\\) puede expresarse como una combinación lineal de las demás. Matemáticamente, esto es equivalente a escribir \\(x_j \\sim x_{-j}\\) para algún \\(j\\). Por ejemplo, para \\(j=1\\) tendríamos \\[ x_1 \\sim x_2 + x_3 + \\cdots + x_k. \\] Esta expresión indica que la variable independiente/controlable \\(x_1\\) puede escribirse como una combinación lineal de las demás variables controlables. O, en otras palabras, que la información contenida en \\(x_1\\) puede explicarse por las demás variables controlables medidas en el proceso durante la etapa de muestreo. Una manera de interpretar la multicolinealidad es como sinónimo de redundancia. Esta redundancia se refiere a que existen variables controlables en el proceso de producción que contienen la misma información que las demás. Por lo tanto, basta con medir sólo aquellas que realmente determinan dicho proceso. En la expresión de \\(\\hat{\\mathbf{\\beta}}_{\\text{OLS}}\\), el término \\((\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\) se refiere a la inversa de \\(\\mathbf{X}^\\prime\\mathbf{X}\\). Si existe multicolinealidad, \\[ \\text{det}(\\mathbf{X}^\\prime \\mathbf{X}) \\approx 0 \\quad \\Rightarrow \\quad \\frac{1}{|\\mathbf{X}^\\prime \\mathbf{X}|} \\rightarrow\\infty \\] Por lo tanto, \\[ \\hat{\\mathbf{\\beta}}_{\\text{OLS}} \\rightarrow \\infty. \\] 3.7.1 Cómo determinar si existe multicolinealidad? Existen varios indicadores para sospechar que existe multicolinealidad: Una alta correlación en las variables independientes. Esto es posible determinarlo gráficamente a través de una matriz de dispersión (ver por ejemplo ?pairs en la consola del R) o utilizando una prueba de independencia completa. Que se rechace la prueba de significancia global pero no todas las pruebas de significancia marginal. Que ocurran ambios considerables en \\(\\hat{\\mathbf{\\beta}}\\) cuando se agrega o elimina una variable predictora. Para probar que efectivamente existe, podemos usar tres aproximaciones: El número de condición de la matriz \\(\\mathbf{X}^\\prime \\mathbf{X}\\). También conocido como I-ll condicion number o ICN, este número mide qué tan “enferma” se encuentra la matriz que debe ser invertida para poder calcular \\(\\hat{\\mathbf{\\beta}}_{\\text{OLS}}\\). El ICN se calcula como \\[\\text{ICN}(\\mathbf{X}^\\prime\\mathbf{X}) = \\sqrt{\\frac{\\lambda_\\text{máx}}{\\lambda_\\text{min}}} \\] con \\(\\lambda_\\text{máx}\\) y \\(\\lambda_\\text{min}\\) los valores propios máximos y mínimos de \\(\\mathbf{X}^\\prime \\mathbf{X}\\), obtenidos a partir de la descomposición espectral de dicha matriz. Decimos que existe multicolinealidad cuando \\(\\text{ICN}(\\mathbf{X}^\\prime \\mathbf{X}) &gt; 30\\). El inconveniente con el ICN es que no nos da información acerca de cuál de las variables independentes es la más multicolineal (o redundante) en el sistema. En R, la función clave para calcular el ICN es kappa. Para más detalles, se recomienda escribir ?kappa en la consola. En el caso del ejemplo del espesor de pintura, tendríamos los siguientes resultados: ## ICN para el modelo ajustado kappa(fit) ## [1] 48.62071 Puesto que \\(\\text{ICN}(\\mathbf{X}^\\prime \\mathbf{X}) &gt; 30\\), podemos concluir que existe evidencia para sospechar que, efectivamente, existe multicolinealidad. El factor de inflación de varianza (VIF). A través de este indicador podemos detectar cuál de las variables independientes es la más colineal de las \\(k\\) medidas. Para la \\(j\\)-ésima variable independiente, \\[\\text{VIF}_j = \\frac{1}{1-R_j^2}\\] donde \\(R_j^2\\) es el \\(R^2_\\text{adjusted}\\) del modelo \\(x_j\\sim x_{-j}\\). Decimos que la variable \\(x_j\\) es responsable por la multicolineal en el sistema si \\(\\text{VIF}_j &gt; 5\\). En R, el VIF puede calcularse a través de la función vif del paquete car. Para más detalles, se recomienda escribir ?vif en la consola. En nuestro ejemplo tendríamos los siguientes resultados: ## cálculo del VIF car:::vif(fit) ## x1 x2 ## 1.001645 1.001645 Puesto que en ninguna de las dos variables controlables el \\(\\text{VIF}&gt;5\\), concluimos que no existe multicolinealidad. Pruebas complementarias. En algunos casos, el ICN indica que existe multicolinealidad, pero para ninguno de los predictores el \\(\\text{VIF}&gt;5\\). Cuando esto ocurre, lo mejor es utilizar pruebas complementaria, más robustas, que permitan decidir si efectivamente existe multicolinealidad en el modelo de RLM. Las pruebas complementarias pueden realizarse en R con la función mctest del paquete mctest. En nuestro ejemplo tendríamos los siguientes resultados: ## pruebas complementarias de multicolinealidad require(mctest) mctest(fit) ## ## Call: ## omcdiag(mod = mod, Inter = TRUE, detr = detr, red = red, conf = conf, ## theil = theil, cn = cn) ## ## ## Overall Multicollinearity Diagnostics ## ## MC Results detection ## Determinant |X&#39;X|: 0.9984 0 ## Farrar Chi-Square: 0.1602 0 ## Red Indicator: 0.0405 0 ## Sum of Lambda Inverse: 2.0033 0 ## Theil&#39;s Method: -0.8683 0 ## Condition Number: 8.2787 0 ## ## 1 --&gt; COLLINEARITY is detected by the test ## 0 --&gt; COLLINEARITY is not detected by the test De acuerdo con estos resultados, podemos concluir que no existe multicolinealidad. 3.8 Selección de Modelos 3.8.1 Método de Todas las Regresiones Posibles El Método de Todas las Regresiones Posibles permite, a partir de un conjunto de variables independientes \\(X_1, X_2, \\ldots, X_k\\) que potencialmente podrían explicar una respuesta continua \\(Y\\), ajustar hasta \\(2^{k}-1\\) modelos de regresión y seleccionar el mejor de estos utilizando algún criterio. En la práctica, algunos de los criterios más utilizados incluyen \\(R^2\\), \\(R^2_\\text{adj}\\), \\(\\sqrt{MSE}\\), AIC, BIC, PRESS y \\[\\begin{equation} C_p = p+\\frac{\\text{SSE}_p}{\\text{MSE}_\\text{todos}} - (n-2p) = \\begin{cases} = p &amp; \\text{ para el modelo completo } \\\\ \\approx p &amp; \\text{ el sesgo es pequeño $\\rightarrow$ ideal!} \\\\ &gt; p &amp; \\text{ sesgo es alto }\\\\ &lt; p &amp; \\text{ no hay sesgo }\\\\ \\end{cases} \\end{equation}\\] también conocido como el estadístico de Mallows. En la expresión anterior, \\(\\text{MSE}_\\text{todos}\\) es \\(\\hat{\\sigma}^2\\) usando todas las covariables, y \\(\\text{SSE}_p\\) es el SSE del modelo con sólo \\(p^\\prime &lt; p\\) de ellas. Como ejemplo, consideraremos los siguientes datos que corresponden al peso del producto terminado en gramos (variable \\(y\\)) cuando se controlan los parámetros \\(x_1, x_2, \\ldots, x_{10}\\) de una inyectora: ## datos inyectora d &lt;- read.table(&quot;https://www.dropbox.com/s/a9gzu54luabtubo/inyectora.txt?dl=1&quot;, header = TRUE) ## primeras 3 filas head(d, 3) ## y x1 x2 x3 x4 x5 x6 ## 1 22.49879 -0.4248450 0.19997792 -0.5225479 0.5691505 0.9721086 -0.2927878 ## 2 17.60173 0.5766103 -0.33435292 0.9247179 -0.9811402 -0.7258651 -0.2671171 ## 3 22.50649 -0.1820462 -0.02277393 0.2027315 0.5581318 0.8106192 -0.4257997 ## x7 x8 x9 x10 ## 1 -0.5255406 0.6898671 -0.05863633 0.84739843 ## 2 0.3729807 -0.4797351 -0.26830905 0.08519674 ## 3 -0.5483632 -0.9537110 -0.75745589 0.70472920 En total se tienen 100 unidades experimentales. Dado que en los datos sólo existen la variable respuesta y las covariables, podemos utilizar la siguiente sintaxis para ajustar el modelo de RLM: ## modelo de RLM ajustado fit_inyectora &lt;- lm(y ~ ., data = d) summary(fit_inyectora) ## ## Call: ## lm(formula = y ~ ., data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.09900 -0.61503 -0.04698 0.49843 2.35473 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.97501 0.09496 210.353 &lt; 2e-16 *** ## x1 3.04897 0.17692 17.234 &lt; 2e-16 *** ## x2 0.85418 0.18412 4.639 1.20e-05 *** ## x3 4.98623 0.17088 29.179 &lt; 2e-16 *** ## x4 4.97033 0.17161 28.963 &lt; 2e-16 *** ## x5 2.99030 0.17537 17.052 &lt; 2e-16 *** ## x6 4.03875 0.16979 23.786 &lt; 2e-16 *** ## x7 2.08153 0.16505 12.612 &lt; 2e-16 *** ## x8 0.89152 0.16997 5.245 1.05e-06 *** ## x9 2.90108 0.18030 16.090 &lt; 2e-16 *** ## x10 2.89888 0.17135 16.918 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9418 on 89 degrees of freedom ## Multiple R-squared: 0.9778, Adjusted R-squared: 0.9753 ## F-statistic: 391.2 on 10 and 89 DF, p-value: &lt; 2.2e-16 Como el número de variables independientes es \\(k=10\\), debemos estimar \\(2^k-1= 1023\\) modelos diferentes al utilizar el Método de Todas las Regresiones Posibles. Para encontrar el mejor modelo, usamos la función ols_step_all_possible del paquete olsrr. Para más detalles, se recomienda escribir ?ols_step_all_possible en la consola del R. ## método de todas las regresiones posibles require(olsrr) k &lt;- ols_step_all_possible(fit_inyectora) head(k, 10) ## Index N Predictors R-Square Adj. R-Square Mallow&#39;s Cp ## 4 1 1 x4 0.381431483 0.375119559 2379.065 ## 10 2 1 x10 0.221939980 0.214000593 3017.235 ## 5 3 1 x5 0.150758161 0.142092428 3302.054 ## 3 4 1 x3 0.122800863 0.113849851 3413.919 ## 9 5 1 x9 0.065647890 0.056113685 3642.604 ## 6 6 1 x6 0.061473423 0.051896621 3659.307 ## 7 7 1 x7 0.049953135 0.040258779 3705.403 ## 8 8 1 x8 0.037568650 0.027747921 3754.957 ## 1 9 1 x1 0.005957422 -0.004185870 3881.442 ## 2 10 1 x2 0.005662949 -0.004483347 3882.620 En el objeto k se encuentran todos los resultados. La función ols_step_all_possible permite estimar 11 indicadores diferentes que facilitan la elección del mejor modelo. Por faciidad, sólo se muestran 3 de ellos. Por ejemplo, si queremos seleccionar el mejor modelo utilizando el \\(R^2_\\text{adj}\\), basta con escribir ## mejor modelo basado en el R^2_{adj} k[which.max(k[,&#39;adjr&#39;]), ] ## mindex n predictors rsquare adjr predrsq cp ## 1023 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 0.9752579 0.9715036 11 ## aic sbic sbc msep fpe apc hsp ## 1023 284.1403 3.041157 315.4024 88.79029 0.9845406 0.02774113 0.01007924 Si usamos el AIC, el mejor modelo será: ## mejor modelo basado en el Cp k[which.min(with(k, (cp-n)^2)), ] ## mindex n predictors rsquare adjr predrsq cp ## 1023 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 0.9752579 0.9715036 11 ## aic sbic sbc msep fpe apc hsp ## 1023 284.1403 3.041157 315.4024 88.79029 0.9845406 0.02774113 0.01007924 Otra posibilidad es utilizar dos criterios selección a la vez: ## selección usando el R^2_{adj} y el AIC require(ggplot2) ggplot(k, aes(x = adjr, y = aic)) + geom_point() + theme_minimal() + xlab(expression(R^2 * &quot; ajustado&quot;)) + ylab(&quot;AIC&quot;) Figura 3.7: AIC vs. \\(R^2\\) ajustado en todas las regresiones posibles. De acuerdo con la definición de cada criterio, el mejor modelo debe tener un \\(R^2_{\\text{adj}}\\rightarrow1\\) y \\(\\text{AIC}\\rightarrow 0\\). De la gráfica, es posible observar que hay cuatro modelos que sobresalen: ## selección de modelos competitivos k &lt;- as.data.frame(k) subset(k, aic &lt; 350 &amp; adjr &gt; 0.8, select = n:aic) ## n predictors rsquare adjr predrsq cp ## 998 8 x1 x3 x4 x5 x6 x7 x9 x10 0.9650304 0.9619562 0.9577212 57.92307 ## 1021 9 x1 x3 x4 x5 x6 x7 x8 x9 x10 0.9723781 0.9696159 0.9653333 30.52284 ## 1015 9 x1 x2 x3 x4 x5 x6 x7 x9 x10 0.9708818 0.9679700 0.9640209 36.51001 ## 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 0.9752579 0.9715036 11.00000 ## aic ## 998 325.3860 ## 1021 303.7989 ## 1015 309.0744 ## 1023 284.1403 Por ejemplo, el modelo 968 no incluye x2 ni x8 y tiene un \\(R^2_{\\text{adj}}=0.9619\\), mientras el modelo 1023 tiene el \\(R^2_{\\text{adj}}\\) más alto e incluye 10 variables predictoras. A pesar de que este modelo es levemente mejor el 968, la ganancia en \\(R^2_{\\text{adj}}\\) es ínfima y podría no justificar la medición en proceso de dos factores controlables más. En cuanto al AIC, se tiene que los modelos 1021 y 1015 son prácticamente los mismos. 3.8.2 Selección secuencial Need to add some introductory remarks Método stepwise To be completed Método forward To be completed Método backward To be completed "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

[["index.html", "Modelos de Regresión: Una aproximación práctica con R Bienvenido Estructura del libro Software y convenciones Bloques informativos Dedicatoria Sobre el autor", " Modelos de Regresión: Una aproximación práctica con R Jorge I. Vélez 2021-09-14 Bienvenido Este libro está destinado para estudiantes de ingeniería, psicología, economía, estadística, matemáticas y áreas afines interesados en el análisis de datos, especialmente en la formulación, ajuste, validación e implementación de modelos de regresión avanzados, pronósticos en series de tiempo y métodos no paramétricos utilizando R. En este texto se recopilan los elementos básicos del curso de Análisis de Datos en Ingeniería II del Programa de Ingeniería Industrial de la Universidad del Norte, y está pensado como una guía para dicho curso. Por supuesto, los errores presentes a lo largo de todo el documento son sólo mi responsabilidad. Estructura del libro En el Capítulo 1 se presentan algunos conceptos básicos de Análisis de Datos, además de qué es R, cómo instalarlo, dónde y cómo buscar ayuda y la estructura básica del programa. En el capítulo 2 se discute el Modelo de Regresión Lineal Simple y en el capítulo 3 el Modelo de Regresión Lineal Múltiple. En el capítulo 4 se extienden estos modelos para el caso donde la variable respuesta puede tomar valores discretos, y se discuten los modelos de Regresión Logística y Regresión Poisson. Posteriormente, en el capítulo 5, se realiza una corta Introducción a Series de Tiempo. Como parte de ello, se discuten algunos conceptos generales y se presentan tres estrategias diferentes para realizar pronósticos. Finalmente, en el capítulo 6, se introducen conceptos básicos de Estadística No Paramétrica entre los que se incluyen las pruebas de signo, rangos con signos, Mann-Whitney-Wilcoxon y Kruskal-Wallis, además de algunas estrategias para el análisis de tablas de contingencia. Para afianzar el proceso de aprendizaje, en cada capítulo se presentan una serie de ejercicios donde el estudiante puede poner en práctica los conceptos teóricos a través de la solución de problemas que comúnmente se presentan en ingeniería. Software y convenciones Este libro está construido utilizando los paquetes knitr (Xie 2015) y bookdown (Xie 2019) de R. En todo el libro se presentarán códigos que el lector puede seleccionar y posteriormente ejecutar en su consola de R para obtener los mismos resultados del libro. El código se destaca en una recuadro de color similar al que se muestra a continuación: 3 + 7 x &lt;- c(2, 9, 11) 5 * x 1:5 Los resultados obtenidos de cualquier código R se destacan con los símbolos ## al inicio de cada renglón. Por ejemplo, el resultado de 3 + 7 será mostrado como ## [1] 10 Bloques informativos En varias partes del libro se usarán bloques informativos para resaltar algún aspecto importante: Nota aclaratoria. Sugerencia. Advertencia. Dedicatoria Dedicado a Lilyam y Abraham por su apoyo; a Norma Botero por mostrarme el camino; a mis maestros y amigos, Mauricio Arcos-Burgos y Juan Carlos Correa por sus enseñanzas y acompañamiento; a Yolima, por su amor incondicional, su infinita comprensión y paciencia; y a Gabriela, mi monstruo peludo, por darme fuerzas cuando no había razones para continuar. Sobre el autor Jorge I. Vélez es Profesor Asistente del Departamento de Ingeniería Industrial de la Universidad del Norte en Barranquilla, Colombia en las áreas de Analítica de Datos y Diseño de Experimentos para los programas de Pregrado, Maestría y Doctorado. Experto en Analítica de Datos, Bioinformática, Genómica Predictiva, Genética de Poblaciones, Epidemiología Genética y Estadística Computacional y Genética, el Profesor Vélez es instructor en las áreas de Genética de Poblaciones, Diseño de Experimentos, Bioinformática, y Bioestadística en los programas de Maestría y Doctorado en Ciencias Biomédicas del Departamento de Medicina de la Universidad del Norte. Sus intereses de investigación incluyen Analítica de Datos, Bioinformática y Genómica Predictiva aplicadas a enfermedad de Alzheimer, Trastorno de Déficit de Atención con Hiperactividad y Enfermedades Autoinmunes. Referencias "],["intro.html", "Capítulo 1 Introducción 1.1 Analítica de Datos 1.2 R 1.3 RStudio", " Capítulo 1 Introducción El análisis de datos es el pilar fundamental de Data Analytics y Data Science. Por supuesto, el uso herramientas computacionales es importante. A lo largo de este materal usaremos el R, pero muchos de los conceptos aquí expuestos pueden trabajarse, sin problema, en otro lenguaje como Python o Julia. En la Universidad del Norte, los estudiantes tienen la posibilidad de tomar, a nivel de pregrado, cursos de Análisis de Datos. Parcticularmente, los estudiantes de la División de Ingenierías pueden tomar los cursos de : Análisis de Datos en Ingeniería I, ofrecido por el Departamento de Ingeniería Industrial; Análisis de Datos en Ingeniería II, ofrecido por el Departamento de Ingeniería Industrial; Machine Learning, ofrecido por el Departamento de Ingeniería Eléctrica y Electrónica y Analítica de Datos Avanzada; ofrecido por el Departamento de Ingeniería Industrial. Estos cursos, cuya dificultad aumenta conforme a la enumeración anterior, permiten a los estudiantes de la División de Ingenierías tener herramientas básicas, intermedias y avanzadoas para convertir datos en información. 1.1 Analítica de Datos 1.1.1 Conceptos básicos 1.1.2 Gráficos básicos 1.1.3 Tópicos avanzados 1.2 R R es un entorno de programación multiplataforma y un lenguaje de programación orientado a objetos similar a S, pero bajo licencia GNU, para el análisis gráfico y estadístico de datos creado en por Robert Gentleman y Ross Ihaka en Agosto de 1993. La página oficial de R es https://www.r-project.org/. Figura 1.1: Logo oficial del R. Por ser multiplataforma, R puede utilizarse en diferentes sistemas operativos, incluyendo Windows, macos y varias distribuciones de Linux. Figura 1.2: Qué es R? Ver https://www.youtube.com/watch?v=XcBLEVknqvY. Desde su concepción y gracias a su constante evolución, R proporciona una gran cantidad de métodos estadísticos y gráficos, y es altamente extendible. Esta última característica permite el desarrollo e implementación de nuevos métodos estadísticos y nuevos tipos de gráficos, además de la automatización de reportes con relativa facilidad. 1.2.1 Por qué R? 1.2.2 CRAN CRAN es el acrónimo de Comprehensive, R, Archive, Network, y corresponde a una red de servidores ftp y web de todo el mundo que almacenan versiones idénticas y actualizadas de código y documentación para R. El sitio web oficial es https://cran.r-project.org/ 1.2.3 Descarga e instalación A Septiembre de 2021, la versión más reciente de R es la 4.1.1, también denominada “Kick Things.” Figura 1.3: Página oficial del R. Note que la versión más reciente es la 4.1.1. Dependiendo del sistema operativo, R puede descargarse en los siguientes enlances: Windows. macos. Varias distribuciones de Linux. Las versiones de R tienen un ciclo de actualización cada 6 meses, y es altamente recomendable tener instalada la última versión del programa en el computador. Una vez descargado el instalador, basta con seguir las instrucciones para instalar el programa con las opciones por defecto. 1.2.4 Paquetes Los paquetes o packages son colecciones de funciones y/o datos que ayudan a potencializar las funcionalidades de R. Cada paquete tiene su documentación y, en muchos casos, ejemplos concretos que permiten comenzar a utilizarlos de inmediato. Figura 1.4: Algnos paquetes de R. Existen paquetes para visualización, minería de texto, automatización de reportes y mucho más. La lista completa de paquetes se encuentra aquí. A la fecha existen 18188 paquetes; algunos ejemplos conocidos son MASS, ggplot2, shiny, y caret, entre otros. Para utilizar un paquete, debemos seguir dos pasos Instalar el paquete; y cargarlo a la sesión de trabajo. La instalación de los paquetes se realiza utilizando la función install.packages('PackageName'), donde el argumento PackageName corresponde al nombre del paquete que quiere instalarse. Por ejemplo, si quisiéramos instalar el paquete DescTools, escribimos: ## instalación del paquete DescTools install.packages(&#39;DescTools&#39;, dependencies = TRUE) Posteriormente, aparecerán algunos mensajes que nos indican el progreso de dicha instalación. Cuando el proceso concluya, procedemos cargar el paquete a la sesión de trabajo. Para ello, utilizamos la función library('PackageName'). En nuestro caso, reemplazamos el argumento PackageName por el nombre del paquete que quieres utilizar: ## carga del paquete DescTools en la sesión de trabajo library(&#39;DescTools&#39;) Con esta instrucción, R importa las funciones contenidas en el paquete DescTools a la sesión de trabajo actual. La instrucción install.packages('PackageName') sólo debe ejecutarse una vez. Sin embargo, es necesario ejecutar library('PackageName') cada que inicies una sesión en R. Esto se debe a que aunque hayas instalado las funciones de un paquete con anterioridad, las sesiones de R se inician sólo con los objetos y funciones de base. 1.2.5 Operadores básicos Operadores aritméticos Figura 1.5: Operadores artiméticos en R. Tomado de R para principantes. Operadores relacionales Figura 1.6: Operadores relacionales en R. Tomado de R para principantes. Operadores lógicos Figura 1.7: Operadores lógicos en R. Tomado de R para principantes. 1.2.6 Creación de funciones Una de las grandes ventajas de R es la posibilidad de construir y/o agregar funciones propias. Como se mencionó anteriormente, los paquetes están conformados por funciones, además de datos. Una función es un conjunto de instrucciones organizadas para realizar una tarea específica y corresponde a un objeto que el intérprete de R es capaz de entender y utilizar sus argumentos para completar una acción. Algunas funciones disponibles por defecto en R son ## funciones R por defecto sqrt() mean() summary() rowSums() colSums() La estructura de una función es Figura 1.8: Estructura de una funcíon en R. Tomado de Learn by &lt;example&gt;. lo cual implica que toda función, como mínimo, cuatro componentes: name: Corresponde al nombre de la función. Se recomienda usar guión bajo o nombres con tipografía camello para nombrarlas, es decir, calcula_media y GraficoHist. Bajo ninguna circunstancia deben usarse nombres como coeficiente-variacion. args: Este componente de la función hace referencia a los argumentos de esta. Por argumentos denominamos a las entradas que requiere la función para hacer su trabajo. Una función puede tener varios argumentos; por ejemplo la función t.test tiene los argumentos x, y, alternative, mu, paired, var.equal, y conf.level. Algunas veces los argumentos tienen valores por defecto. De esta forma, cuando se utiliza la función, no es necesario especificar el valor de dicho argumento. Por ejemplo, en la función mean, el argumento trim = 0. function body: Contiene los pasos, procedimientos y cálculos que deben hacerse para producir un resultado. value: Este es el resultado de la función. Dependiendo de qué tan compleja sea la función, el value puede ser un escalar, un data.frame, una lista, un archivo de texto o un gráfico. Hay muchas posibilidades. Ejemplo Como ilustración, construyamos una función que, dado un vector de datos de cualquier longitud, calcule el coeficiente de variación (CV). El CV está definido como \\[CV = \\frac{s}{\\bar{x}}\\] donde \\(s\\) es la desviación estándar y \\(\\bar{x}\\) es la media muestral. Si el vector de datos es ## datos v &lt;- c(3, 2, 3, 4, 5, 4, 5, 5, 4, 2, 9, 8) entonces nuestra función, que llamaremos CoefVar, debe tener como argumento dicho vector. Internamente, CoefVar debe calcular la desviación estándar y la media muestral de v. Por lo tanto, ## cálculo del coeficiente de variación ## v es un vector numérico CoefVar &lt;- function(v){ ## cálculos m &lt;- mean(v) ## media s &lt;- sd(v) ## s resultado &lt;- s/m ## CV ## output return(resultado) } Ahora, para usar nuestra función CoefVar con el vector v hacemos ## cálculo del CV CoefVar(v) ## [1] 0.4784937 Teniendo el valor del CV, podríamos determinar si la media muestral es una cantidad representativa del conjunto de datos. Para ello comparamos el resultado de \\(100\\times\\)CoefVar con 30%. ## función para determinar si la media representativa representativa &lt;- function(v, cutoff = 30){ CV_p &lt;- 100*CoefVar(v) decision &lt;- ifelse(CV_p &gt; cutoff, &#39;no&#39;, &#39;si&#39;) list(CV_p = CV_p, decision = decision) } Observe que en esta función incluimos un argumento denominado cutoff. Por defecto, cutoff = 30. Además. la eultima línea de la función comienza con list, lo cual implica que el resultado de la función será una lista de dos componentes: CV_p y decision. Veamos qué ocurre con el vector v utilizando los valores por defecto: ## aplicación de la función representativa representativa(v) ## $CV_p ## [1] 47.84937 ## ## $decision ## [1] &quot;no&quot; Por lo tanto, el CV del vector v es 0.478. Como \\(100\\times\\)CVp\\(&gt;30\\), la decisión es que la media no es representativa de los datos. Si queremos cambiar el argumento cutoff basta con hacer ## representatividad con cutoff = 50 representativa(v, cutoff = 50) ## $CV_p ## [1] 47.84937 ## ## $decision ## [1] &quot;si&quot; Observe que decision es diferente esta vez. Si queremos extraer el segundo elemento del resultado obtenido con la función representativa, debemos asignar dicho resultado a un objeto y posteriormente operar sobre él de la siguiente manera: ## extrayendo la decisión decision &lt;- representativa(v, cutoff = 50) decision[[2]] ## [1] &quot;si&quot; 1.2.7 Ayuda? Hay múltiples formas de acceder a la ayuda cuando trabajamos con R. En https://www.r-project.org/help.html se encuentra una descripción de cómo hacerlo. Podemos buscar ayuda, principalmente, a través de la consola del R utilizando los siguientes mecanismos: help y ?. Si requerimos ayuda sobre la función mean, podemos escribir ?mean o help(mean) en la consola del R. El resultado será: Figura 1.9: Resultado de escribir ?mean en la consola del R. apropos('word'). Esta función en R permite encontrar todas las funciones del espacio de tabajo que contienen la palabra word. Por ejemplo, apropos('mean') produce ## [1] &quot;.colMeans&quot; &quot;.rowMeans&quot; &quot;colMeans&quot; ## [4] &quot;kmeans&quot; &quot;mean&quot; &quot;mean.Date&quot; ## [7] &quot;mean.default&quot; &quot;mean.difftime&quot; &quot;mean.POSIXct&quot; ## [10] &quot;mean.POSIXlt&quot; &quot;rowMeans&quot; &quot;weighted.mean&quot; Si queremos saber qué hace rowMeans o cuáles son sus argumentos, basta con escribir ?rowMeans o help(rowMeans) en la consola. RSiteSearch(). Esta función busca información en las páginas de ayuda de las todas las funciones y en las viñetas de todos los paquetes de CRAN, así como en CRAN Task Views. R FAQs. Hay tres listas principales de preguntas frecuentes (FAQs) que se actualizan periódicamente para reflejar las preguntas más frecuentes de los usuarios de R: R Main FAQs, R FAQs para Windows y R FAQs para macos Stack overflow. Este es un sitio bien organizado para ayuda y discusiones sobre programación. Para acceder directamente a las discusiones relacionadas con R, se recomienda visitar https://stackoverflow.com/questions/tagged/r R-help-es. Esta es una lista de correo, creada en 2009, para solicitar ayuda sobre R en español y se entiende como un complemento social a la documentación, libros, etc. disponibles sobre R. Puede acceder directamente a los archivos en https://stat.ethz.ch/pipermail/r-help-es/ 1.2.8 Lectura de datos 1.3 RStudio RStudio es un entorno de desarrollo integrado (IDE) para R disponible en ediciones comerciales y de código abierto y se ejecuta en el escritorio (Windows, macos y Linux). Figura 1.10: Página oficial del R. Note que la versión más reciente es la 4.1.1. RStudio incluye una consola, un editor que resalta la sintaxis y admite la ejecución directa del código, así como herramientas para el trazado, el historial, la depuración y la gestión del espacio de trabajo. Para descargar Rstudio, basta con visitar https://www.rstudio.com/products/rstudio/download/. Una vez allí, basta escoger la versión acorde con nuestro sistema operativo. Figura 1.11: Página de descarga de RStudio. Una vez descargado, es suficiente con abrir el instalador y seguir las instrucciones. Cuando el proceso termine, el siguiente paso es abrirl el programa haciendo click en el ícono de RStudio y vse abrirá una ventana similar a esto: Figura 1.12: RStudio IDE. Tomado de Diego Calvo. Por ser un IDE, RStudio está compuesto por cuatro paneles: Panel 1. Está ubicado en la esquina superior izquierda. En este panel se escriben los scripts, los cuales corresponde a instrucciones que R puede leer y ejecutar. Panel 2. Está ubicado en la parte superior derecha. Este panel está compuesto, principalmente, por dos pestañas: Workspace y History. La pestaña Workspace muestra los objetos y funciones creados dutante la sesión de trabajo. Panel 3. Este panel, ubicado en la esquina inferior derecha, está compuesto por varias pestañas, incuyendo Files, Plots, Packages y Help. La pestaña Files muestra los archivos presentes en el directorio de trabajo; Plots muestra los diferentes gráficos que se crean en la sesión; Packages muestra los diferentes paquetes instalados en R y aquellos en uso durante la sesión de trabajo; y, finalmente, Help nos muestra la ayuda de funciones y/o paquetes. Panel 4. Este panel, ubicado en la esquina inferior izquierda, corresponde a la consola del R. En este panel todas las instrucciones o scripts son interpretadas y ejecutadas por el programa. Es posible modificar la organización del RStudio IDE. Para ello, basta con ingresar a Tools –&gt; Global Options... –&gt; Pane Layout. "],["rls.html", "Capítulo 2 Regresión Lineal Simple 2.1 Formulación básica del modelo de RLS 2.2 Estimación 2.3 Tabla ANOVA y medidas de desempeño 2.4 Análisis de Residuales 2.5 Predicción", " Capítulo 2 Regresión Lineal Simple Diariamente, en muchos procesos productivos se controlan las condiciones de operación de un sistema y es de interés determinar cómo una variable respuesta de cambia dependiendo dichas condiciones. Por ejemplo, en un proceso de embutido de cárnicos la cantidad de producto empacado (en gramos, por ejemplo) dependa de la velocidad y la temperatura a la que opere una máquina. En la práctica, por supuesto, es fundamental Entender la relación entre la cantidad de producto empacado, la velocidad y la temperatura de operación; Garantizar que la cantidad de producto empacado cumpla con las especificaciones; Determinar las condiciones óptimas de operación que minimizan los costos asociados y disminuyen considerablemente los desperdicios de producto. El Modelo de Regresión Lineal es ampliamente utilizado en la práctica ingenieril cuando, en un proceso productivo como el anterior, se quiere dar respuesta los numerales 1, 2 y 3. En un sentido menos amplio, los modelos de regresión lineal se utilizan para estudiar la relación entre una variable numérica, que denominamos variable respuesta \\(Y\\), y un conjunto de variables controlables de dicho proceso denotadas como \\(X_1, X_2,\\ldots, X_k\\). Una vez entendemos dicha relación, es posible predecir futuros valores de la variable respuesta para valores específicos y conocidos de las variabes controlables. Por variable controlable entendemos aquellas variables de proceso que pueden medirse fácilmente y se mantienen fijas durante la operación. Dependiendo del número de variables controlables, los modelos de Regresión Lineal pueden dividirse en Modelo de Regresión Lineal Simple (RLS) y Modelo de Regresión Lineal Múltiple (RLM). Cuando se tiene sólo una variable controlable, digamos \\(x\\), hablamos de RLS. Cuando se tienen más de una variable controlable, digamos \\(x_1, x_2, \\ldots, x_k\\), hablamos de RLM. En el caso de proceso de embutidos, \\(x_1 = \\text{velocidad}\\) y \\(x_2 = \\text{temperatura}\\). 2.1 Formulación básica del modelo de RLS El modelo de RLS surge ante la necesidad de predecir una variable respuesta \\(Y\\), generalmente continua, como función de una variable controlable \\(X\\). Matemáticamente el modelo puede expresarse como: \\[\\begin{align} \\label{mod1} Y_i &amp;= \\beta_0 + \\beta_1X_i + \\epsilon_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] donde \\((\\beta_0, \\beta_1, \\sigma^2)\\) corresponden a los parámetros del modelo y \\(\\epsilon_i\\) es el error aleatorio para la observación \\(i\\). Los términos \\((\\beta_0, \\beta_1)\\) corresponden a los coeficientes del modelo, mientras \\(\\sigma^2\\) es la varianza. En la práctica, se tiene o toma una muestra aleatoria de tamaño \\(n\\) de una población o proceso de producción/servicios, y se quieren estimar, a partir de dicha muestra, los valores de \\((\\beta_0, \\beta_1, \\sigma^2\\)). Consideremos un proceso donde se tienen registros de la Resistencia, en psi, y la Edad, en semanas, de varias soldaduras. Los datos pueden leerse en R como se muestra a continuación: ## lectura de datos en URL file &lt;- &quot;https://www.dropbox.com/s/h4abseyfnvfwgxq/data_rls.txt?dl=1&quot; datos &lt;- read.table(file = file, header = TRUE) ## primeras 6 filas de los datos head(datos) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 Con miras explorar si existe una relación entre la Resistenciay la Edad de la soldadura, existen dos estrategias. En RLS, es posible establecer si existe una posible relación entre \\(X\\) y \\(Y\\) es Construir un gráfico de dispersión. Calculando el coeficiente de correlación lineal muestral; Para construir un gráfico de dispersión procedemos de la siguiente manera: ## gráfico de dispersión require(ggplot2) ggplot(datos, aes(x = Edad, y = Resistencia)) + geom_point() + theme_minimal() Observe que, aparentemente, cuanto mayor sea la Edad de la soldadura menor será su Resistencia. Por lo tanto, podemos decir que la relación entre Edad y Resistencia es inversamente proporcional. El coeficiente de correlación \\(\\rho\\) entre dos variables aleatorias \\(X\\) e \\(Y\\) determina la relación lineal que existe entre ellas. A partir de una muestra de tamaño \\(n\\), dicho coeficiente puede calcularse como \\[\\begin{eqnarray} \\hat{\\rho}_{XY} &amp;=&amp; \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\\\\\nonumber &amp;=&amp; \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n{(x_i-\\bar{x})^2}\\sum_{i=1}^n{(y_i-\\bar{y})^2}}} \\end{eqnarray}\\] donde \\[\\begin{eqnarray} \\bar{x} &amp;=&amp; \\frac{\\sum_{i=1}^n{x_i}}{n} \\\\\\nonumber \\bar{y} &amp;=&amp; \\frac{\\sum_{i=1}^n{y_i}}{n}. \\end{eqnarray}\\] El coeficiente de correlación muestral \\(-1\\leq \\hat{\\rho}_{XY} \\leq 1\\) determina la relación lineal entre las variables \\(X\\) y \\(Y\\). \\(\\hat{\\rho}_{XY} \\approx -1\\) indica \\(X\\) y \\(Y\\) tienen una relación lineal inversa, es decir, que cuando \\(X\\) aumenta, la variable \\(Y\\) disminuye. \\(\\hat{\\rho}_{XY} \\approx 0\\) indica que no existe una relación lineal entre \\(X\\) y \\(Y\\). Sin embargo, este resultado no es suficiente para descartar que exista otro tipo de relación (por ejemplo cuadrática o cúbica) entre las dos variables. \\(\\hat{\\rho}_{XY} \\approx 1\\) indica \\(X\\) y \\(Y\\) tienen una relación lineal positiva, es decir, que cuando \\(X\\) aumenta, la variable \\(Y\\) también. Con los datos de soldaduras se tendría que: ## coeficiente de correlación with(datos, cor(Edad, Resistencia)) ## [1] -0.8827718 Por lo tanto, concluimos que la relación entre la Edad de la soldadura y su Resistencia es inversamente proporcional. 2.2 Estimación Para estimar el modelo de RLS, se requiere una muestra aleatoria de tamaño \\(n\\) de una población. Esta muestra puede corresponder a observaciones específicas de un proceso de producción o servicios donde se controla una variable \\(X\\) y se registra una variable respuesta \\(Y\\). Por lo general, los datos para \\(n\\) unidades experimentales de dicho proceso están constituidos por los pares \\[(x_1,y_1),(x_2,y_2),(x_3, y_3),\\ldots,(x_n,y_n).\\] En nuestro ejemplo, los datos están organizados de la siguiente forma: ## muestra los primeros 10 pares de datos head(datos, 10) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 ## 7 12.1 16.0 ## 8 14.3 7.0 ## 9 11.9 14.9 ## 10 11.1 15.6 Así, los pares \\((x_1,y_1)\\) y \\((x_5, y_5)\\) corresponden a \\((18.7, 10.9)\\) y \\((14.6, 13.0)\\), respectivamente. Para determinar el tamaño de la muestra, hacemos ## número total de observaciones NROW(datos) ## [1] 100 Usualmente, los parámetros \\((\\beta_0, \\beta_1,\\sigma^2)\\) el modelo de RLS puede estimarse utilizando el método de mínimos cuadrados. La idea fundamental es minizar la expresión \\[\\begin{eqnarray*} L &amp;=&amp;\\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2. \\end{eqnarray*}\\] Para encontrar los estimadores de \\(\\beta_0\\) y \\(\\beta_1\\) derivamos parcialmente e igualamos a cero las expresiones resultantes, esto es \\[\\frac{\\partial L}{\\partial \\beta_0}=0, \\hspace{1cm} \\frac{\\partial L}{\\partial \\beta_1}=0.\\] Los estimadores \\(\\beta_0\\) y \\(\\beta_1\\) están dados por \\[\\begin{align} \\widehat{\\beta}_0&amp;=\\overline{Y}-\\widehat{\\beta}_1\\overline{X}, \\\\ \\widehat{\\beta}_1&amp;=\\frac{S_{xy}}{S_{xx}} \\end{align}\\] donde \\[ S_{xx} = \\sum_{i=1}^n(x_i-\\bar{x})^2 \\quad\\text{y}\\quad S_{xy} = \\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}). \\] En la práctica, \\(\\widehat{\\beta}_1\\) se interpreta como el cambio unitario sobre el valor esperado de \\(Y\\), \\(E[Y|X]\\), por cada cambio unitario del factor controlable \\(X\\). Por otro lado, \\[\\begin{align} \\hat{\\sigma}^2 &amp;= \\frac{L}{n-2} \\\\ &amp;=\\frac{1}{n-2}\\sum_{i=1}^n\\hat{\\epsilon}_i^2 \\\\ &amp;=\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\ &amp;=\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1X_i)^2 \\\\ &amp;=\\frac{SSE}{n-2} \\\\ &amp;= MSE \\end{align}\\] donde el \\(SSE\\) corresponde a la suma de cuadrados del error, y \\(MSE\\) es el Mean Squared Error. Finalmente, el modelo de RLS estimado es \\[\\begin{align} \\widehat{Y}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1X_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\hat{\\sigma}^2), \\\\ \\hat{\\sigma}^2 &amp;= \\text{constante}. \\end{align}\\] Otra forma de escribirlo es \\[\\begin{align} \\widehat{Y}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\ \\hat{\\mu}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1X_i. \\end{align}\\] A partir del modelo ajustado es posible predicir el valor esperado de \\(Y\\) para valores conocidos de \\(X\\), es decir, calcular \\(\\widehat{E[Y|X=x_0]}\\), donde \\(x_0\\) es un valor conocido del factor controlable \\(X\\). Adicionalmente, podemos calcular intervalos de confianza e intervalos de predicción para \\({E[Y|X=x_0]}\\). Observe que para un valor específico de \\(X\\), digamos \\(x_i\\), se tiene el valor correspondiente de \\(Y\\) es \\(y_i\\). En otras palabras, la \\(i\\)-ésima unidad experimental se produjo cuando \\(X = x_i\\) y que, bajo estas condiciones, se obtuvo que \\(Y = y_i\\). En R la función clave paara ajustar modelos de RLS es lm. Para mayor información, puede consultar la ayuda de la función escribiendo ?lm en la consola, o en la página oficial. En general, la sintaxis es modelo &lt;- lm(y ~ x, data = datos) donde y es la variable respuesta, x es el factor controlable o variable independiente y datos es el objeto en R que contiene los datos con los que estamos trabajando. Para los datos de las soldaduras se tiene que: ## ajuste del modelo de RLS (modelo &lt;- lm(Resistencia ~ Edad, data = datos)) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Coefficients: ## (Intercept) Edad ## 15.7427 -0.2438 Los resultados indican que \\(\\widehat{\\beta}_0 = 15.74\\) y \\(\\widehat{\\beta}_1 = -0.244\\). Por lo tanto, el modelo ajustado puede escribirse como: \\[\\begin{align} \\widehat{\\text{Resistencia}}_i &amp;= 15.74 -0.244\\,\\text{Edad}_i ,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] Cómo interpretamos \\(\\widehat{\\beta}_0\\) y \\(\\widehat{\\beta}_1\\)? Se espera que, por cada semana que transcurre, la resistencia de la soldadura disminuya \\(0.244\\) psi. Una soldadura nueva \\((\\text{Edad} = 0)\\) tiene una resistencia promedio de \\(15.74\\) psi Observe que aún no tenemos el valor estimado de \\(\\sigma\\). Este parámetro del modelo de RLS puede obtenerse como el \\(MSE\\) de la tabla de análisis de varianza o Tabla ANOVA, que describiremos a continuación. 2.3 Tabla ANOVA y medidas de desempeño La Tabla ANOVA es útil para estimar la varianza del modelo de RLS, \\(\\sigma^2\\); calcular el coeficiente de determinación \\(R^2\\); verificar la bondad de ajuste del modelo. En el modelo de RLS, la varianza total de la respuesta puede descomponerse como \\[SST = SSR + SSE\\] donde \\[\\begin{align} SSR &amp;= \\text{ varianza atribuible al factor controlable } x\\\\ SSE &amp;= \\text{ varianza atribuible a factores no controlables} \\end{align}\\] Usualmente, el término \\(SSR\\) se refiere a cuánto de la varianza observada en \\(Y\\) puede ser explicada por el factor controlable \\(x\\). Por otro lado, el término \\(SSE\\) corresponde a cuánto de la varianza de \\(Y\\) puede explicarse a través de otros factores que, desafortunadamente, no fueron controlados ni medidos durante la toma de la muestra. Si el modelo es suficientemente bueno, \\(SSE\\rightarrow 0\\). A partir de la muestra, \\[\\begin{align} SST &amp;= \\sum_{i=1}^n(y_i-\\bar{y})^2, \\\\ SSR &amp;= \\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2=\\widehat{\\beta}_1S_{xy}, \\\\ SSE &amp;= L = \\sum_{i=1}^n\\widehat{\\epsilon}_i^2 = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\ \\widehat{y}_i &amp;= \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i \\end{align}\\] Estimación de \\(\\sigma^2\\) A partir de los resultados una vez es estima el modelo, \\[\\widehat{\\sigma}^2 = \\frac{L}{n-2} = \\frac{SSE}{n-2} = MSE\\] El término \\(MSE\\) se obtiene de la Tabla ANOVA. Al igual a como ocurre con el \\(SSE\\), el \\(MSE\\rightarrow 0\\) para un modelo de RLS ideal. Aunque esto es deseable, es difícilmente observable en la práctica debido a las innumerables fuentes de variabilidad de los procesos de producción y de servicios. En R, el valor de Residual Standard Error, \\(RSE\\), calculado como \\(RSE = \\sqrt{MSE} = \\hat{\\sigma}\\) puede obtenerse como: ## estimación de sigma summary(modelo)$sigma ## [1] 0.5908683 A partir de etse resultado se concluye que \\(\\hat{\\sigma} = 0.591\\). Por lo tanto, \\(MSE = 0.591^2 = 0.349\\). Finalmente, el modelo ajustado puede expresarse como \\[\\begin{align} \\widehat{\\text{Resistencia}}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2),\\\\ \\hat{\\mu}_i &amp;\\sim 15.743 - 0.244\\,\\text{Edad}, \\\\ \\hat{\\sigma}^2 &amp;= 0.591^2 = 0.349. \\end{align}\\] Inferencia para \\(\\sigma^2\\) A partir \\(\\widehat{\\sigma}^2\\) es posible probar si \\[\\begin{align} H_0: \\sigma^2 = \\sigma^2_0 \\\\ H_1: \\sigma^2 \\neq \\sigma^2_0 \\end{align}\\] donde \\(\\sigma^2_0 &gt; 0\\) es un valor específico para \\(\\sigma^2\\). El resultado básico es \\[\\frac{(n-2)\\,\\widehat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2_{n-2} \\] Así, un intervalo de confianza del \\((1-\\alpha)100\\%\\) para \\(\\sigma^2\\) está dado por: \\[\\sigma^2 \\in (a, b)\\] donde \\[a = \\frac{(n-2)\\,\\widehat{\\sigma}^2}{\\chi^2_{1-\\alpha/2,n-2}}, \\quad \\quad b = \\frac{(n-2)\\widehat{\\sigma}^2}{\\chi^2_{\\alpha/2,n-2}} \\] Finalmente, diremos que \\(\\sigma^2 \\neq \\sigma^2_0\\) si el valor \\(\\sigma^2_0\\) no se encuentra en el intervalo \\((a,b)\\). En R todo este procedimiento puede hacerse de la siguiente manera: ## intervalo de confianza para sigma^2 alpha &lt;- 0.05 resultados &lt;- summary(modelo) sigma &lt;- resultados$sigma df &lt;- resultados$df[2] upper_limit &lt;- df*sigma^2/qchisq(alpha/2, df) lower_limit &lt;- df*sigma^2/qchisq(1-alpha/2, df) c(lower_limit = lower_limit, sigma2_hat =sigma^2, upper_limit = upper_limit) ## lower_limit sigma2_hat upper_limit ## 0.2688068 0.3491253 0.4719150 Finalmente, con una confianza del 95%, \\(\\sigma^2 \\in(0.269, 0.472)\\). Ahora, como el valor \\(0\\) no se encuentra en el intervalo de confianza, podemos afirmar que \\(\\sigma^2 &gt; 0\\) a nivel poblacional. Para obtener un intervalo de confianza del 95% para \\(\\sigma\\), basta calcular la raíz cuadrada de estos límites. Por lo tanto, \\(\\sigma \\in(0.519, 0.687)\\). Coeficiente de determinación \\(R^2\\) En el modelo de RLS, una de las medidas más utilizadas para evaluar su desempeño o ajuste es el coeficiente de determinación \\(R^2\\). Matemáticamente, el \\(R^2\\) puede calcularse como \\[R^2 = \\widehat{\\rho}_{XY}^2,\\] donde \\(\\widehat{\\rho}_{XY}\\) es el coeficiente de correlación muestral entre \\(X\\) e \\(Y\\), es un indicador de la calidad del modelo. En nuestro caso, una forma de hacerlo en R es ## coeficiente de determinación rho_est &lt;- with(datos, cor(Resistencia, Edad)) rho_est^2 ## [1] 0.7792861 En la práctica, el \\(R^2\\) puede interpretarse como el porcentaje de variabilidad total de la respuesta \\(Y\\) que puede ser explicado por el modelo de RLS. Utilizando el objeto en R que contiene el modelo ajustado, podemos hacer ## obtención del R^2 a partir de un objeto lm summary(modelo)$r.squared ## [1] 0.7792861 para obtener el coeficiente de determinación. Por lo tanto, podemos concluir, con base en nuestro modelo estimado de RLS, que la Edad de la soldadura explica alrededor del 78% de la varabilidad total de la Resistencia. A partir de esta definición y los valores de \\(SST\\), \\(SSR\\) y \\(SSE\\) es fácil llegar a que \\[R^2 = 1-\\frac{SSE}{SST}\\] Técnicamente, \\(0\\leq R^2 \\leq 1\\), donde \\(R^2 \\rightarrow 0\\) y \\(R^2 \\rightarrow 1\\) representan modelos de RLS con desempeños deficientes e ideales, respectivamente. Sin embargo, en el Capítulo 3 estudiaremos otras medidas que nos permiten determinar si nuestro modelo es lo suficientemente bueno. Al final, el objetivo es seleccionar el mejor modelo de regresión que nos permite explicar, correctamente, una variable respuesta de interés en función de una o más variables explicativas o controlables. Validación del modelo de RLS Al utilizar RLS queremos estar seguros de que el modelo ajustado es considerablemente mejor que no tener dicho modelo. Esto equivale a determinar si la contribución del factor controlable \\(X\\) para predecir \\(E[Y|X=x_0]\\) es sustancial. Formalmente, esto equivale a realizar una prueba de significancia global. La prueba de significancia global puede realizarse de dos maneras en RLS: Utilizando los resultados de la Tabla ANOVA; A través de un procedimiento de pruebas de hipótesis basado en la distribución \\(t\\) de Student. En términos generales, la prueba de significancia global equivale a probar \\[ \\begin{split} &amp;H_0: \\beta_1 = 0 \\\\ &amp;H_A: \\beta_1 \\neq0 \\end{split} \\] Cuando utilizamos la Tabla ANOVA, el estadístico de prueba es \\[F_\\text{calculado} = \\frac{MSR}{MSE} \\sim F_{1, n-2}\\] donde \\(F_{1, n-2}\\) corresponde a la distribución \\(F\\) con \\(1\\) grado de libertad en el numerador y \\(n-2\\) grados de libertad en el denominador. El estadístico \\(F_\\text{calculado}\\) contrasta qué tanto de la varianza de \\(Y\\) puede explicarse con el factor controlable \\(X\\) versus los factores incontrolables del proceso. \\(F_\\text{calculado} &lt;&lt;&lt; 1\\) indica que los factores incontrolables explican más que el factor controlable \\(X\\). Si este fuera el caso, deberíamos explorar otro factor \\(X\\) para explicar \\(Y\\). Cuando \\(F_\\text{calculado} \\approx 1\\), la varianza de \\(Y\\) explicada por el factor controlable y los factores incontrolables es similar. En esta situación, lo mejor es explorar otro factor \\(X\\) para explicar \\(Y\\). Cuando \\(F_\\text{calculado} &gt;&gt;&gt; 1\\) concluimos que la variabilidad de la respuesta puede ser explicada mayormente por el factor controlable que por aquellos **factores incontrolables*. Este es el caso ideal. Para un nivel de significancia \\(\\alpha \\in (0,1)\\), rechazamos \\(H_0\\) si \\(F_\\text{calculado} &gt; F_{1-\\alpha,1,n-1}\\), donde \\(F_{\\alpha/2,1,n-2}\\) es el percentil \\(1-\\alpha\\) de una distribución \\(F\\) con 1 y \\(n-2\\) grados de libertad. En R, estos percentiles pueden calcularse utilizando la función qf. Por ejemplo, \\(F_{0.95,1,98}\\) es 3.94 y se obtiene como qf(0.95, 1, 98). El valor \\(p\\), calculado como \\(p=P(F_{1,n-1}&gt;F_\\text{calculado})\\), es menor que \\(\\alpha\\). Por ejemplo, si \\(F_\\text{calculado} = 7.34\\), el valor \\(p\\) para \\(n=49\\) será 0.009. Este valor se obtiene con la función pf() haciendo pf(7.34, 1, 49, lower.tail = FALSE) La prueba de significancia global se realiza de manera automática cuando empleamos usamos summary(modelo). La parte relevante de los resultados corresponde a la última línea de la salida del R, es decir, a F-statistic: 346 on 1 and 98 DF, p-value: &lt; 2.2e-16 En este caso, F-statistic corresponde al estadístico de prueba \\(F_\\text{calculado}\\), y los valores 1 y 98 a los grados de libertad del \\(MSR\\) y \\(MSE\\), respectivamente. Finalmente, el valor \\(p\\) es \\(p=2.2\\times10^{16}\\), por lo que rechazamos \\(H_0\\) y concluimos que incluir el modelo de RLS que incluye la Edad para explicar la Resistencia de una soldadura es mejor que no tener un modelo de sólo intercepto, es decir, un modelo que no incluye ningún factor controlable que probablemente modifique los valores de dicha Resistencia. Cuando utilizamos la distribución \\(t\\) de Student, podemos probar \\[ \\begin{split} &amp;H_0: \\beta_1 = 0 \\\\ &amp;H_A: \\beta_1 \\neq0 \\end{split} \\] A partir de las propiedades de los coeficientes estimados \\((\\widehat{\\beta}_0,\\widehat{\\beta}_1)\\) y bajo condiciones de regularidad, es fácil llegar a que \\[\\begin{align} \\widehat{\\beta}_0 \\sim N(\\beta_0, \\widehat{\\sigma}_{\\widehat{\\beta}_0}^2) \\\\ \\widehat{\\beta}_1 \\sim N(\\beta_1, \\widehat{\\sigma}_{\\widehat{\\beta}_1}^2) \\end{align}\\] donde \\[\\begin{align} \\widehat{\\sigma}_{\\widehat{\\beta}_0}^2 &amp;= \\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n}+\\frac{\\bar{x}}{SS_x}\\right)} \\\\ \\widehat{\\sigma}_{\\widehat{\\beta}_1}^2 &amp;=\\sqrt{\\frac{\\hat{\\sigma}^2}{SS_x}} \\end{align}\\] Así, el estadístico de prueba será \\[t_{\\text{calculado}} = \\frac{\\hat{\\beta}_1}{\\widehat{\\sigma}_{\\widehat{\\beta}_1}}\\sim t_{n-2}\\] donde \\(t_{n-2}\\) corresponde a la distribución \\(t\\) con \\(n-2\\) grados de libertad. Bajo \\(H_0\\), \\(t_{\\text{calculado}} \\sim t_{n-2}\\). Para un nivel de significancia \\(\\alpha \\in (0,1)\\), rechazamos \\(H_0\\) si \\(|t_\\text{calculado}| &gt; t_{1-\\alpha/2, n-2}\\), donde \\(t_{1-\\alpha/2, n-2}\\) es el percentil \\(1-\\alpha/2\\) de una distribución \\(t\\) con \\(n-2\\) grados de libertad. En R, estos percentiles pueden calcularse utilizando la función qt. Por ejemplo, \\(t_{0.95,98}\\) es 1.66 y se obtiene como qt(0.95, 98). El valor \\(p\\), calculado como \\(p=2\\,P(t_{n-2} &gt; |t_\\text{calculado}|\\), es menor que \\(\\alpha\\). Por ejemplo, si \\(t_\\text{calculado} = 2.54\\) cuando \\(n=60\\), el valor \\(p\\) será 0.014. Para calcularlo, usamos 2*pt(abs(2.54), 58, lower.tail = FALSE) en R. Cuando \\(n-2 &gt; 30\\), podemos aproximar la distribución \\(t_{n-2}\\) a la distribución Normal estándar. En R, utilizaríamos la pnorm() en lugar de pt(). Para más detalles, escriba ?pnorm en la consola del R. Inferencia para \\(\\beta_0\\) y \\(\\beta_1\\) A partir de las distribuciones muestrales de \\(\\widehat{\\beta}_0\\) y \\(\\widehat{\\beta}_1\\) es posible realizar inferencia sobre \\(\\beta_0\\) y \\(\\beta_1\\). Este trabajo inferencial puede hacerse a través de: Pruebas de hipótesis Intervalos de confianza Cuando utilizamos pruebas de hipótesis, se prueban \\(H_0:\\beta_0 = 0\\) vs. \\(H_1:\\beta_0 \\neq 0\\) usando el estadístico de prueba \\[t_{\\text{calc}} = \\frac{\\hat{\\beta}_0-0}{\\widehat{\\sigma}_{\\widehat{\\beta}_0}}\\] \\(H_0:\\beta_1 = 0\\) vs. \\(H_1:\\beta_1 \\neq 0\\) usando el estadístico \\[t_{\\text{calc}} = \\frac{\\hat{\\beta}_1-0}{\\widehat{\\sigma}_{\\widehat{\\beta}_1}}\\] y, en cualquiera de los dos casos, rechazamos \\(H_0\\) si \\(|t_{\\text{calculado}}| &gt; t_{1-\\alpha/2,n-2}\\). Los intervalos de confianza del \\((1-\\alpha)100\\%\\) para los coeficientes de represión serán \\[\\begin{align} {\\beta}_0 \\in (\\hat{\\beta}_0 - t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_0},\\quad \\hat{\\beta}_0 + t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_0})\\\\ {\\beta}_1 \\in (\\hat{\\beta}_1 - t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_1},\\quad \\hat{\\beta}_1 + t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_1}) \\end{align}\\] Decimos que, a nivel poblacional, \\(\\beta_0\\) y \\(\\beta_1\\) son estadísticamente cero si el intervalo de confianza contienen dicho valor. En nuestro ejemplo, podemos obtener los estadísticos de prueba para \\(\\beta_0\\) y \\(\\beta_1\\) utilizando el objeto modelo que contiene los resultados del modelo de RLS: ## estadísticos de prueba para beta_0 y beta_1 coefficients(summary(modelo)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.7427499 0.17868164 88.10502 3.989520e-95 ## Edad -0.2437636 0.01310455 -18.60144 6.394519e-34 En este caso, los estadístos de prueba se encuentran en la columna t value de la salida del R. Así, el estadístico para \\(\\beta_0\\) es \\(t_0=88.1\\) y el estadístico para \\(\\beta_1\\) es \\(t_1 = -18.6\\). Por otro lado, los valores \\(p\\) se encuentran en la columna Pr(&gt;|t|) y son, respectivamente, \\(3.98\\times 10^{-95}\\) y \\(6.39\\times 10^{-34}\\). Puesto que ambos valores \\(p\\) son inferiores a 0.05, concluimos que ambos parámetros son significativos (i.e., son diferentes de cero) a nivel poblacional con una confianza del 95%. Para el mismo nivel de significancia, los intervalos de confianza pueden calcularse como en R a través de la función confint.default: ## intervalos de confianza via confint confint.default(modelo) ## 2.5 % 97.5 % ## (Intercept) 15.392540 16.0929595 ## Edad -0.269448 -0.2180791 Por lo tanto, con una confianza del 95%, \\[\\begin{align} \\beta_0 &amp;\\in(15.39, 16.09) \\\\ \\beta_1 &amp;\\in(-0.269, -0.218) \\end{align}\\] También es posible construir intervalos de confianza para los coeficientes del modelo utilizando la función confint. Sin embargo, estos difieren levemente de aquellos basados en la distribución \\(t\\) o los construidos con la función confint.default. Esto se debe a que confint utiliza likelihood profiling. 2.4 Análisis de Residuales El análisis de residuales es fundamental en RLS puesto que es posible establecer si: Los errores del modelo ajustado cumplen con los supuestos; existen observaciones atípias y/u observaciones influenciales. A continuación se describen las estrategias utilizadas para establecer 1 y 2. Validación de Supuestos En la formulación matemática del modelo, se estableció que \\[\\epsilon\\sim N(0, \\sigma^2), \\hspace{1cm} \\sigma^2 = \\text{constante},\\] y \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\) son independientes. Por lo tanto, cuando se ajusta un modelo de RLS, debemos validar los siguientes supuestos sobre el error: Independencia; Normalidad; Media cero; y Varianza \\(\\sigma^2\\) constante. Sólo cuando se han validado todos los supuestos del error, podemos proceder a realizar predicción de futuros valores de la variable respuesta con el modelo de RLS ajustado. Independencia Para validar el supuesto de independencia de los errores se recomiendan dos aproximaciones: la prueba formal de independencia de Durbin-Watson y la prueba gráfica basada en la Función de Autocorrelación (ACF en inglés). La prueba de Durbin-Watson está implementada en la función durbinWatsonTest del paquete car. La ACF puede obtenerse utilizando la función acf. Para más información escriba ?acf en la consola del R. En R podemos realizar la prueba de Durbin-Watson haciendo: ## prueba de Durbin-Watson require(car) car:::durbinWatsonTest(modelo) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.02653343 2.050609 0.786 ## Alternative hypothesis: rho != 0 Basados en el valor \\(p\\), no es posible rechazar \\(H_0\\) puesto que el valor \\(p\\) de la prueba de Durbin-Watson es superior a un nivel de significancia \\(\\alpha\\) del 5%. Por lo tanto, concluimos que los residuales del modelo ajustados son independientes. Para graficar la ACF podemos hacer: ## ACF de los residuales r &lt;- residuals(modelo) ## cálculo de los residuales acf(r, las = 1, main = &quot;&quot;) Para concluir que los residuales del modelo ajustado son independientes basados en la ACF, ninguna de las barras verticales, llamadas correlaciones, debe superar las bandas de color azul para \\(\\text{Lag} &gt; 0\\). En nuestro caso, podemos concluir que los residuales son independientes. Normalidad Formalmente, probar el supuesto de normalidad de los errores equivale a realizar el siguiente procedimiento de prueba de hipótesis: \\[ \\begin{split} &amp;H_0: \\text{los errores siguen una distribución Normal.} \\\\ &amp;H_A: \\text{los errores NO siguen una distribución Normal.} \\end{split} \\] Este supuesto puede validarse utilizando pruebas formales, o de manera gráfica utilizando histogramas, gráficos de densidad o un gráfico cuantil-cuantil, también conocido como Q-Q plot. Existen diferentes pruebas de Normalidad, varias de ellas implementadas en R. Prueba Shapiro-Wilk, implementada en la función shapiro.test. Prueba Anderson-Darling, implementada en la función ad.test del paquete nortest. Prueba Cramer-von Mises, implementada en la función cvm.test del paquete nortest. Prueba Lilliefors (Kolmogorov-Smirnov), implementada en la función lillie.test del paquete nortest. Prueba Pearson basada en la distribución \\(\\chi^2\\), implementada en la función pearson.test del paquete nortest. Prueba Shapiro-Francia, implementada en la función sf.test del paquete nortest). El Q-Q plot compara los cuantiles teóricos de la distribución Normal con los cuantiles muestrales del error estimado. Existen también varias implementaciones del gráfico cuantil-cuantil en R, entre las que se encuentran la función qqnorm del paquete base y la función qqPlot del paquete car. Para realizar por ejemplo la prueba de Normalidad de Shapiro-Wilk sobre los residuales del modelo ajustado, podemos proceder de la siguiente forma utilizando el objeto R que contiene los resultados de dicho modelo: ## prueba de Normalidad de Shapiro-Wilk shapiro.test(r) ## ## Shapiro-Wilk normality test ## ## data: r ## W = 0.98872, p-value = 0.5629 Estos resultados indican que el valor \\(p\\) de la prueba de Normalidad es \\(p = 0.563\\). Como \\(p &gt; 0.05\\), no rechazamos \\(H_0\\) y concluimos que los residuales del modelo ajustado siguen una distribución Normal. En R, podemos graficar un Q-Q plot de 2 formas: Vía gráficos básicos ## Q-Q plot básico qqnorm(r, las = 1, main = &quot;&quot;) qqline(r, col = 2) Para más detalles, escribir ?qqnorm en la consola del R. La idea fundamental es que los residuales del modelo (i.e., puntos en la gráfica) deben estar alrededor de la línea de color rojo para concluir, al menos gráficamente, que los residuales siguen una distribución Normal. En este caso, existen varios puntos por fuera de la línea, lo cual nos hace sospechar que esas observaciones outliers afectan el supesto. Utilizando el paquete car ## Q-Q plot usando el paquete car library(car, quietly = TRUE) car:::qqPlot(r, las = 1) ## [1] 77 18 Aunque la elaboración de este gráfico es mucho más compleja, su interpretación es simple: diremos que los residuales del modelo ajustado cumplen con el supuesto de Normalidad si todos los puntos en encuentran dentro de las bandas de color azul. En este caso, las observaciones 18 y 77 parecen ser extremas. Media cero Formalmente la validación del supuesto de media cero para los errores consiste en probar \\[ \\begin{split} &amp;H_0: \\mu_\\epsilon=0 \\\\ &amp;H_A: \\mu_\\epsilon\\neq0 \\end{split} \\] donde \\(\\mu_\\epsilon\\) es la media poblacional de \\(\\epsilon\\). Teniendo en cuenta que los coeficientes del modelo de RLS ajustado se obtienen a partir de MCO, no es necesario validar este supuesto. Básicamente, el método de MCO garantiza no rechazar \\(H_0\\). En caso de que se requiera hacerlo, podemos proceder de la siguiente manera en R: ## validación media cero en RLS t.test(r, mu = 0) ## ## One Sample t-test ## ## data: r ## t = -3.6292e-16, df = 99, p-value = 1 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.1166475 0.1166475 ## sample estimates: ## mean of x ## -2.133551e-17 De este resultado es fácil ver que el valor \\(p\\) de la prueba es \\(p=1\\), por lo que no rechazamos H_0. Finalmente concluimos, con una confianza del 95%, que la media de los residuales es cero a nivel poblacional. Varianza constante Por último, pero no menos importante, debemos validar el supuesto de varianza constante. Formalmente esto es equivalente a probar \\[ \\begin{split} &amp;H_0:\\sigma^2 \\text{ es constante}\\\\ &amp;H_1:\\sigma^2 \\text{ NO es constante} \\end{split} \\] Esta prueba puede realizarse formalmente a través de diferentes pruebas estadísticas como la prueba de Breusch-Pagan, o utilizando métodos gráficos. La prueba de Breusch-Pagan puede realizarse utilizando la función ncvTest del paquete car. Los métodos gráficos consisten en construir gráficos de dispersión para \\(\\hat{y}\\) vs. \\(y\\), \\(\\hat{y}\\) vs. \\(x\\) y \\(\\hat{y}\\) vs. el orden de medición. En el caso ideal, estos gráficos no deberían exhibir ningún patrón. Prueba de Breusch-Pagan Sin entrar en los detalles formales, la prueba de Breusch-Pagan busca determinar, estadísticamente, si la varianza del modelo de RLS ajustado es constante o no. En R podemos realizar esta prueba con a función bptest del paquete lmtest: ## prueba de Breusch-Pagan para varianza constante require(lmtest) lmtest:::bptest(modelo) ## ## studentized Breusch-Pagan test ## ## data: modelo ## BP = 9.7572, df = 1, p-value = 0.001786 Como el valor \\(p\\) de la prueba es mayor que 0.05, concluimos que la varianza del modelo no es costante. Aproximación gráfica Al usar esta aproximación, lo que buscamos es que los gráficos resultantes tenga un patrón aleatorio. En términos generales, podemos hacer gráficos de \\(\\hat{y}\\) vs. \\(x\\); \\(\\hat{y}\\) vs. \\(r\\); y \\(\\hat{y}\\) vs. orden de las observaciones, y evaluar los patrones observados. Esencialmente, buscamos que ocurra el patrón del panel superior izquierdo aquí. En R podemos proceder de la siguiente manera: ## cálculos y_hat &lt;- predict(modelo) # valores predichos de y x &lt;- datos$Edad # seleccionamos el predictor y &lt;- datos$Resistencia # seleccionamos la variable respuesta ## gráfico de residuales vs. valores predichos par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) plot(y_hat, r, las = 1, main = expression(hat(y) * &quot; vs. r&quot;), xlab = expression(hat(y)), ylab = &quot;Residuals&quot;) abline(h = 0, col = 2) plot(y_hat, las = 1, main = expression(hat(y) * &quot; vs. orden&quot;), xlab = &quot;Orden&quot;, ylab = expression(hat(y))) plot(y, r, las = 1, main = expression(&quot;r vs. y&quot;), ylab = &quot;Residuals&quot;) abline(h = 0, col = 2) plot(x, r, las = 1, main = expression(&quot;r vs. Edad&quot;), ylab = &quot;Residuals&quot;) abline(h = 0, col = 2) De los gráficos anteriores se concluye que la varianza del modelo no es constante. 2.5 Predicción Sólo cuando se validan todos los supuestos del modelo es posible realizar predicción. La idea fundamental es poder calcular \\(\\widehat{E[Y|X=x_0]} = \\widehat{\\mu}|_{X=x_0}\\), donde \\(x_0\\) es un valor específico del factor controlable \\(X\\). Adicionalmente, interesa construir intervalos de confianza e intervalos de predicción del \\((1-\\alpha)100\\%\\) para \\(\\mu|_{X=x_0}\\). Estos intervalos son fundamentales para determinar los valores máximos y mínimos del valor esperado de la respuesta \\(Y\\) para valores puntuales de \\(X\\) a nivel poblacional, para una confianza dada. Intervalo de confianza para \\(E[Y|X=x_0]\\) Para \\(\\alpha\\in(0,1)\\), el intervalo de confianza del \\((1-\\alpha)100\\%\\) para \\(\\mu |_{X=x_0}\\) está dado por: \\[\\widehat{\\mu} |_{X=x_0} \\pm t_{\\alpha/2,n-2}\\,\\hat{\\sigma}\\,\\sqrt{\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}}\\] Intervalo de predicción para \\(E[Y|X=x_0]\\) Similarmente, el intervalo de predicción para \\(Y |_{X=x_0}\\) será: \\[\\widehat{\\mu} |_{X=x_0} \\pm t_{\\alpha/2,n-2}\\,\\hat{\\sigma}\\,\\sqrt{1+\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}}\\] Observe que, aunque el intervalo de confianza y el intervalo de predicción están centrados en el mismo valor, el error estándar de \\(\\widehat{\\mu} |_{X=x_0}\\) es mayor en el intervalo de predicción que en el intervalo de confianza, es decir, \\[ \\sqrt{1+\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}} &gt; \\sqrt{\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}} \\] Esto se debe a que, al construir intervalos de predicción, la incertidumbre de \\(\\widehat{\\mu} |_{X=x_0}\\) es mayor para valores de \\(X\\) por fuera de las condiciones de operación evaluadas al recolectar la muestra. Los intervalos de confianza permiten determinar el rango donde se encontrará el valor promedio de \\(Y\\) para un valor específico de \\(X\\) cuando se producen varias unidades experimentales en esas codiciones. Formalmente, esto es \\(\\widehat{\\mu}|_{X=x_0} = \\widehat{E[Y|X=x_0]}\\). Los intervalos de predicción permiten determinar el rango donde se encontrará el próximo valor de \\(Y\\) cuando \\(X=x_0\\). Aunque los residuales del modelo ajustado para los datos de soldadura no cumple con el supuesto de varianza constante, se mostrará cómo predecir la Resistencia de una soldadura con \\(\\text{Edad} = 20\\), es decir, \\(x_0=20\\). Con el objeto modelo, podemos construir el intervalo de confianza del 95% haciendo ## intervalo de confianza del 95% para E[Resistencia|Edad = 20] predict(modelo, newdata = data.frame(Edad = 20), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 10.86748 10.64805 11.08691 Este resultado implica que, si medimos la Resistencia de varias soldaduras con \\(\\text{Edad} = 20\\), se espera que la Resistencia promedio sea 10.87 psi, y que, a nivel poblacional, dicho promedio se encuentre en el intervalo \\((10.65, 11.09)\\) con una confianza del 95%. Finalmente, el intervalo de predicción cuando \\(\\text{Edad} = 20\\) será: ## intervalo de confianza del 95% para Resistencia|Edad = 20 predict(modelo, newdata = data.frame(Edad = 20), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 10.86748 9.674564 12.06039 Por lo tanto, se espera que en una próxima soldadura de \\(\\text{Edad} = 20\\) el valor de Resistencia sea 10.87 psi. A nivel poblacional, dicha Resistencia estará en el intervalo \\((9.67, 12.06)\\) el 95% de las veces. "],["rlm.html", "Capítulo 3 Regresión Lineal Múltiple 3.1 Formulación básica del modelo de RLM 3.2 Propiedades de los estimadores de \\({\\mathbf{\\beta}}\\) 3.3 Estimación de \\(\\sigma^2\\) 3.4 Inferencia para \\(\\mathbf{\\beta}\\) 3.5 Inferencia para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) 3.6 Análisis de Residuales 3.7 Análisis de Multicolinealidad 3.8 Selección de Modelos", " Capítulo 3 Regresión Lineal Múltiple Como se mencionó en el Capítulo 2, los modelos de regresión lineal pueden utilizarse para predecir futuros valores de una variable respuesta continua a partir de valores específicos de las variables controlables del proceso. En la práctica, pueden existir múltiples variables controlables en un proceso de producción o de servicios. Por ejemplo, en un proceso de pintura electrostática, puede ser de interés determinar el espesor de la capa de pintura (variable respuesta \\(y\\), en micrones) con la que se recubre una lámina de área determinada, a partir de valores conocidos de la presión de aire (variable \\(x_1\\) en psi) y la velocidad de la banda transportadora (variable \\(x_2\\) en m/s) en la que se desplaza dicha lámina. En este caso, el interés es: Determinar la magnitud de la influencia de las variables \\(x_1\\) y \\(x_2\\) sobre el espesor de capa esperado; construir una función \\(f(x_1,x_2)\\) que permita predecir el espesor de capa esperado; y construir intervalos de confianza y predicción para dicho valor. Si la variable respuesta \\(y\\) es continua y aproximadamente simétrica, podemos desarrollar 1, 2 y 3 a partir de la estimación de un modelo de regresión lineal. Puesto que el número de variables controlables es \\(k&gt;1\\), una posibilidad es utilizar el modelo de Regresión Lineal Múltiple (RLM). El modelo de RLM es una extensión del modelo de RLS cuando se tiene más de una variable controlable. 3.1 Formulación básica del modelo de RLM Matemáticamente, el modelo de RLM puede expresarse como: \\[\\begin{align} \\label{mod1} y_i &amp;= \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_1X_{ki} + \\epsilon_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] Este modelo es equivalente a \\[\\begin{align} \\label{mod2} \\mathbf{y} &amp;= \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon} \\end{align}\\] donde \\(\\mathbf{y} = (y_1,y_2,\\ldots,y_n)\\) es el vector respuesta, \\(\\mathbf{X} = (\\mathbf{1}, \\mathbf{x}_1, \\mathbf{x}_2,\\ldots,\\mathbf{x}_k)_{n\\times p}\\) es la matriz de diseño y \\(\\mathbf{\\epsilon} = (\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n)\\) es el error aleatorio. 3.1.1 Estimación Similar a RLS, la estimación del modelo de RLM se realiza utilizando el método de mínimos cuadrados ordinarios (MCO). A partir de una muestra aleatoria de tamaño \\(n\\) del proceso de producción, los datos se registran en una estructura rectangular similar a: Figura 3.1: Estructura de datos en RLM. De esta forma, se tienen \\(n\\) unidades experimentales para cada una de estas se determina el valor de la variable respuesta \\(y_i\\) para condiciones fijas \\(\\mathbf{X}_i\\). Por ejemplo, para la quinta unidad experimental, se obtuvo el valor \\(y_6\\) cuando las variables controlables tomaron los valores fijos \\((x_{1,6}, x_{2,6}, \\ldots, x_{k,6})\\). Al igual que en RLS, la estimación del modelo de RLM realiza utilizando minimos cuadrados La idea fundamental consiste en minimizar \\[\\begin{eqnarray*}\\label{L} L &amp;=&amp;\\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_{1,i} -\\beta_2X_{2,i} - \\ldots -\\beta_kX_{k,i} )^2. \\end{eqnarray*}\\] Los estimadores de mínimos cuadrados deben satisfacer las siguientes dos condiciones fundamentales: \\[\\begin{eqnarray*} \\frac{\\partial L}{\\partial \\beta_0} | _{\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots \\hat{\\beta}_k} &amp;=&amp; -2\\sum_{i=1}^n \\left(y_i-\\hat{\\beta}_0-\\sum_{j=1}^k{\\hat{\\beta}_jx_{ij}}\\right) = 0 \\\\ \\frac{\\partial L}{\\partial \\beta_j} | _{\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots \\hat{\\beta}_k} &amp;=&amp; -2\\sum_{i=1}^n \\left(y_i-\\hat{\\beta}_0-\\sum_{j=1}^k{\\hat{\\beta}_jx_{ij}}\\right)x_{ij} = 0 \\end{eqnarray*}\\] La solución al sistema de ecuaciones de condiciones fundamentales da origen al sistema de ecuaciones normales de mínimos cuadrados dado por Figura 3.2: Ecuaciones normales de mínimos cuadrados. Las solución de estas ecuaciones permite determinar \\(\\hat{\\mathbf{\\beta}}\\). Es fácil llegar a que el vector de coeficientes estimado para el modelo de RLM puede obtenerse como \\[ \\hat{\\mathbf{\\beta}} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime \\mathbf{y} \\] Finalmente, el modelo estimado es \\[\\hat{y}_i = \\hat{\\beta}_0+\\sum_{j=1}^k\\hat{\\beta}_jx_{ij},\\] que, matricialmente, puede representarse como \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\mathbf{\\beta}}\\] A partir del modelo ajustado, un valor específico \\(y_i\\) puede calculase como: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2+\\cdots+\\hat{\\beta}_kx_k\\] Como ilustración, consideremos los siguientes datos provenientes de un proceso de pintura electrostática: ## lectura de datos url &lt;- &#39;https://www.dropbox.com/s/st5xk1prkxg1pj4/datalab2.txt?dl=1&#39; datos &lt;- read.table(url, header = TRUE) head(datos) ## y x1 x2 ## 1 91.88308 18.722091 6 ## 2 94.37544 19.056131 3 ## 3 74.63026 9.292093 3 ## 4 93.66348 17.456714 4 ## 5 76.84981 14.626183 9 ## 6 76.05198 12.786439 9 En este caso, la variable respuesta \\(y\\) representa el espesor de la capa de pintura en micrones, \\(x_1\\) es la presión de aire en psi y \\(x_2\\) es la velocidad de la banda transportadora. Una forma de comenzar a establecer si existe relación lineal entre \\(y\\) y las variables \\(x_1\\) y \\(x_2\\) es a través de una red, como se muestra a continuación: ## matriz de correlación require(IsingSampler) require(qgraph) corMat &lt;- cor(datos) par(mfrow = c(1, 1), mar = c(0.1, 0.1, 0.1, 0.1)) qgraph(corMat, graph = &quot;cor&quot;, layout = &quot;spring&quot;, sampleSize = nrow(datos), legend.cex = 1, alpha = 0.05) Figura 3.3: Red de correlación para \\(x_1\\), \\(x_2\\) y \\(y\\). Correlaciones positivas se muestran en verde, y las negativas en rojo. O utilizando una matriz de dispersión: ## pairs plot panel.cor &lt;- function(x, y, digits = 2, prefix = &quot;&quot;, cex.cor, ...) { usr &lt;- par(&quot;usr&quot;) on.exit(par(usr)) par(usr = c(0, 1, 0, 1)) r &lt;- cor(x, y) txt &lt;- format(c(r, 0.123456789), digits = digits)[1] txt &lt;- paste0(prefix, txt) text(0.5, 0.5, txt, cex = 1.5) } pairs(datos, lower.panel = panel.smooth, upper.panel = panel.cor, las = 1) Figura 3.4: Matriz de dispersión para \\(x_1\\), \\(x_2\\) y \\(y\\). Este último gráfico indica que \\(x_1\\) e \\(y\\) están linealmente relacionados y la correlación es \\(\\hat\\rho = 0.89\\), mientras la correlación de \\(y\\) y \\(x_2\\) es \\(\\hat\\rho = -0.32\\). Para otras posibilidades relacionadas con correlogramas, se recomienda revisar las alternativas en R Graph Gallery. Para estimar el modelo de RLM, utilizamos la función lm como se muestra a continuación: ## ajuste del modelo de RLM fit &lt;- lm(y ~ x1 + x2, data = datos) summary(fit) ## ## Call: ## lm(formula = y ~ x1 + x2, data = datos) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.0571 -1.6610 -0.1362 1.5409 8.7008 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.28483 1.10350 60.068 &lt; 2e-16 *** ## x1 1.48275 0.06144 24.132 &lt; 2e-16 *** ## x2 -1.05521 0.13660 -7.725 1.03e-11 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.767 on 97 degrees of freedom ## Multiple R-squared: 0.8716, Adjusted R-squared: 0.8689 ## F-statistic: 329.1 on 2 and 97 DF, p-value: &lt; 2.2e-16 A partir de estos resultados podemos realizar las pruebas de significancia global y marginales tal y como se mostró en el Capítulo 2. 3.2 Propiedades de los estimadores de \\({\\mathbf{\\beta}}\\) Cuando estimamos \\(\\hat{\\mathbf{\\beta}}\\), los cálculos están basados en los resultados obtenidos al tomar una muestra aleatoria de tamaño \\(n\\). Como consecuencia, el valor de los estimadores \\(\\mathbf{\\hat{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k)\\) cambian si cambiamos la muestra. Desde el punto de vista formal, los estimadores \\(\\mathbf{\\hat{\\beta}}\\) cumplen con las siguientes propiedades: Los estimadores \\(\\mathbf{\\hat\\beta}\\) son insesgados. Esta propiedad implica que, al aumentar \\(n\\), el valor de los estimadores de \\(\\mathbf{\\beta}\\) se aproximan a los verdaderos valores de los parámetros. Matemáticamente se tiene que: \\[\\begin{eqnarray} E[\\mathbf{\\hat\\beta}] &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{y}]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime(\\mathbf{X\\beta} + \\mathbf{\\epsilon})]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{X\\beta} + (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{\\epsilon}]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{X\\beta}] + E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{\\epsilon}]\\\\\\nonumber &amp;=&amp; E[\\mathbf\\beta] + \\mathbf{0} = \\mathbf{\\beta}\\nonumber \\end{eqnarray}\\] La varianza de \\(\\hat{\\beta}_j\\) y la covarianza entre \\(\\hat\\beta_{i}\\) y \\(\\hat\\beta_{j}\\) están dadas por: \\[\\begin{eqnarray*} V(\\hat\\beta_{j}) &amp;=&amp; \\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj} \\hspace{0.5cm} 0,1,2,\\ldots,p;\\\\ \\text{cov}(\\hat\\beta_{i}, \\hat\\beta_{j}) &amp;=&amp; \\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{ij} \\hspace{0.5cm} i\\neq j. \\end{eqnarray*}\\] Ahora, a partir de \\(E[\\hat{\\beta}_j]\\) y \\(V(\\hat\\beta_{j})\\), es posible hacer inferencia para el parámetro \\(\\beta_j\\), \\(j=1,2,\\ldots,k.\\) Sin embargo, observe que \\(V(\\hat\\beta_{j})\\) depende de \\(\\sigma^2\\), la varianza del modelo de RLM, que se estima a través del MSE. Los coeficientes del modelo de RLM ajustado para los datos provenientes del proceso de pintura electrostática se obtienen haciendo: ## coeficientes estimados coefficients(fit) ## (Intercept) x1 x2 ## 66.284831 1.482753 -1.055206 A partir de estos coeficientes, el modelo ajustado será \\[\\hat{y} = 66.285 + 1.483x_1 - 1.055x_2\\] Note que esta es la ecuación de un plano en el espacio \\((x_1, x_2, y)\\). Los coeficientes estimados del modelo de RLM se interpretan en términos de una derivada parcial. En particular, si incrementamos \\(x_1\\) en una unidad y mantenemos constante \\(x_2\\), esperaríamos que el espesor de la capa de pintura aumentara, en promedio, \\(\\hat{\\beta}_1 = 1.483\\) micrones. Por otro lado, si incrementamos \\(x_2\\) en una unidad y mantenemos constante \\(x_1\\), se espera que el espesor de la capa de pintura disminuya, en promedio, \\(\\hat{\\beta}_2 = -1.055\\) micrones. 3.3 Estimación de \\(\\sigma^2\\) Similar a como se observó en RLS, en RLM, también se cumple la misma relación que en RLS en cuanto que \\[SST = SSR + SSE \\] Adicionalmente, \\(\\mathbf{\\hat{\\beta}}\\), se tiene que \\[\\begin{eqnarray*}\\label{L2} L &amp;=&amp;\\sum_{i=1}^n\\hat{\\epsilon}_i^2 = \\sum_{i=1}^n(Y_i-\\hat{Y}_i^2)^2 \\\\ &amp;=&amp;\\sum_{i=1}^n(Y_i-\\hat{\\beta}_0-\\hat{\\beta}_1X_{1,i} -\\hat{\\beta}_2X_{2,i} - \\cdots -\\hat{\\beta}_kX_{k,i})^2 \\\\ &amp;=&amp; SSE \\end{eqnarray*}\\] Por lo tanto, \\[\\hat{\\sigma}^2 = \\frac{SSE}{n-p} = MSE\\] donde \\(p = k+1\\) es el número de coeficientes del modelo ajustado. Este resulado indica que la varianza de los errores, también conocida como la varianza del modelo, puede estimarse utilizando el MSE. El MSE se obtiene de la tabla ANOVA que tiene la siguiente forma: Figura 3.5: Tabla ANOVA en RLM. En el caso del modelo de RLM para el proceso de pintura electrostática, podemos obtener el valor de \\(\\hat{\\sigma}^2\\) haciendo: ## estimación de sigma^2 summary(fit)$sigma^2 ## [1] 7.65454 Por lo tanto, el modelo estimado será: \\[ \\begin{eqnarray} \\hat{y} &amp;=&amp; 66.285 + 1.483x_1 - 1.055x_2 \\\\\\nonumber \\epsilon &amp;\\sim&amp; N(0, \\sigma^2) \\\\\\nonumber \\hat{\\sigma}^2 &amp;=&amp; 7.655.\\nonumber \\end{eqnarray} \\] Recordemos que, adicional al MSE, a partir de la tabla ANOVA es posible calcular el porcentaje de variabilidad de la respuesta explicado por el modelo de RLM, también conocido como coeficiente de determinación o, simplemente, como \\(R^2\\): \\[R^2 = SSR/SST = 1 - SSE/SST\\] Puesto que \\(R^2\\) incrementa a medida que el número de variables aumenta, se recomienda utilizar el \\(R^2\\) ajustado en RLM: \\[R^2_{\\text{ajustado}} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\\] La interpretación de esta medida de desempeño es similar a la interpretación de \\(R^2\\) discutida en el Capítulo 2. El ejemplo de la pintura electrostática, \\(R^2_{\\text{adj}}=0.8689\\), por lo que podemos afirmar que incluir las variables \\(x_1\\) y \\(x_2\\) en un modelo de regresión permite explica cerca del 87% de la variabilidad de la respuesta. 3.4 Inferencia para \\(\\mathbf{\\beta}\\) Uno de los propósitos de la inferencia estadística es determinar el valor de los verdaderos parámetros de una población a partir de los resultados obtenidos en una muestra. En este caso, los parámetros poblacionales son \\(\\mathbf{{\\beta}} = (\\beta_0, \\beta_1, \\ldots, \\beta_k)\\), además de \\(\\sigma^2\\). Con los valores muestrales, podemos construir pruebas de hipótesis de dos tipos para los parámetros del modelo de RLM: la prueba de significancia total y las pruebas de significancia marginal. 3.4.1 Prueba de significancia global Esta prueba se utiliza para determinar la significancia total del modelo, es decir, para detemrinar si incluir las variables controlables en el modelo de regresión es mejor que no incluirlas para explicar la respuesta \\(Y\\). La idea fundamental es determinar si, en la población, \\[\\begin{eqnarray} H_0&amp;:&amp; \\beta_1=\\beta_2=\\cdots\\beta_k=0 \\\\\\nonumber H_1&amp;:&amp; \\text{Al menos un $\\beta_j \\neq 0$}\\nonumber \\end{eqnarray}\\] Este procedimiento de prueba de hipótesis se realiza a través de la tabla de ANOVA utilizando el estadístico \\(F\\) dado por \\[F_0 = \\frac{SSR/k}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F_{k, n-p}\\] Rechazamos \\(H_0: \\beta_1=\\beta_2=\\cdots\\beta_k=0\\) si \\(F_0 &gt; F_{\\alpha,k,n-p}\\), donde \\(\\alpha\\in(0,1)\\) es un nivel de significancia predeterminado. Cuando esto ocurre, concluimos que al menos un \\(\\beta_j\\) es estad'isticamente significativo al \\(100(1-\\alpha)\\%\\). Observe que en la prueba de significancia global del modelo, \\(F_{\\text{calc}} = 329.1\\) y \\(p &lt; 2.2\\times 10^{-16}\\). Esto indica que tener este modelo de RLM para explicar el espesor de la capa de pintura es mejor que no tenerlo. 3.4.2 Prueba de significancia marginal Esta prueba se realiza si rechazamos la prueba de significancia global. Lo que intentamos hacer es determinar si, a nivel poblacional, los coeficientes asociados a cada \\(x_j\\) son o diferentes de cero. Esto es equivalente a probar: \\[\\begin{eqnarray*} H_0&amp;:&amp; \\beta_j=0 \\\\\\nonumber H_1&amp;:&amp; \\beta_j \\neq 0\\nonumber \\end{eqnarray*}\\] Para \\(j\\) fijo, el estadístico de prueba es \\[t_j = \\frac{\\hat{\\beta}_j - 0}{\\text{s.e.}(\\hat\\beta_j)} = \\frac{\\hat{\\beta}_j}{\\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}}}\\sim t_{n-p}\\] Por lo tanto, rechazamos \\(H_0\\) con un nivel de significancia de \\(100\\times(1-\\alpha)\\%\\) si \\(|t_j| &gt; t_{\\alpha/2, n-p}\\). Para el problema de la pintura electrostática, tendríamos: ## ajuste del modelo de RLM summary(fit)$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 66.284831 1.10349995 60.067815 1.529216e-78 ## x1 1.482753 0.06144364 24.131927 8.750958e-43 ## x2 -1.055206 0.13659758 -7.724922 1.026117e-11 Según estos resultados, todas las pruebas marginales del tipo \\(H_0: \\beta_j = 0\\) vs. \\(H_1: \\beta_j \\neq 0\\), el valor \\(p\\) es \\(&lt; 0.05\\). Por lo tanto, la magnitud de la influencia de cada variable controlable sobre \\(E[y|x_1, x_2]\\) es estadísticamente diferente de cero a nivel poblacional. En otras palabras, controlando las variables \\(x_1\\) y \\(x_2\\) en el proceso, permitiría modificar satisfactoriamente el espesor de la capa de pintura. 3.4.3 Intervalos de confianza para \\(\\beta_j\\) Otra forma de realizar inferencia para \\(\\mathbf{\\beta}\\) es a través de la construcción de intervalos de confianza del \\(100\\times(1-\\alpha)100\\%\\). Es fácil mostrar que, para \\(j\\) fijo, \\[\\begin{equation}\\label{eq:icbeta} \\beta_j\\in \\left( \\hat{\\beta}_j - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}}, \\hat{\\beta}_j + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}} \\right) \\end{equation}\\] Otra alternativa para construir intervalos de confianza es vía bootstrap o likelihood profiling. Finalmente concluimos, con un nivel de confianza del \\(100\\times(1-\\alpha)\\%\\), que \\(\\beta_j\\) está en el intervalo anterior. Uan forma de construir los intervalos de confianza para los coeficientes del modelo es utilizando la función confint.default: ## intervalos de confianza del 95% para los coeficientes confint.default(fit) ## 2.5 % 97.5 % ## (Intercept) 64.122011 68.4476513 ## x1 1.362326 1.6031807 ## x2 -1.322932 -0.7874793 Por lo tanto, con una confianza del 95%, \\[\\begin{align} \\beta_0 &amp;\\in(64.12, 68.45) \\\\ \\beta_1 &amp;\\in(1.36, 1.60) \\\\ \\beta_2 &amp;\\in(-1.323, -0.787). \\end{align}\\] 3.5 Inferencia para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) El modelo de RLM ajustado puede utilizarse para predecir \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) sólo si se validan todos los supuestos. Para más detalles ver Análisis de Residuales. 3.5.1 Intervalos de confianza para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) A partir del modelo ajustado y para valores fijos de las variables controlables, digamos \\(\\mathbf{x}_0\\), se tiene que \\[\\begin{eqnarray} \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0} &amp;=&amp; \\hat{E[\\mathbf{Y} | \\mathbf{x}_0]} = \\mathbf{x}_0^\\prime\\hat{\\mathbf{\\beta}} \\\\\\nonumber V[\\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}] &amp;=&amp; \\hat{\\sigma}^2\\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0 \\end{eqnarray}\\] Finalmente, el intervalo de confianza del \\(100\\times(1-\\alpha)\\%\\) puede calcularse como \\[\\begin{eqnarray} \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0} \\pm t_{\\alpha/2,n-p}\\sqrt{\\hat{\\sigma}^2\\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0} \\end{eqnarray}\\] Supongamos que queremos determinar el espesor de la capa de pintura para las condiciones \\(\\mathbf{x}_0 = (10, 8)\\), es decir, cuando \\(x_1 = 10\\) y \\(x_2 = 8\\). En R procedemos de la siguiente manera: ## cálculo de E[y|x_1 = 10, x_2 = 8] predict(fit, newdata = data.frame(x1 = 10, x2 = 8)) ## 1 ## 72.67072 Si trabajamos bajo las condiciones \\(\\mathbf{x}_0\\), se espera que, en promedio, el espesor de la capa de pintura sea 72.67 micrones. Para otros argumentos y opciones, se sugiere al lector escribir ?predict.lm en la consola de R. Ahora, si es de interés calcular un intervalo de confianza del 95%, agregamos el argumento interval = 'confidence' a la instrucción anterior: ## cálculo de E[y|x_1 = 10, x_2 = 8] predict(fit, newdata = data.frame(x1 = 10, x2 = 8), interval = &#39;confidence&#39;) ## fit lwr upr ## 1 72.67072 71.64637 73.69507 Por lo tanto, se espera que, si continuamos trabajando bajo las condiciones \\(x_1 = 10\\) y \\(x_2 = 8\\), el espesor de capa promedio sea 72.67 micrones. A nivel poblacional, dicho promedio se encontrará en el intervalo \\((71.646, 73.695)\\) con una confianza del 95%. 3.5.2 Intervalos de predicción para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) Sea \\(\\hat{y}_0 = \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}\\), donde \\(\\mathbf{x}_0\\) es el vector de covariables futuro. Un intervalo de predicción del \\(100\\times(1-\\alpha)\\%\\) para \\(Y_0\\) está dado por: \\[\\begin{eqnarray} \\hat{y}_0 \\pm t_{\\alpha/2,n-p}\\sqrt{\\hat{\\sigma}^2(1 + \\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0)} \\end{eqnarray}\\] Otra posibilidad para construir dicho intervalo es vía bootstrap. Observe que lo único que cambia en este intervalo en relación con el intervalo de confianza es la varianza de \\(Y_0\\) \\(-\\) existe más incertidumbre. Observe que en el intervalo de predicción estamos interesados en \\(Y_0 | \\mathbf{x}_0\\) y no \\(\\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}\\). Para calcular un intervalo de predicción del 95%, agregamos el argumento interval = 'prediction' a la instrucción anterior: ## intervalo de predicción para E[y|x_1 = 10, x_2 = 8] predict(fit, newdata = data.frame(x1 = 10, x2 = 8), interval = &#39;prediction&#39;) ## fit lwr upr ## 1 72.67072 67.08489 78.25655 Este resultado indica que, bajo las condiciones \\(x_1 = 10\\) y \\(x_2 = 8\\), el valor del espesor de la capa de pintura para la próxima unidad experimental será 72.67 micrones. A nivel poblacional, dicho valor se encontrará en el intervalo \\((67.085, 78.257)\\) con una confianza del 95%. 3.6 Análisis de Residuales El análisis de residuales en RLM es fundamental para: Validar los supuestos del error; identicar observaciones outlier; e identificar observaciones influenciales. 3.6.1 Validación de supuestos La validación de los supestos del error en el modelo de RLM se realiza de manera similar a como se mostró para el modelo de RLS. Para más detalles, ver Análisis de Residuales. 3.6.2 Identificación de outliers Los outliers son también conocidos como observaciones atípicas en los datos. A partir del modelo ajustado, podemos calcular: Residuales crudos \\[\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\] Residuales estandarizados \\[d_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\hat{\\sigma}^2}} = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{MSE}}} \\] Residuales estudentizados \\[r_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii})}} = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{MSE}(1-h_{ii})}}\\] donde \\(h_{ii} = \\mathbf{X}(\\mathbf{X^\\prime\\mathbf{X}})^{-1}\\mathbf{X}^\\prime_{ii}\\) y \\(\\mathbf{X}(\\mathbf{X^\\prime\\mathbf{X}})^{-1}\\mathbf{X}\\) denominada la matriz hat. La mejor manera de identificar outliers es a partir del cálculo de los residuales estudentizados. Decimos que la \\(i\\)-ésima observación es un outlier si \\(r_i\\notin (-3, 3)\\). En R las funciones clave son: ## residuales crudos r_crudo &lt;- residuals(fit) ## residuales estudentizados r_est &lt;- rstudent(fit) ## residuales estandarizados r_normal &lt;- rstandard(fit) donde fit es el objeto R que contiene en modelo de RLM estimado. Para más detalles, se sugiere consultar la ayuda de cada función: ?residuals, ?rstudent y ?rstandard. En nuestro caso, ## residuales estudentizados r &lt;- rstudent(fit) which(r &lt; -3 | r &gt; 3) ## 18 ## 18 la observación 18 podría considerarse un outlier. En la práctica, el siguiente paso es evaluar la trazabilidad de esa observación y determinar si existen o no causas asignables para que esta sea un outlier. En caso de que exista una causa asignable, dicha observación debería removerse de la base de datos y, con los datos reducidos, estimar nuevamente el modelo de RLM. Otra forma de detectar outliers es a través de la prueba de Bonferroni. Esta prueba está implementada en la función outlierTest del paquete car. En nuestro ejemplo, procedemos de la siguiente manera: ## prueba Bonferroni para outliers require(car) outlierTest(fit, n.max = 5) ## No Studentized residuals with Bonferroni p &lt; 0.05 ## Largest |rstudent|: ## rstudent unadjusted p-value Bonferroni p ## 18 3.360964 0.001116 0.1116 En la parte superior vemos que la observación 18 tiene el mayor valor del residual estudentizado. Sin embargo, el valor \\(p\\) es superior a 0.05, por lo que no tenemos evidencia suficiente para concluir que dicha observación representa un outlier. Gráficamente es posible identificar gráficamente cuáles son las observaciones influenciales utilizando la función influenceIndexPlot del paquete car: ## gráfico de variables influenciales influenceIndexPlot(fit, vars = &quot;Bonf&quot;, las = 1) Figura 3.6: Gráfico de observaciones influenciales. El eje \\(y\\) corresponde al valor \\(p\\) de la prueba de Bonferroni. Para más detalles, se recomienda escribir ?viainfluenceIndexPlot en la consola del R. 3.6.3 Identificación de observaciones influenciales En ciertas ocasiones encontramos observaciones que lucen algo anormales y es importante determinar si estas son influenciales o no. A diferencia de los outliers, las observaciones influenciales controlan el modelo y por ello es importante determinar si el modelo ajustado es consistente cuando estas se remueven. La identificación de este tipo de observaciones se realiza utilizando, principalmente, la Distancia de Cook. Para la \\(i\\)-ésima observación, esta distancia se calcula como \\[D_i = \\frac{r_i^2}{p}\\frac{h_{ii}}{(1-h_{ii})} = \\frac{\\hat{\\epsilon}_i^2 \\,h_{ii}}{p\\,\\hat{\\sigma}^2(1-h_{ii})^2}\\] donde \\(p\\) es el número de variables controlables incluídas en el modelo. Usualmente, cuando \\(D_i&gt;\\frac{4}{n-p-2}\\) decimos que la \\(i\\)-ésima observacion es influencial. Este criterio es el utilizado por la función cooks.distance() del R. 3.7 Análisis de Multicolinealidad Cuando se utiliza el modelo de RLM, se asume que las variables controlables \\(X_1, X_2,\\ldots,X_k\\) son independientes. Desde el punto de vista práctico, esto tiene consideraciones importantes puesto que permite evaluar la magnitud del efecto de sobre \\(\\widehat{E[Y]}\\) cuando modificamos, en una unidad, digamos \\(x_j\\), mientras se mantienen el resto de ellas constantes. Este efecto corresponde, sin duda, a \\(\\hat{\\beta}_j\\). Sin embargo, cuando estas variables controlables no son independientes, este efecto no puede calcularse de la misma forma. Buscamos modelos en los que las covariables estén altamente correlacionadas con la respuesta, pero mínimamente entre ellas. Desde el punto de vista teórico, la existencia de no independencia en las variables controlables tiene consecuencias importantes sobre los estimadores de los parámetros del modelo dados por \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k)\\). Cuando se usa el método de mínimos cuadrados, los estimadores de \\(\\mathbf{\\beta}\\) están dados por: \\[ \\hat{\\mathbf{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime \\mathbf{y} \\] La existencia de multicolinealidad es sinónimo de que no existe independencia en las variables controlables del modelo. Si esto es cierto, las columnas de la matriz de diseño \\(\\mathbf{X}\\) no son independientes, es decir, que la columna \\(x_j\\) puede expresarse como una combinación lineal de las demás. Matemáticamente, esto es equivalente a escribir \\(x_j \\sim x_{-j}\\) para algún \\(j\\). Por ejemplo, para \\(j=1\\) tendríamos \\[ x_1 \\sim x_2 + x_3 + \\cdots + x_k. \\] Esta expresión indica que la variable independiente/controlable \\(x_1\\) puede escribirse como una combinación lineal de las demás variables controlables. O, en otras palabras, que la información contenida en \\(x_1\\) puede explicarse por las demás variables controlables medidas en el proceso durante la etapa de muestreo. Una manera de interpretar la multicolinealidad es como sinónimo de redundancia. Esta redundancia se refiere a que existen variables controlables en el proceso de producción que contienen la misma información que las demás. Por lo tanto, basta con medir sólo aquellas que realmente determinan dicho proceso. En la expresión de \\(\\hat{\\mathbf{\\beta}}_{\\text{OLS}}\\), el término \\((\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\) se refiere a la inversa de \\(\\mathbf{X}^\\prime\\mathbf{X}\\). Si existe multicolinealidad, \\[ \\text{det}(\\mathbf{X}^\\prime \\mathbf{X}) \\approx 0 \\quad \\Rightarrow \\quad \\frac{1}{|\\mathbf{X}^\\prime \\mathbf{X}|} \\rightarrow\\infty \\] Por lo tanto, \\[ \\hat{\\mathbf{\\beta}}_{\\text{OLS}} \\rightarrow \\infty. \\] 3.7.1 Cómo determinar si existe multicolinealidad? Existen varios indicadores para sospechar que existe multicolinealidad: Una alta correlación en las variables independientes. Esto es posible determinarlo gráficamente a través de una matriz de dispersión (ver por ejemplo ?pairs en la consola del R) o utilizando una prueba de independencia completa. Que se rechace la prueba de significancia global pero no todas las pruebas de significancia marginal. Que ocurran ambios considerables en \\(\\hat{\\mathbf{\\beta}}\\) cuando se agrega o elimina una variable predictora. Para probar que efectivamente existe, podemos usar tres aproximaciones: El número de condición de la matriz \\(\\mathbf{X}^\\prime \\mathbf{X}\\). También conocido como I-ll condicion number o ICN, este número mide qué tan “enferma” se encuentra la matriz que debe ser invertida para poder calcular \\(\\hat{\\mathbf{\\beta}}_{\\text{OLS}}\\). El ICN se calcula como \\[\\text{ICN}(\\mathbf{X}^\\prime\\mathbf{X}) = \\sqrt{\\frac{\\lambda_\\text{máx}}{\\lambda_\\text{min}}} \\] con \\(\\lambda_\\text{máx}\\) y \\(\\lambda_\\text{min}\\) los valores propios máximos y mínimos de \\(\\mathbf{X}^\\prime \\mathbf{X}\\), obtenidos a partir de la descomposición espectral de dicha matriz. Decimos que existe multicolinealidad cuando \\(\\text{ICN}(\\mathbf{X}^\\prime \\mathbf{X}) &gt; 30\\). El inconveniente con el ICN es que no nos da información acerca de cuál de las variables independentes es la más multicolineal (o redundante) en el sistema. En R, la función clave para calcular el ICN es kappa. Para más detalles, se recomienda escribir ?kappa en la consola. En el caso del ejemplo del espesor de pintura, tendríamos los siguientes resultados: ## ICN para el modelo ajustado kappa(fit) ## [1] 48.62071 Puesto que \\(\\text{ICN}(\\mathbf{X}^\\prime \\mathbf{X}) &gt; 30\\), podemos concluir que existe evidencia para sospechar que, efectivamente, existe multicolinealidad. El factor de inflación de varianza (VIF). A través de este indicador podemos detectar cuál de las variables independientes es la más colineal de las \\(k\\) medidas. Para la \\(j\\)-ésima variable independiente, \\[\\text{VIF}_j = \\frac{1}{1-R_j^2}\\] donde \\(R_j^2\\) es el \\(R^2_\\text{adjusted}\\) del modelo \\(x_j\\sim x_{-j}\\). Decimos que la variable \\(x_j\\) es responsable por la multicolineal en el sistema si \\(\\text{VIF}_j &gt; 5\\). En R, el VIF puede calcularse a través de la función vif del paquete car. Para más detalles, se recomienda escribir ?vif en la consola. En nuestro ejemplo tendríamos los siguientes resultados: ## cálculo del VIF car:::vif(fit) ## x1 x2 ## 1.001645 1.001645 Puesto que en ninguna de las dos variables controlables el \\(\\text{VIF}&gt;5\\), concluimos que no existe multicolinealidad. Pruebas complementarias. En algunos casos, el ICN indica que existe multicolinealidad, pero para ninguno de los predictores el \\(\\text{VIF}&gt;5\\). Cuando esto ocurre, lo mejor es utilizar pruebas complementaria, más robustas, que permitan decidir si efectivamente existe multicolinealidad en el modelo de RLM. Las pruebas complementarias pueden realizarse en R con la función mctest del paquete mctest. En nuestro ejemplo tendríamos los siguientes resultados: ## pruebas complementarias de multicolinealidad require(mctest) mctest(fit) ## ## Call: ## omcdiag(mod = mod, Inter = TRUE, detr = detr, red = red, conf = conf, ## theil = theil, cn = cn) ## ## ## Overall Multicollinearity Diagnostics ## ## MC Results detection ## Determinant |X&#39;X|: 0.9984 0 ## Farrar Chi-Square: 0.1602 0 ## Red Indicator: 0.0405 0 ## Sum of Lambda Inverse: 2.0033 0 ## Theil&#39;s Method: -0.8683 0 ## Condition Number: 8.2787 0 ## ## 1 --&gt; COLLINEARITY is detected by the test ## 0 --&gt; COLLINEARITY is not detected by the test De acuerdo con estos resultados, podemos concluir que no existe multicolinealidad. 3.8 Selección de Modelos 3.8.1 Método de Todas las Regresiones Posibles El Método de Todas las Regresiones Posibles permite, a partir de un conjunto de variables independientes \\(X_1, X_2, \\ldots, X_k\\) que potencialmente podrían explicar una respuesta continua \\(Y\\), ajustar hasta \\(2^{k}-1\\) modelos de regresión y seleccionar el mejor de estos utilizando algún criterio. En la práctica, algunos de los criterios más utilizados incluyen \\(R^2\\), \\(R^2_\\text{adj}\\), \\(\\sqrt{MSE}\\), AIC, BIC, PRESS y \\[\\begin{equation} C_p = p+\\frac{\\text{SSE}_p}{\\text{MSE}_\\text{todos}} - (n-2p) = \\begin{cases} = p &amp; \\text{ para el modelo completo } \\\\ \\approx p &amp; \\text{ el sesgo es pequeño $\\rightarrow$ ideal!} \\\\ &gt; p &amp; \\text{ sesgo es alto }\\\\ &lt; p &amp; \\text{ no hay sesgo }\\\\ \\end{cases} \\end{equation}\\] también conocido como el estadístico de Mallows. En la expresión anterior, \\(\\text{MSE}_\\text{todos}\\) es \\(\\hat{\\sigma}^2\\) usando todas las covariables, y \\(\\text{SSE}_p\\) es el SSE del modelo con sólo \\(p^\\prime &lt; p\\) de ellas. Como ejemplo, consideraremos los siguientes datos que corresponden al peso del producto terminado en gramos (variable \\(y\\)) cuando se controlan los parámetros \\(x_1, x_2, \\ldots, x_{10}\\) de una inyectora: ## datos inyectora d &lt;- read.table(&quot;https://www.dropbox.com/s/a9gzu54luabtubo/inyectora.txt?dl=1&quot;, header = TRUE) ## primeras 3 filas head(d, 3) ## y x1 x2 x3 x4 ## 1 22.49879 -0.4248450 0.19997792 -0.5225479 0.5691505 ## 2 17.60173 0.5766103 -0.33435292 0.9247179 -0.9811402 ## 3 22.50649 -0.1820462 -0.02277393 0.2027315 0.5581318 ## x5 x6 x7 x8 x9 ## 1 0.9721086 -0.2927878 -0.5255406 0.6898671 -0.05863633 ## 2 -0.7258651 -0.2671171 0.3729807 -0.4797351 -0.26830905 ## 3 0.8106192 -0.4257997 -0.5483632 -0.9537110 -0.75745589 ## x10 ## 1 0.84739843 ## 2 0.08519674 ## 3 0.70472920 En total se tienen 100 unidades experimentales. Dado que en los datos sólo existen la variable respuesta y las covariables, podemos utilizar la siguiente sintaxis para ajustar el modelo de RLM: ## modelo de RLM ajustado fit_inyectora &lt;- lm(y ~ ., data = d) summary(fit_inyectora) ## ## Call: ## lm(formula = y ~ ., data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.09900 -0.61503 -0.04698 0.49843 2.35473 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.97501 0.09496 210.353 &lt; 2e-16 *** ## x1 3.04897 0.17692 17.234 &lt; 2e-16 *** ## x2 0.85418 0.18412 4.639 1.20e-05 *** ## x3 4.98623 0.17088 29.179 &lt; 2e-16 *** ## x4 4.97033 0.17161 28.963 &lt; 2e-16 *** ## x5 2.99030 0.17537 17.052 &lt; 2e-16 *** ## x6 4.03875 0.16979 23.786 &lt; 2e-16 *** ## x7 2.08153 0.16505 12.612 &lt; 2e-16 *** ## x8 0.89152 0.16997 5.245 1.05e-06 *** ## x9 2.90108 0.18030 16.090 &lt; 2e-16 *** ## x10 2.89888 0.17135 16.918 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.9418 on 89 degrees of freedom ## Multiple R-squared: 0.9778, Adjusted R-squared: 0.9753 ## F-statistic: 391.2 on 10 and 89 DF, p-value: &lt; 2.2e-16 Como el número de variables independientes es \\(k=10\\), debemos estimar \\(2^k-1= 1023\\) modelos diferentes al utilizar el Método de Todas las Regresiones Posibles. Para encontrar el mejor modelo, usamos la función ols_step_all_possible del paquete olsrr. Para más detalles, se recomienda escribir ?ols_step_all_possible en la consola del R. ## método de todas las regresiones posibles require(olsrr) k &lt;- ols_step_all_possible(fit_inyectora) head(k, 10) ## Index N Predictors R-Square Adj. R-Square Mallow&#39;s Cp ## 4 1 1 x4 0.381431483 0.375119559 2379.065 ## 10 2 1 x10 0.221939980 0.214000593 3017.235 ## 5 3 1 x5 0.150758161 0.142092428 3302.054 ## 3 4 1 x3 0.122800863 0.113849851 3413.919 ## 9 5 1 x9 0.065647890 0.056113685 3642.604 ## 6 6 1 x6 0.061473423 0.051896621 3659.307 ## 7 7 1 x7 0.049953135 0.040258779 3705.403 ## 8 8 1 x8 0.037568650 0.027747921 3754.957 ## 1 9 1 x1 0.005957422 -0.004185870 3881.442 ## 2 10 1 x2 0.005662949 -0.004483347 3882.620 En el objeto k se encuentran todos los resultados. La función ols_step_all_possible permite estimar 11 indicadores diferentes que facilitan la elección del mejor modelo. Por faciidad, sólo se muestran 3 de ellos. Por ejemplo, si queremos seleccionar el mejor modelo utilizando el \\(R^2_\\text{adj}\\), basta con escribir ## mejor modelo basado en el R^2_{adj} k[which.max(k[,&#39;adjr&#39;]), ] ## Index N Predictors R-Square ## 1023 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 ## Adj. R-Square Mallow&#39;s Cp ## 1023 0.9752579 11 Si usamos el AIC, el mejor modelo será: ## mejor modelo basado en el Cp k[which.min(with(k, (cp-n)^2)), ] ## Index N Predictors R-Square ## 1023 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 ## Adj. R-Square Mallow&#39;s Cp ## 1023 0.9752579 11 Otra posibilidad es utilizar dos criterios selección a la vez: ## selección usando el R^2_{adj} y el AIC require(ggplot2) ggplot(k, aes(x = adjr, y = aic)) + geom_point() + theme_minimal() + xlab(expression(R^2 * &quot; ajustado&quot;)) + ylab(&quot;AIC&quot;) Figura 3.7: AIC vs. \\(R^2\\) ajustado en todas las regresiones posibles. De acuerdo con la definición de cada criterio, el mejor modelo debe tener un \\(R^2_{\\text{adj}}\\rightarrow1\\) y \\(\\text{AIC}\\rightarrow 0\\). De la gráfica, es posible observar que hay cuatro modelos que sobresalen: ## selección de modelos competitivos k &lt;- as.data.frame(k) subset(k, aic &lt; 350 &amp; adjr &gt; 0.8, select = n:aic) ## n predictors rsquare adjr ## 998 8 x1 x3 x4 x5 x6 x7 x9 x10 0.9650304 0.9619562 ## 1021 9 x1 x3 x4 x5 x6 x7 x8 x9 x10 0.9723781 0.9696159 ## 1015 9 x1 x2 x3 x4 x5 x6 x7 x9 x10 0.9708818 0.9679700 ## 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 0.9752579 ## predrsq cp aic ## 998 0.9577212 57.92307 325.3860 ## 1021 0.9653333 30.52284 303.7989 ## 1015 0.9640209 36.51001 309.0744 ## 1023 0.9715036 11.00000 284.1403 Por ejemplo, el modelo 968 no incluye x2 ni x8 y tiene un \\(R^2_{\\text{adj}}=0.9619\\), mientras el modelo 1023 tiene el \\(R^2_{\\text{adj}}\\) más alto e incluye 10 variables predictoras. A pesar de que este modelo es levemente mejor el 968, la ganancia en \\(R^2_{\\text{adj}}\\) es ínfima y podría no justificar la medición en proceso de dos factores controlables más. En términos del AIC, se tiene que los modelos 1021 y 1015 son similares. 3.8.2 Selección secuencial Need to add some introductory remarks Método stepwise To be completed Método forward To be completed Método backward To be completed "],["glm.html", "Capítulo 4 Modelos de Regresión Avanzados 4.1 Regresión No Lineal 4.2 Regresión Logística 4.3 Regresión Poisson", " Capítulo 4 Modelos de Regresión Avanzados Here is a review of existing methods. 4.1 Regresión No Lineal 4.1.1 Ejercicios 4.2 Regresión Logística 4.2.1 Ejercicios 4.3 Regresión Poisson 4.3.1 Variaciones Aunque no hace parte del contenido del curso, el modelo de Regresión Poisson tiene algunas variaciones. El tipo de model 4.3.2 Ejercicios "],["series.html", "Capítulo 5 Introducción a Series de Tiempo 5.1 Qué es una Serie de Tiempo? 5.2 Definiciones básicas 5.3 Por qué y para qué? 5.4 Modelos básicos 5.5 Validación de supuestos 5.6 Pronósticos 5.7 Ejercicios", " Capítulo 5 Introducción a Series de Tiempo 5.1 Qué es una Serie de Tiempo? 5.2 Definiciones básicas 5.3 Por qué y para qué? 5.4 Modelos básicos 5.4.1 Método de Descomposición 5.4.2 Métodos de Suavizamiento 5.4.3 Metodología Box-Jenkins 5.5 Validación de supuestos 5.6 Pronósticos 5.7 Ejercicios "],["enp.html", "Capítulo 6 Estadística No Paramétrica 6.1 Por qué y para qué? 6.2 Modelos básicos 6.3 Ejercicios", " Capítulo 6 Estadística No Paramétrica 6.1 Por qué y para qué? 6.2 Modelos básicos 6.2.1 Prueba de signos 6.2.1.1 Ejemplos 6.2.2 Prueba de Rangos con Signos 6.2.2.1 Ejemplos 6.2.3 Prueba de Mann-Whitney-Wilcoxon 6.2.3.1 Ejemplos 6.2.4 Prueba de Kruskal-Wallis 6.2.4.1 Ejemplos 6.3 Ejercicios "],["palabras-finales.html", "Capítulo 7 Palabras Finales", " Capítulo 7 Palabras Finales We have finished a nice book. "],["referencias.html", "Referencias", " Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

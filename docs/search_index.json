[["index.html", "Modelos de Regresión: Una aproximación práctica con R Bienvenido Estructura del libro Por qué R? Software y convenciones Bloques informativos Dedicatoria Sobre el autor", " Modelos de Regresión: Una aproximación práctica con R Jorge I. Vélez 2021-09-11 Bienvenido Este libro está destinado para estudiantes de ingeniería, psicología, economía, estadística, matemáticas y áreas afines interesados en el análisis de datos, especialmente en la formulación, ajuste, validación e implementación de modelos de regresión avanzados, pronósticos en series de tiempo y métodos no paramétricos utilizando R. En este texto se recopilan los elementos básicos del curso de Análisis de Datos en Ingeniería II del Programa de Ingeniería Industrial de la Universidad del Norte, y está pensado como una guía para dicho curso. Por supuesto, los errores presentes a lo largo de todo el documento son sólo mi responsabilidad. Estructura del libro En el Capítulo 1 se presentan algunos conceptos básicos de Análisis de Datos, además de qué es R, cómo instalarlo, dónde y cómo buscar ayuda y la estructura básica del programa. En el capítulo 2 se discute el Modelo de Regresión Lineal Simple y en el capítulo 3 el Modelo de Regresión Lineal Múltiple. En el capítulo 4 se extienden estos modelos para el caso donde la variable respuesta puede tomar valores discretos, y se discuten los modelos de Regresión Logística y Regresión Poisson. Posteriormente, en el capítulo 5, se realiza una corta Introducción a Series de Tiempo. Como parte de ello, se discuten algunos conceptos generales y se presentan tres estrategias diferentes para realizar pronósticos. Finalmente, en el capítulo 6, se introducen conceptos básicos de Estadística No Paramétrica entre los que se incluyen las pruebas de signo, rangos con signos, Mann-Whitney-Wilcoxon y Kruskal-Wallis, además de algunas estrategias para el análisis de tablas de contingencia. Para afianzar el proceso de aprendizaje, en cada capítulo se presentan una serie de ejercicios donde el estudiante puede poner en práctica los conceptos teóricos a través de la solución de problemas que comúnmente se presentan en ingeniería. Por qué R? Software y convenciones Este libro está construido utilizando los paquetes knitr (Xie 2015) y bookdown (Xie 2019) de R. En todo el libro se presentarán códigos que el lector puede seleccionar y posteriormente ejecutar en su consola de R para obtener los mismos resultados del libro. El código se destaca en una recuadro de color similar al que se muestra a continuación: 3 + 7 x &lt;- c(2, 9, 11) 5 * x 1:5 Los resultados obtenidos de cualquier código R se destacan con los símbolos ## al inicio de cada renglón. Por ejemplo, el resultado de 3 + 7 será mostrado como ## [1] 10 Bloques informativos En varias partes del libro se usarán bloques informativos para resaltar algún aspecto importante: Nota aclaratoria. Sugerencia. Advertencia. Dedicatoria Dedicado a Lilyam y Abraham por su apoyo; a Norma Botero por mostrarme el camino; a mis maestros y amigos, Mauricio Arcos-Burgos y Juan Carlos Correa por sus enseñanzas y acompañamiento; a Yolima, por su amor incondicional, su infinita comprensión y paciencia; y a Gabriela, mi monstruo peludo, por darme fuerzas cuando no había razones para continuar. Sobre el autor Jorge I. Vélez es Profesor Asistente del Departamento de Ingeniería Industrial de la Universidad del Norte en Barranquilla, Colombia en las áreas de Analítica de Datos y Diseño de Experimentos para los programas de Pregrado, Maestría y Doctorado. Experto en Analítica de Datos, Bioinformática, Genómica Predictiva, Genética de Poblaciones, Epidemiología Genética y Estadística Computacional y Genética, el Profesor Vélez es instructor en las áreas de Genética de Poblaciones, Diseño de Experimentos, Bioinformática, y Bioestadística en los programas de Maestría y Doctorado en Ciencias Biomédicas del Departamento de Medicina de la Universidad del Norte. Sus intereses de investigación incluyen Analítica de Datos, Bioinformática y Genómica Predictiva aplicadas a enfermedad de Alzheimer, Trastorno de Déficit de Atención con Hiperactividad y Enfermedades Autoinmunes. Referencias "],["intro.html", "Capítulo 1 Introducción 1.1 Análisis Exploratorio de Datos 1.2 Conceptos básicos 1.3 Gráficos básicos 1.4 R como herramienta", " Capítulo 1 Introducción El análisis de datos es el pilar fundamental de Data Analytics y Data Science. Por supuesto, el uso herramientas computacionales es importante. A lo largo de este materal usaremos el R, pero muchos de los conceptos aquí expuestos pueden trabajarse, sin problema, en otro lenguaje como Python o Julia. En la Universidad del Norte, los estudiantes tienen la posibilidad de tomar, a nivel de pregrado, cursos de Análisis de Datos. Parcticularmente, los estudiantes de la División de Ingenierías pueden tomar los cursos de : Análisis de Datos en Ingeniería I, ofrecido por el Departamento de Ingeniería Industrial; Análisis de Datos en Ingeniería II, ofrecido por el Departamento de Ingeniería Industrial; Machine Learning, ofrecido por el Departamento de Ingeniería Eléctrica y Electrónica y Analítica de Datos Avanzada; ofrecido por el Departamento de Ingeniería Industrial. Estos cursos, cuya dificultad aumenta conforme a la enumeración anterior, permiten a los estudiantes de la División de Ingenierías tener herramientas básicas, intermedias y avanzadoas para convertir datos en información. 1.1 Análisis Exploratorio de Datos 1.2 Conceptos básicos 1.3 Gráficos básicos 1.4 R como herramienta 1.4.1 Qué es R? 1.4.2 CRAN 1.4.3 Descarga 1.4.4 Operaciones básicas 1.4.5 Creación de funciones 1.4.6 Modelos básicos 1.4.7 Modelos avanzados 1.4.8 Cursos online Jorge I. Vélez, 2021-09-11 "],["rls.html", "Capítulo 2 Regresión Lineal Simple 2.1 Formulación básica del modelo de RLS 2.2 Estimación 2.3 Tabla ANOVA y medidas de desempeño 2.4 Análisis de Residuales 2.5 Predicción", " Capítulo 2 Regresión Lineal Simple Diariamente, en muchos procesos productivos se controlan las condiciones de operación de un sistema y es de interés determinar cómo una variable respuesta de cambia dependiendo dichas condiciones. Por ejemplo, en un proceso de embutido de cárnicos la cantidad de producto empacado (en gramos, por ejemplo) dependa de la velocidad y la temperatura a la que opere una máquina. En la práctica, por supuesto, es fundamental Entender la relación entre la cantidad de producto empacado, la velocidad y la temperatura de operación; Garantizar que la cantidad de producto empacado cumpla con las especificaciones; Determinar las condiciones óptimas de operación que minimizan los costos asociados y disminuyen considerablemente los desperdicios de producto. El Modelo de Regresión Lineal es ampliamente utilizado en la práctica ingenieril cuando, en un proceso productivo como el anterior, se quiere dar respuesta los numerales 1, 2 y 3. En un sentido menos amplio, los modelos de regresión lineal se utilizan para estudiar la relación entre una variable numérica, que denominamos variable respuesta \\(Y\\), y un conjunto de variables controlables de dicho proceso denotadas como \\(X_1, X_2,\\ldots, X_k\\). Una vez entendemos dicha relación, es posible predecir futuros valores de la variable respuesta para valores específicos y conocidos de las variabes controlables. Por variable controlable entendemos aquellas variables de proceso que pueden medirse fácilmente y se mantienen fijas durante la operación. Dependiendo del número de variables controlables, los modelos de Regresión Lineal pueden dividirse en Modelo de Regresión Lineal Simple (RLS) y Modelo de Regresión Lineal Múltiple (RLM). Cuando se tiene sólo una variable controlable, digamos \\(x\\), hablamos de RLS. Cuando se tienen más de una variable controlable, digamos \\(x_1, x_2, \\ldots, x_k\\), hablamos de RLM. En el caso de proceso de embutidos, \\(x_1 = \\text{velocidad}\\) y \\(x_2 = \\text{temperatura}\\). 2.1 Formulación básica del modelo de RLS El modelo de RLS surge ante la necesidad de predecir una variable respuesta \\(Y\\), generalmente continua, como función de una variable controlable \\(X\\). Matemáticamente el modelo puede expresarse como: \\[\\begin{align} \\label{mod1} Y_i &amp;= \\beta_0 + \\beta_1X_i + \\epsilon_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] donde \\((\\beta_0, \\beta_1, \\sigma^2)\\) corresponden a los parámetros del modelo y \\(\\epsilon_i\\) es el error aleatorio para la observación \\(i\\). Los términos \\((\\beta_0, \\beta_1)\\) corresponden a los coeficientes del modelo, mientras \\(\\sigma^2\\) es la varianza. En la práctica, se tiene o toma una muestra aleatoria de tamaño \\(n\\) de una población o proceso de producción/servicios, y se quieren estimar, a partir de dicha muestra, los valores de \\((\\beta_0, \\beta_1, \\sigma^2\\)). Consideremos un proceso donde se tienen registros de la Resistencia, en psi, y la Edad, en semanas, de varias soldaduras. Los datos pueden leerse en R como se muestra a continuación: ## lectura de datos en URL file &lt;- &quot;https://www.dropbox.com/s/h4abseyfnvfwgxq/data_rls.txt?dl=1&quot; datos &lt;- read.table(file = file, header = TRUE) ## primeras 6 filas de los datos head(datos) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 Con miras explorar si existe una relación entre la Resistenciay la Edad de la soldadura, existen dos estrategias. En RLS, es posible establecer si existe una posible relación entre \\(X\\) y \\(Y\\) es Construir un gráfico de dispersión. Calculando el coeficiente de correlación lineal muestral; Para construir un gráfico de dispersión procedemos de la siguiente manera: ## gráfico de dispersión require(ggplot2) ggplot(datos, aes(x = Edad, y = Resistencia)) + geom_point() + theme_minimal() Observe que, aparentemente, cuanto mayor sea la Edad de la soldadura menor será su Resistencia. Por lo tanto, podemos decir que la relación entre Edad y Resistencia es inversamente proporcional. El coeficiente de correlación \\(\\rho\\) entre dos variables aleatorias \\(X\\) e \\(Y\\) determina la relación lineal que existe entre ellas. A partir de una muestra de tamaño \\(n\\), dicho coeficiente puede calcularse como \\[\\begin{eqnarray} \\hat{\\rho}_{XY} &amp;=&amp; \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\\\\\nonumber &amp;=&amp; \\frac{\\sum_{i=1}^n{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sqrt{\\sum_{i=1}^n{(x_i-\\bar{x})^2}\\sum_{i=1}^n{(y_i-\\bar{y})^2}}} \\end{eqnarray}\\] donde \\[\\begin{eqnarray} \\bar{x} &amp;=&amp; \\frac{\\sum_{i=1}^n{x_i}}{n} \\\\\\nonumber \\bar{y} &amp;=&amp; \\frac{\\sum_{i=1}^n{y_i}}{n}. \\end{eqnarray}\\] El coeficiente de correlación muestral \\(-1\\leq \\hat{\\rho}_{XY} \\leq 1\\) determina la relación lineal entre las variables \\(X\\) y \\(Y\\). \\(\\hat{\\rho}_{XY} \\approx -1\\) indica \\(X\\) y \\(Y\\) tienen una relación lineal inversa, es decir, que cuando \\(X\\) aumenta, la variable \\(Y\\) disminuye. \\(\\hat{\\rho}_{XY} \\approx 0\\) indica que no existe una relación lineal entre \\(X\\) y \\(Y\\). Sin embargo, este resultado no es suficiente para descartar que exista otro tipo de relación (por ejemplo cuadrática o cúbica) entre las dos variables. \\(\\hat{\\rho}_{XY} \\approx 1\\) indica \\(X\\) y \\(Y\\) tienen una relación lineal positiva, es decir, que cuando \\(X\\) aumenta, la variable \\(Y\\) también. Con los datos de soldaduras se tendría que: ## coeficiente de correlación with(datos, cor(Edad, Resistencia)) ## [1] -0.8827718 Por lo tanto, concluimos que la relación entre la Edad de la soldadura y su Resistencia es inversamente proporcional. 2.2 Estimación Para estimar el modelo de RLS, se requiere una muestra aleatoria de tamaño \\(n\\) de una población. Esta muestra puede corresponder a observaciones específicas de un proceso de producción o servicios donde se controla una variable \\(X\\) y se registra una variable respuesta \\(Y\\). Por lo general, los datos para \\(n\\) unidades experimentales de dicho proceso están constituidos por los pares \\[(x_1,y_1),(x_2,y_2),(x_3, y_3),\\ldots,(x_n,y_n).\\] En nuestro ejemplo, los datos están organizados de la siguiente forma: ## muestra los primeros 10 pares de datos head(datos, 10) ## Resistencia Edad ## 1 10.9 18.7 ## 2 10.6 19.1 ## 3 13.4 9.3 ## 4 10.7 17.5 ## 5 13.0 14.6 ## 6 13.1 12.8 ## 7 12.1 16.0 ## 8 14.3 7.0 ## 9 11.9 14.9 ## 10 11.1 15.6 Así, los pares \\((x_1,y_1)\\) y \\((x_5, y_5)\\) corresponden a \\((18.7, 10.9)\\) y \\((14.6, 13.0)\\), respectivamente. Para determinar el tamaño de la muestra, hacemos ## número total de observaciones NROW(datos) ## [1] 100 Usualmente, los parámetros \\((\\beta_0, \\beta_1,\\sigma^2)\\) el modelo de RLS puede estimarse utilizando el método de mínimos cuadrados. La idea fundamental es minizar la expresión \\[\\begin{eqnarray*} L &amp;=&amp;\\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_i)^2. \\end{eqnarray*}\\] Para encontrar los estimadores de \\(\\beta_0\\) y \\(\\beta_1\\) derivamos parcialmente e igualamos a cero las expresiones resultantes, esto es \\[\\frac{\\partial L}{\\partial \\beta_0}=0, \\hspace{1cm} \\frac{\\partial L}{\\partial \\beta_1}=0.\\] Los estimadores \\(\\beta_0\\) y \\(\\beta_1\\) están dados por \\[\\begin{align} \\widehat{\\beta}_0&amp;=\\overline{Y}-\\widehat{\\beta}_1\\overline{X}, \\\\ \\widehat{\\beta}_1&amp;=\\frac{S_{xy}}{S_{xx}} \\end{align}\\] donde \\[ S_{xx} = \\sum_{i=1}^n(x_i-\\bar{x})^2 \\quad\\text{y}\\quad S_{xy} = \\sum_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y}). \\] En la práctica, \\(\\widehat{\\beta}_1\\) se interpreta como el cambio unitario sobre el valor esperado de \\(Y\\), \\(E[Y|X]\\), por cada cambio unitario del factor controlable \\(X\\). Por otro lado, \\[\\begin{align} \\hat{\\sigma}^2 &amp;= \\frac{L}{n-2} \\\\ &amp;=\\frac{1}{n-2}\\sum_{i=1}^n\\hat{\\epsilon}_i^2 \\\\ &amp;=\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\ &amp;=\\frac{1}{n-2}\\sum_{i=1}^n(y_i-\\hat{\\beta}_0-\\hat{\\beta}_1X_i)^2 \\\\ &amp;=\\frac{SSE}{n-2} \\\\ &amp;= MSE \\end{align}\\] donde el \\(SSE\\) corresponde a la suma de cuadrados del error, y \\(MSE\\) es el Mean Squared Error. Finalmente, el modelo de RLS estimado es \\[\\begin{align} \\widehat{Y}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1X_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\hat{\\sigma}^2), \\\\ \\hat{\\sigma}^2 &amp;= \\text{constante}. \\end{align}\\] Otra forma de escribirlo es \\[\\begin{align} \\widehat{Y}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2), \\\\ \\hat{\\mu}_i &amp;= \\hat{\\beta}_0 + \\hat{\\beta}_1X_i. \\end{align}\\] A partir del modelo ajustado es posible predicir el valor esperado de \\(Y\\) para valores conocidos de \\(X\\), es decir, calcular \\(\\widehat{E[Y|X=x_0]}\\), donde \\(x_0\\) es un valor conocido del factor controlable \\(X\\). Adicionalmente, podemos calcular intervalos de confianza e intervalos de predicción para \\({E[Y|X=x_0]}\\). Observe que para un valor específico de \\(X\\), digamos \\(x_i\\), se tiene el valor correspondiente de \\(Y\\) es \\(y_i\\). En otras palabras, la \\(i\\)-ésima unidad experimental se produjo cuando \\(X = x_i\\) y que, bajo estas condiciones, se obtuvo que \\(Y = y_i\\). En R la función clave paara ajustar modelos de RLS es lm. Para mayor información, puede consultar la ayuda de la función escribiendo ?lm en la consola, o en la página oficial. En general, la sintaxis es modelo &lt;- lm(y ~ x, data = datos) donde y es la variable respuesta, x es el factor controlable o variable independiente y datos es el objeto en R que contiene los datos con los que estamos trabajando. Para los datos de las soldaduras se tiene que: ## ajuste del modelo de RLS (modelo &lt;- lm(Resistencia ~ Edad, data = datos)) ## ## Call: ## lm(formula = Resistencia ~ Edad, data = datos) ## ## Coefficients: ## (Intercept) Edad ## 15.7427 -0.2438 Los resultados indican que \\(\\widehat{\\beta}_0 = 15.74\\) y \\(\\widehat{\\beta}_1 = -0.244\\). Por lo tanto, el modelo ajustado puede escribirse como: \\[\\begin{align} \\widehat{\\text{Resistencia}}_i &amp;= 15.74 -0.244\\,\\text{Edad}_i ,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] Cómo interpretamos \\(\\widehat{\\beta}_0\\) y \\(\\widehat{\\beta}_1\\)? Se espera que, por cada semana que transcurre, la resistencia de la soldadura disminuya \\(0.244\\) psi. Una soldadura nueva \\((\\text{Edad} = 0)\\) tiene una resistencia promedio de \\(15.74\\) psi Observe que aún no tenemos el valor estimado de \\(\\sigma\\). Este parámetro del modelo de RLS puede obtenerse como el \\(MSE\\) de la tabla de análisis de varianza o Tabla ANOVA, que describiremos a continuación. 2.3 Tabla ANOVA y medidas de desempeño La Tabla ANOVA es útil para estimar la varianza del modelo de RLS, \\(\\sigma^2\\); calcular el coeficiente de determinación \\(R^2\\); verificar la bondad de ajuste del modelo. En el modelo de RLS, la varianza total de la respuesta puede descomponerse como \\[SST = SSR + SSE\\] donde \\[\\begin{align} SSR &amp;= \\text{ varianza atribuible al factor controlable } x\\\\ SSE &amp;= \\text{ varianza atribuible a factores no controlables} \\end{align}\\] Usualmente, el término \\(SSR\\) se refiere a cuánto de la varianza observada en \\(Y\\) puede ser explicada por el factor controlable \\(x\\). Por otro lado, el término \\(SSE\\) corresponde a cuánto de la varianza de \\(Y\\) puede explicarse a través de otros factores que, desafortunadamente, no fueron controlados ni medidos durante la toma de la muestra. Si el modelo es suficientemente bueno, \\(SSE\\rightarrow 0\\). A partir de la muestra, \\[\\begin{align} SST &amp;= \\sum_{i=1}^n(y_i-\\bar{y})^2, \\\\ SSR &amp;= \\sum_{i=1}^n(\\hat{y}_i-\\bar{y})^2=\\widehat{\\beta}_1S_{xy}, \\\\ SSE &amp;= L = \\sum_{i=1}^n\\widehat{\\epsilon}_i^2 = \\sum_{i=1}^n(y_i-\\hat{y}_i)^2 \\\\ \\widehat{y}_i &amp;= \\widehat{\\beta}_0-\\widehat{\\beta}_1X_i \\end{align}\\] Estimación de \\(\\sigma^2\\) A partir de los resultados una vez es estima el modelo, \\[\\widehat{\\sigma}^2 = \\frac{L}{n-2} = \\frac{SSE}{n-2} = MSE\\] El término \\(MSE\\) se obtiene de la Tabla ANOVA. Al igual a como ocurre con el \\(SSE\\), el \\(MSE\\rightarrow 0\\) para un modelo de RLS ideal. Aunque esto es deseable, es difícilmente observable en la práctica debido a las innumerables fuentes de variabilidad de los procesos de producción y de servicios. En R, el valor de Residual Standard Error, \\(RSE\\), calculado como \\(RSE = \\sqrt{MSE} = \\hat{\\sigma}\\) puede obtenerse como: ## estimación de sigma summary(modelo)$sigma ## [1] 0.5908683 A partir de etse resultado se concluye que \\(\\hat{\\sigma} = 0.591\\). Por lo tanto, \\(MSE = 0.591^2 = 0.349\\). Finalmente, el modelo ajustado puede expresarse como \\[\\begin{align} \\widehat{\\text{Resistencia}}_i &amp;\\sim N(\\hat{\\mu}_i, \\hat{\\sigma}^2),\\\\ \\hat{\\mu}_i &amp;\\sim 15.743 - 0.244\\,\\text{Edad}, \\\\ \\hat{\\sigma}^2 &amp;= 0.591^2 = 0.349. \\end{align}\\] Inferencia para \\(\\sigma^2\\) A partir \\(\\widehat{\\sigma}^2\\) es posible probar si \\[\\begin{align} H_0: \\sigma^2 = \\sigma^2_0 \\\\ H_1: \\sigma^2 \\neq \\sigma^2_0 \\end{align}\\] donde \\(\\sigma^2_0 &gt; 0\\) es un valor específico para \\(\\sigma^2\\). El resultado básico es \\[\\frac{(n-2)\\,\\widehat{\\sigma}^2}{\\sigma^2}\\sim \\chi^2_{n-2} \\] Así, un intervalo de confianza del \\((1-\\alpha)100\\%\\) para \\(\\sigma^2\\) está dado por: \\[\\sigma^2 \\in (a, b)\\] donde \\[a = \\frac{(n-2)\\,\\widehat{\\sigma}^2}{\\chi^2_{1-\\alpha/2,n-2}}, \\quad \\quad b = \\frac{(n-2)\\widehat{\\sigma}^2}{\\chi^2_{\\alpha/2,n-2}} \\] Finalmente, diremos que \\(\\sigma^2 \\neq \\sigma^2_0\\) si el valor \\(\\sigma^2_0\\) no se encuentra en el intervalo \\((a,b)\\). En R todo este procedimiento puede hacerse de la siguiente manera: ## intervalo de confianza para sigma^2 alpha &lt;- 0.05 resultados &lt;- summary(modelo) sigma &lt;- resultados$sigma df &lt;- resultados$df[2] upper_limit &lt;- df*sigma^2/qchisq(alpha/2, df) lower_limit &lt;- df*sigma^2/qchisq(1-alpha/2, df) c(lower_limit = lower_limit, sigma2_hat =sigma^2, upper_limit = upper_limit) ## lower_limit sigma2_hat upper_limit ## 0.2688068 0.3491253 0.4719150 Finalmente, con una confianza del 95%, \\(\\sigma^2 \\in(0.269, 0.472)\\). Ahora, como el valor \\(0\\) no se encuentra en el intervalo de confianza, podemos afirmar que \\(\\sigma^2 &gt; 0\\) a nivel poblacional. Para obtener un intervalo de confianza del 95% para \\(\\sigma\\), basta calcular la raíz cuadrada de estos límites. Por lo tanto, \\(\\sigma \\in(0.519, 0.687)\\). Coeficiente de determinación \\(R^2\\) En el modelo de RLS, una de las medidas más utilizadas para evaluar su desempeño o ajuste es el coeficiente de determinación \\(R^2\\). Matemáticamente, el \\(R^2\\) puede calcularse como \\[R^2 = \\widehat{\\rho}_{XY}^2,\\] donde \\(\\widehat{\\rho}_{XY}\\) es el coeficiente de correlación muestral entre \\(X\\) e \\(Y\\), es un indicador de la calidad del modelo. En nuestro caso, una forma de hacerlo en R es ## coeficiente de determinación rho_est &lt;- with(datos, cor(Resistencia, Edad)) rho_est^2 ## [1] 0.7792861 En la práctica, el \\(R^2\\) puede interpretarse como el porcentaje de variabilidad total de la respuesta \\(Y\\) que puede ser explicado por el modelo de RLS. Utilizando el objeto en R que contiene el modelo ajustado, podemos hacer ## obtención del R^2 a partir de un objeto lm summary(modelo)$r.squared ## [1] 0.7792861 para obtener el coeficiente de determinación. Por lo tanto, podemos concluir, con base en nuestro modelo estimado de RLS, que la Edad de la soldadura explica alrededor del 78% de la varabilidad total de la Resistencia. A partir de esta definición y los valores de \\(SST\\), \\(SSR\\) y \\(SSE\\) es fácil llegar a que \\[R^2 = 1-\\frac{SSE}{SST}\\] Técnicamente, \\(0\\leq R^2 \\leq 1\\), donde \\(R^2 \\rightarrow 0\\) y \\(R^2 \\rightarrow 1\\) representan modelos de RLS con desempeños deficientes e ideales, respectivamente. Sin embargo, en el Capítulo 3 estudiaremos otras medidas que nos permiten determinar si nuestro modelo es lo suficientemente bueno. Al final, el objetivo es seleccionar el mejor modelo de regresión que nos permite explicar, correctamente, una variable respuesta de interés en función de una o más variables explicativas o controlables. Validación del modelo de RLS Al utilizar RLS queremos estar seguros de que el modelo ajustado es considerablemente mejor que no tener dicho modelo. Esto equivale a determinar si la contribución del factor controlable \\(X\\) para predecir \\(E[Y|X=x_0]\\) es sustancial. Formalmente, esto equivale a realizar una prueba de significancia global. La prueba de significancia global puede realizarse de dos maneras en RLS: Utilizando los resultados de la Tabla ANOVA; A través de un procedimiento de pruebas de hipótesis basado en la distribución \\(t\\) de Student. En términos generales, la prueba de significancia global equivale a probar \\[ \\begin{split} &amp;H_0: \\beta_1 = 0 \\\\ &amp;H_A: \\beta_1 \\neq0 \\end{split} \\] Cuando utilizamos la Tabla ANOVA, el estadístico de prueba es \\[F_\\text{calculado} = \\frac{MSR}{MSE} \\sim F_{1, n-2}\\] donde \\(F_{1, n-2}\\) corresponde a la distribución \\(F\\) con \\(1\\) grado de libertad en el numerador y \\(n-2\\) grados de libertad en el denominador. El estadístico \\(F_\\text{calculado}\\) contrasta qué tanto de la varianza de \\(Y\\) puede explicarse con el factor controlable \\(X\\) versus los factores incontrolables del proceso. \\(F_\\text{calculado} &lt;&lt;&lt; 1\\) indica que los factores incontrolables explican más que el factor controlable \\(X\\). Si este fuera el caso, deberíamos explorar otro factor \\(X\\) para explicar \\(Y\\). Cuando \\(F_\\text{calculado} \\approx 1\\), la varianza de \\(Y\\) explicada por el factor controlable y los factores incontrolables es similar. En esta situación, lo mejor es explorar otro factor \\(X\\) para explicar \\(Y\\). Cuando \\(F_\\text{calculado} &gt;&gt;&gt; 1\\) concluimos que la variabilidad de la respuesta puede ser explicada mayormente por el factor controlable que por aquellos **factores incontrolables*. Este es el caso ideal. Para un nivel de significancia \\(\\alpha \\in (0,1)\\), rechazamos \\(H_0\\) si \\(F_\\text{calculado} &gt; F_{1-\\alpha,1,n-1}\\), donde \\(F_{\\alpha/2,1,n-2}\\) es el percentil \\(1-\\alpha\\) de una distribución \\(F\\) con 1 y \\(n-2\\) grados de libertad. En R, estos percentiles pueden calcularse utilizando la función qf. Por ejemplo, \\(F_{0.95,1,98}\\) es 3.94 y se obtiene como qf(0.95, 1, 98). El valor \\(p\\), calculado como \\(p=P(F_{1,n-1}&gt;F_\\text{calculado})\\), es menor que \\(\\alpha\\). Por ejemplo, si \\(F_\\text{calculado} = 7.34\\), el valor \\(p\\) para \\(n=49\\) será 0.009. Este valor se obtiene con la función pf() haciendo pf(7.34, 1, 49, lower.tail = FALSE) La prueba de significancia global se realiza de manera automática cuando empleamos usamos summary(modelo). La parte relevante de los resultados corresponde a la última línea de la salida del R, es decir, a F-statistic: 346 on 1 and 98 DF, p-value: &lt; 2.2e-16 En este caso, F-statistic corresponde al estadístico de prueba \\(F_\\text{calculado}\\), y los valores 1 y 98 a los grados de libertad del \\(MSR\\) y \\(MSE\\), respectivamente. Finalmente, el valor \\(p\\) es \\(p=2.2\\times10^{16}\\), por lo que rechazamos \\(H_0\\) y concluimos que incluir el modelo de RLS que incluye la Edad para explicar la Resistencia de una soldadura es mejor que no tener un modelo de sólo intercepto, es decir, un modelo que no incluye ningún factor controlable que probablemente modifique los valores de dicha Resistencia. Cuando utilizamos la distribución \\(t\\) de Student, podemos probar \\[ \\begin{split} &amp;H_0: \\beta_1 = 0 \\\\ &amp;H_A: \\beta_1 \\neq0 \\end{split} \\] A partir de las propiedades de los coeficientes estimados \\((\\widehat{\\beta}_0,\\widehat{\\beta}_1)\\) y bajo condiciones de regularidad, es fácil llegar a que \\[\\begin{align} \\widehat{\\beta}_0 \\sim N(\\beta_0, \\widehat{\\sigma}_{\\widehat{\\beta}_0}^2) \\\\ \\widehat{\\beta}_1 \\sim N(\\beta_1, \\widehat{\\sigma}_{\\widehat{\\beta}_1}^2) \\end{align}\\] donde \\[\\begin{align} \\widehat{\\sigma}_{\\widehat{\\beta}_0}^2 &amp;= \\sqrt{\\hat{\\sigma}^2\\left(\\frac{1}{n}+\\frac{\\bar{x}}{SS_x}\\right)} \\\\ \\widehat{\\sigma}_{\\widehat{\\beta}_1}^2 &amp;=\\sqrt{\\frac{\\hat{\\sigma}^2}{SS_x}} \\end{align}\\] Así, el estadístico de prueba será \\[t_{\\text{calculado}} = \\frac{\\hat{\\beta}_1}{\\widehat{\\sigma}_{\\widehat{\\beta}_1}}\\sim t_{n-2}\\] donde \\(t_{n-2}\\) corresponde a la distribución \\(t\\) con \\(n-2\\) grados de libertad. Bajo \\(H_0\\), \\(t_{\\text{calculado}} \\sim t_{n-2}\\). Para un nivel de significancia \\(\\alpha \\in (0,1)\\), rechazamos \\(H_0\\) si \\(|t_\\text{calculado}| &gt; t_{1-\\alpha/2, n-2}\\), donde \\(t_{1-\\alpha/2, n-2}\\) es el percentil \\(1-\\alpha/2\\) de una distribución \\(t\\) con \\(n-2\\) grados de libertad. En R, estos percentiles pueden calcularse utilizando la función qt. Por ejemplo, \\(t_{0.95,98}\\) es 1.66 y se obtiene como qt(0.95, 98). El valor \\(p\\), calculado como \\(p=2\\,P(t_{n-2} &gt; |t_\\text{calculado}|\\), es menor que \\(\\alpha\\). Por ejemplo, si \\(t_\\text{calculado} = 2.54\\) cuando \\(n=60\\), el valor \\(p\\) será 0.014. Para calcularlo, usamos 2*pt(abs(2.54), 58, lower.tail = FALSE) en R. Cuando \\(n-2 &gt; 30\\), podemos aproximar la distribución \\(t_{n-2}\\) a la distribución Normal estándar. En R, utilizaríamos la pnorm() en lugar de pt(). Para más detalles, escriba ?pnorm en la consola del R. Inferencia para \\(\\beta_0\\) y \\(\\beta_1\\) A partir de las distribuciones muestrales de \\(\\widehat{\\beta}_0\\) y \\(\\widehat{\\beta}_1\\) es posible realizar inferencia sobre \\(\\beta_0\\) y \\(\\beta_1\\). Este trabajo inferencial puede hacerse a través de: Pruebas de hipótesis Intervalos de confianza Cuando utilizamos pruebas de hipótesis, se prueban \\(H_0:\\beta_0 = 0\\) vs. \\(H_1:\\beta_0 \\neq 0\\) usando el estadístico de prueba \\[t_{\\text{calc}} = \\frac{\\hat{\\beta}_0-0}{\\widehat{\\sigma}_{\\widehat{\\beta}_0}}\\] \\(H_0:\\beta_1 = 0\\) vs. \\(H_1:\\beta_1 \\neq 0\\) usando el estadístico \\[t_{\\text{calc}} = \\frac{\\hat{\\beta}_1-0}{\\widehat{\\sigma}_{\\widehat{\\beta}_1}}\\] y, en cualquiera de los dos casos, rechazamos \\(H_0\\) si \\(|t_{\\text{calculado}}| &gt; t_{1-\\alpha/2,n-2}\\). Los intervalos de confianza del \\((1-\\alpha)100\\%\\) para los coeficientes de represión serán \\[\\begin{align} {\\beta}_0 \\in (\\hat{\\beta}_0 - t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_0},\\quad \\hat{\\beta}_0 + t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_0})\\\\ {\\beta}_1 \\in (\\hat{\\beta}_1 - t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_1},\\quad \\hat{\\beta}_1 + t_{\\alpha/2,n-2}\\,\\widehat{\\sigma}_{\\widehat{\\beta}_1}) \\end{align}\\] Decimos que, a nivel poblacional, \\(\\beta_0\\) y \\(\\beta_1\\) son estadísticamente cero si el intervalo de confianza contienen dicho valor. En nuestro ejemplo, podemos obtener los estadísticos de prueba para \\(\\beta_0\\) y \\(\\beta_1\\) utilizando el objeto modelo que contiene los resultados del modelo de RLS: ## estadísticos de prueba para beta_0 y beta_1 coefficients(summary(modelo)) ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 15.7427499 0.17868164 88.10502 3.989520e-95 ## Edad -0.2437636 0.01310455 -18.60144 6.394519e-34 En este caso, los estadístos de prueba se encuentran en la columna t value de la salida del R. Así, el estadístico para \\(\\beta_0\\) es \\(t_0=88.1\\) y el estadístico para \\(\\beta_1\\) es \\(t_1 = -18.6\\). Por otro lado, los valores \\(p\\) se encuentran en la columna Pr(&gt;|t|) y son, respectivamente, \\(3.98\\times 10^{-95}\\) y \\(6.39\\times 10^{-34}\\). Puesto que ambos valores \\(p\\) son inferiores a 0.05, concluimos que ambos parámetros son significativos (i.e., son diferentes de cero) a nivel poblacional con una confianza del 95%. Para el mismo nivel de significancia, los intervalos de confianza pueden calcularse como en R a través de la función confint.default: ## intervalos de confianza via confint confint.default(modelo) ## 2.5 % 97.5 % ## (Intercept) 15.392540 16.0929595 ## Edad -0.269448 -0.2180791 Por lo tanto, con una confianza del 95%, \\[\\begin{align} \\beta_0 &amp;\\in(15.39, 16.09) \\\\ \\beta_1 &amp;\\in(-0.269, -0.218) \\end{align}\\] También es posible construir intervalos de confianza para los coeficientes del modelo utilizando la función confint. Sin embargo, estos difieren levemente de aquellos basados en la distribución \\(t\\) o los construidos con la función confint.default. Esto se debe a que confint utiliza likelihood profiling. 2.4 Análisis de Residuales El análisis de residuales es fundamental en RLS puesto que es posible establecer si: Los errores del modelo ajustado cumplen con los supuestos; existen observaciones atípias y/u observaciones influenciales. A continuación se describen las estrategias utilizadas para establecer 1 y 2. Validación de Supuestos En la formulación matemática del modelo, se estableció que \\[\\epsilon\\sim N(0, \\sigma^2), \\hspace{1cm} \\sigma^2 = \\text{constante},\\] y \\(\\epsilon_1, \\epsilon_2, \\ldots, \\epsilon_n\\) son independientes. Por lo tanto, cuando se ajusta un modelo de RLS, debemos validar los siguientes supuestos sobre el error: Independencia; Normalidad; Media cero; y Varianza \\(\\sigma^2\\) constante. Sólo cuando se han validado todos los supuestos del error, podemos proceder a realizar predicción de futuros valores de la variable respuesta con el modelo de RLS ajustado. Independencia Para validar el supuesto de independencia de los errores se recomiendan dos aproximaciones: la prueba formal de independencia de Durbin-Watson y la prueba gráfica basada en la Función de Autocorrelación (ACF en inglés). La prueba de Durbin-Watson está implementada en la función durbinWatsonTest del paquete car. La ACF puede obtenerse utilizando la función acf. Para más información escriba ?acf en la consola del R. En R podemos realizar la prueba de Durbin-Watson haciendo: ## prueba de Durbin-Watson require(car) car:::durbinWatsonTest(modelo) ## lag Autocorrelation D-W Statistic p-value ## 1 -0.02653343 2.050609 0.736 ## Alternative hypothesis: rho != 0 Basados en el valor \\(p\\), no es posible rechazar \\(H_0\\) puesto que el valor \\(p\\) de la prueba de Durbin-Watson es superior a un nivel de significancia \\(\\alpha\\) del 5%. Por lo tanto, concluimos que los residuales del modelo ajustados son independientes. Para graficar la ACF podemos hacer: ## ACF de los residuales r &lt;- residuals(modelo) ## cálculo de los residuales acf(r, las = 1, main = &quot;&quot;) Para concluir que los residuales del modelo ajustado son independientes basados en la ACF, ninguna de las barras verticales, llamadas correlaciones, debe superar las bandas de color azul para \\(\\text{Lag} &gt; 0\\). En nuestro caso, podemos concluir que los residuales son independientes. Normalidad Formalmente, probar el supuesto de normalidad de los errores equivale a realizar el siguiente procedimiento de prueba de hipótesis: \\[ \\begin{split} &amp;H_0: \\text{los errores siguen una distribución Normal.} \\\\ &amp;H_A: \\text{los errores NO siguen una distribución Normal.} \\end{split} \\] Este supuesto puede validarse utilizando pruebas formales, o de manera gráfica utilizando histogramas, gráficos de densidad o un gráfico cuantil-cuantil, también conocido como Q-Q plot. Existen diferentes pruebas de Normalidad, varias de ellas implementadas en R. Prueba Shapiro-Wilk, implementada en la función shapiro.test. Prueba Anderson-Darling, implementada en la función ad.test del paquete nortest. Prueba Cramer-von Mises, implementada en la función cvm.test del paquete nortest. Prueba Lilliefors (Kolmogorov-Smirnov), implementada en la función lillie.test del paquete nortest. Prueba Pearson basada en la distribución \\(\\chi^2\\), implementada en la función pearson.test del paquete nortest. Prueba Shapiro-Francia, implementada en la función sf.test del paquete nortest). El Q-Q plot compara los cuantiles teóricos de la distribución Normal con los cuantiles muestrales del error estimado. Existen también varias implementaciones del gráfico cuantil-cuantil en R, entre las que se encuentran la función qqnorm del paquete base y la función qqPlot del paquete car. Para realizar por ejemplo la prueba de Normalidad de Shapiro-Wilk sobre los residuales del modelo ajustado, podemos proceder de la siguiente forma utilizando el objeto R que contiene los resultados de dicho modelo: ## prueba de Normalidad de Shapiro-Wilk shapiro.test(r) ## ## Shapiro-Wilk normality test ## ## data: r ## W = 0.98872, p-value = 0.5629 Estos resultados indican que el valor \\(p\\) de la prueba de Normalidad es \\(p = 0.563\\). Como \\(p &gt; 0.05\\), no rechazamos \\(H_0\\) y concluimos que los residuales del modelo ajustado siguen una distribución Normal. En R, podemos graficar un Q-Q plot de 2 formas: Vía gráficos básicos ## Q-Q plot básico qqnorm(r, las = 1, main = &quot;&quot;) qqline(r, col = 2) Para más detalles, escribir ?qqnorm en la consola del R. La idea fundamental es que los residuales del modelo (i.e., puntos en la gráfica) deben estar alrededor de la línea de color rojo para concluir, al menos gráficamente, que los residuales siguen una distribución Normal. En este caso, existen varios puntos por fuera de la línea, lo cual nos hace sospechar que esas observaciones outliers afectan el supesto. Utilizando el paquete car ## Q-Q plot usando el paquete car library(car, quietly = TRUE) car:::qqPlot(r, las = 1) ## [1] 77 18 Aunque la elaboración de este gráfico es mucho más compleja, su interpretación es simple: diremos que los residuales del modelo ajustado cumplen con el supuesto de Normalidad si todos los puntos en encuentran dentro de las bandas de color azul. En este caso, las observaciones 18 y 77 parecen ser extremas. Media cero Formalmente la validación del supuesto de media cero para los errores consiste en probar \\[ \\begin{split} &amp;H_0: \\mu_\\epsilon=0 \\\\ &amp;H_A: \\mu_\\epsilon\\neq0 \\end{split} \\] donde \\(\\mu_\\epsilon\\) es la media poblacional de \\(\\epsilon\\). Teniendo en cuenta que los coeficientes del modelo de RLS ajustado se obtienen a partir de MCO, no es necesario validar este supuesto. Básicamente, el método de MCO garantiza no rechazar \\(H_0\\). En caso de que se requiera hacerlo, podemos proceder de la siguiente manera en R: ## validación media cero en RLS t.test(r, mu = 0) ## ## One Sample t-test ## ## data: r ## t = -3.6292e-16, df = 99, p-value = 1 ## alternative hypothesis: true mean is not equal to 0 ## 95 percent confidence interval: ## -0.1166475 0.1166475 ## sample estimates: ## mean of x ## -2.133551e-17 De este resultado es fácil ver que el valor \\(p\\) de la prueba es \\(p=1\\), por lo que no rechazamos H_0. Finalmente concluimos, con una confianza del 95%, que la media de los residuales es cero a nivel poblacional. Varianza constante Por último, pero no menos importante, debemos validar el supuesto de varianza constante. Formalmente esto es equivalente a probar \\[ \\begin{split} &amp;H_0:\\sigma^2 \\text{ es constante}\\\\ &amp;H_1:\\sigma^2 \\text{ NO es constante} \\end{split} \\] Esta prueba puede realizarse formalmente a través de diferentes pruebas estadísticas como la prueba de Breusch-Pagan, o utilizando métodos gráficos. La prueba de Breusch-Pagan puede realizarse utilizando la función ncvTest del paquete car. Los métodos gráficos consisten en construir gráficos de dispersión para \\(\\hat{y}\\) vs. \\(y\\), \\(\\hat{y}\\) vs. \\(x\\) y \\(\\hat{y}\\) vs. el orden de medición. En el caso ideal, estos gráficos no deberían exhibir ningún patrón. Prueba de Breusch-Pagan Sin entrar en los detalles formales, la prueba de Breusch-Pagan busca determinar, estadísticamente, si la varianza del modelo de RLS ajustado es constante o no. En R podemos realizar esta prueba con a función ncvTest del paquete car: ## prueba de Breusch-Pagan para varianza constante car:::ncvTest(modelo) ## Non-constant Variance Score Test ## Variance formula: ~ fitted.values ## Chisquare = 12.55073, Df = 1, p = 0.00039605 Como el valor \\(p\\) de la prueba es mayor que 0.05, concluimos que la varianza del modelo no es costante. Aproximación gráfica Al usar esta aproximación, lo que buscamos es que los gráficos resultantes tenga un patrón aleatorio. En términos generales, podemos hacer gráficos de \\(\\hat{y}\\) vs. \\(x\\); \\(\\hat{y}\\) vs. \\(r\\); y \\(\\hat{y}\\) vs. orden de las observaciones, y evaluar los patrones observados. Esencialmente, buscamos que ocurra el patrón del panel superior izquierdo aquí. En R podemos proceder de la siguiente manera: ## cálculos y_hat &lt;- predict(modelo) # valores predichos de y x &lt;- datos$Edad # seleccionamos el predictor y &lt;- datos$Resistencia # seleccionamos la variable respuesta ## gráfico de residuales vs. valores predichos par(mfrow = c(2, 2), mar = c(4, 4, 2, 1)) plot(y_hat, r, las = 1, main = expression(hat(y) * &quot; vs. r&quot;), xlab = expression(hat(y)), ylab = &quot;Residuals&quot;) abline(h = 0, col = 2) plot(y_hat, las = 1, main = expression(hat(y) * &quot; vs. orden&quot;), xlab = &quot;Orden&quot;, ylab = expression(hat(y))) plot(y, r, las = 1, main = expression(&quot;r vs. y&quot;), ylab = &quot;Residuals&quot;) abline(h = 0, col = 2) plot(x, r, las = 1, main = expression(&quot;r vs. Edad&quot;), ylab = &quot;Residuals&quot;) abline(h = 0, col = 2) De los gráficos anteriores se concluye que la varianza del modelo no es constante. 2.5 Predicción Sólo cuando se validan todos los supuestos del modelo es posible realizar predicción. La idea fundamental es poder calcular \\(\\widehat{E[Y|X=x_0]} = \\widehat{\\mu}|_{X=x_0}\\), donde \\(x_0\\) es un valor específico del factor controlable \\(X\\). Adicionalmente, interesa construir intervalos de confianza e intervalos de predicción del \\((1-\\alpha)100\\%\\) para \\(\\mu|_{X=x_0}\\). Estos intervalos son fundamentales para determinar los valores máximos y mínimos del valor esperado de la respuesta \\(Y\\) para valores puntuales de \\(X\\) a nivel poblacional, para una confianza dada. Intervalo de confianza para \\(E[Y|X=x_0]\\) Para \\(\\alpha\\in(0,1)\\), el intervalo de confianza del \\((1-\\alpha)100\\%\\) para \\(\\mu |_{X=x_0}\\) está dado por: \\[\\widehat{\\mu} |_{X=x_0} \\pm t_{\\alpha/2,n-2}\\,\\hat{\\sigma}\\,\\sqrt{\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}}\\] Intervalo de predicción para \\(E[Y|X=x_0]\\) Similarmente, el intervalo de predicción para \\(Y |_{X=x_0}\\) será: \\[\\widehat{\\mu} |_{X=x_0} \\pm t_{\\alpha/2,n-2}\\,\\hat{\\sigma}\\,\\sqrt{1+\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}}\\] Observe que, aunque el intervalo de confianza y el intervalo de predicción están centrados en el mismo valor, el error estándar de \\(\\widehat{\\mu} |_{X=x_0}\\) es mayor en el intervalo de predicción que en el intervalo de confianza, es decir, \\[ \\sqrt{1+\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}} &gt; \\sqrt{\\frac{1}{n} + \\frac{(x_0-\\bar{x})^2}{S_x}} \\] Esto se debe a que, al construir intervalos de predicción, la incertidumbre de \\(\\widehat{\\mu} |_{X=x_0}\\) es mayor para valores de \\(X\\) por fuera de las condiciones de operación evaluadas al recolectar la muestra. Los intervalos de confianza permiten determinar el rango donde se encontrará el valor promedio de \\(Y\\) para un valor específico de \\(X\\) cuando se producen varias unidades experimentales en esas codiciones. Formalmente, esto es \\(\\widehat{\\mu}|_{X=x_0} = \\widehat{E[Y|X=x_0]}\\). Los intervalos de predicción permiten determinar el rango donde se encontrará el próximo valor de \\(Y\\) cuando \\(X=x_0\\). Aunque los residuales del modelo ajustado para los datos de soldadura no cumple con el supuesto de varianza constante, se mostrará cómo predecir la Resistencia de una soldadura con \\(\\text{Edad} = 20\\), es decir, \\(x_0=20\\). Con el objeto modelo, podemos construir el intervalo de confianza del 95% haciendo ## intervalo de confianza del 95% para E[Resistencia|Edad = 20] predict(modelo, newdata = data.frame(Edad = 20), interval = &quot;confidence&quot;) ## fit lwr upr ## 1 10.86748 10.64805 11.08691 Este resultado implica que, si medimos la Resistencia de varias soldaduras con \\(\\text{Edad} = 20\\), se espera que la Resistencia promedio sea 10.87 psi, y que, a nivel poblacional, dicho promedio se encuentre en el intervalo \\((10.65, 11.09)\\) con una confianza del 95%. Finalmente, el intervalo de predicción cuando \\(\\text{Edad} = 20\\) será: ## intervalo de confianza del 95% para Resistencia|Edad = 20 predict(modelo, newdata = data.frame(Edad = 20), interval = &quot;prediction&quot;) ## fit lwr upr ## 1 10.86748 9.674564 12.06039 Por lo tanto, se espera que en una próxima soldadura de \\(\\text{Edad} = 20\\) el valor de Resistencia sea 10.87 psi. A nivel poblacional, dicha Resistencia estará en el intervalo \\((9.67, 12.06)\\) el 95% de las veces. "],["rlm.html", "Capítulo 3 Regresión Lineal Múltiple 3.1 Formulación básica del modelo de RLM 3.2 Propiedades de los estimadores de \\(\\mathbf{\\beta}\\) 3.3 Estimación de \\(\\sigma^2\\) 3.4 Inferencia para \\(\\mathbf{\\beta}\\) 3.5 Inferencia para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) 3.6 Análisis de Residuales 3.7 Análisis de Multicolinealidad 3.8 Selección de Modelos 3.9 Ejercicios", " Capítulo 3 Regresión Lineal Múltiple Como se mencionó en el Capítulo 2, los modelos de regresión lineal pueden utilizarse para predecir futuros valores de una variable respuesta continua a partir de valores específicos de las variables controlables del proceso. En la práctica, pueden existir múltiples variables controlables en un proceso de producción o de servicios. Por ejemplo, en un proceso de pintura electrostática, puede ser de interés determinar el espesor de la capa de pintura (variable respuesta \\(y\\), en micrones) con la que se recubre una lámina de área determinada, a partir de valores conocidos de la presión de aire (variable \\(x_1\\) en psi) y la velocidad de la banda transportadora (variable \\(x_2\\) en m/s) en la que se desplaza dicha lámina. En este caso, el interés es: Determinar la magnitud de la influencia de las variables \\(x_1\\) y \\(x_2\\) sobre el espesor de capa esperado; construir una función \\(f(x_1,x_2)\\) que permita predecir el espesor de capa esperado; y construir intervalos de confianza y predicción para dicho valor. Si la variable respuesta \\(y\\) es continua y aproximadamente simétrica, podemos desarrollar 1, 2 y 3 a partir de la estimación de un modelo de regresión lineal. Puesto que el número de variables controlables es \\(k&gt;1\\), una posibilidad es utilizar el modelo de Regresión Lineal Múltiple (RLM). 3.1 Formulación básica del modelo de RLM Matemáticamente, el modelo de RLM puede expresarse como: \\[\\begin{align} \\label{mod1} y_i &amp;= \\beta_0 + \\beta_1X_{1i} + \\beta_2X_{2i} + \\cdots + \\beta_1X_{ki} + \\epsilon_i,\\\\ \\epsilon_i &amp;\\sim N(0, \\sigma^2), \\\\ \\sigma^2 &amp;= \\text{constante}. \\end{align}\\] Este modelo es equivalente a \\[\\begin{align} \\label{mod2} \\mathbf{y} &amp;= \\mathbf{X}\\mathbf{\\beta} + \\mathbf{\\epsilon} \\end{align}\\] donde \\(\\mathbf{y} = (y_1,y_2,\\ldots,y_n)\\) es el vector respuesta, \\(\\mathbf{X} = (\\mathbf{1}, \\mathbf{x}_1, \\mathbf{x}_2,\\ldots,\\mathbf{x}_k)_{n\\times p}\\) es la matriz de diseño y \\(\\mathbf{\\epsilon} = (\\epsilon_1,\\epsilon_2,\\ldots,\\epsilon_n)\\) es el error aleatorio. 3.1.1 Estimación Similar a RLS, la estimación del modelo de RLM se realiza utilizando el método de mínimos cuadrados ordinarios (MCO). A partir de una muestra aleatoria de tamaño \\(n\\) del proceso de producción, los datos se registran en una estructura rectangular similar a: De esta forma, se tienen \\(n\\) unidades experimentales para cada una de estas se determina el valor de la variable respuesta \\(y_i\\) para condiciones fijas \\(\\mathbf{X}_i\\). Por ejemplo, para la quinta unidad experimental, se obtuvo un valor la respuesta de \\(y_6\\) y las variables controlables tomaron valores fijos \\((x_{1,6}, x_{2,6}, \\ldots, x_{k,6})\\). Al igual que en RLS, la estimación del modelo de RLM realiza utilizando MCO. La idea fundamental consiste en minimizar \\[\\begin{eqnarray*}\\label{L} L &amp;=&amp;\\sum_{i=1}^n\\epsilon_i^2 = \\sum_{i=1}^n(Y_i-\\beta_0-\\beta_1X_{1,i} -\\beta_2X_{2,i} - \\ldots -\\beta_kX_{k,i} )^2. \\end{eqnarray*}\\] Los estimadores de mínimos cuadrados deben satisfacer las siguientes dos condiciones fundamentales: \\[\\begin{eqnarray*} \\frac{\\partial L}{\\partial \\beta_0} | _{\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots \\hat{\\beta}_k} &amp;=&amp; -2\\sum_{i=1}^n \\left(y_i-\\hat{\\beta}_0-\\sum_{j=1}^k{\\hat{\\beta}_jx_{ij}}\\right) = 0 \\\\ \\frac{\\partial L}{\\partial \\beta_j} | _{\\hat{\\beta}_0,\\hat{\\beta}_1,\\ldots \\hat{\\beta}_k} &amp;=&amp; -2\\sum_{i=1}^n \\left(y_i-\\hat{\\beta}_0-\\sum_{j=1}^k{\\hat{\\beta}_jx_{ij}}\\right)x_{ij} = 0 \\end{eqnarray*}\\] La solución al sistema de ecuaciones de condiciones fundamentales da origen al sistema de ecuaciones normales de mínimos cuadrados Por lo tanto, las solución de estas ecuaciones permite determinar \\(\\hat{\\mathbf{\\beta}}\\). Es fácil llegar a que el vector de coeficientes estimado para el modelo de RLM puede obtenerse como \\[ \\hat{\\mathbf{\\beta}} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime \\mathbf{y} \\] Finalmente, el modelo estimado es \\[\\hat{y}_i = \\hat{\\beta}_0+\\sum_{j=1}^k\\hat{\\beta}_jx_{ij},\\] que, matricialmente, puede representarse como \\[\\hat{\\mathbf{y}} = \\mathbf{X}\\hat{\\mathbf{\\beta}}\\] A partir del modelo ajustado, un valor específico \\(y_i\\) puede calculase como: \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_1 + \\hat{\\beta}_2x_2+\\cdots+\\hat{\\beta}_kx_k\\] Como ilustración, consideremos el siguiente ejemplo: ## lectura de datos datos &lt;- read.table() ## modelo de RLM modelo &lt;- lm() ## resultados summary(mod) 3.2 Propiedades de los estimadores de \\(\\mathbf{\\beta}\\) Cuando estimamos \\(\\hat{\\mathbf{\\beta}}\\), los cálculos están basados en los resultados obtenidos al tomar una muestra aleatoria de tamaño \\(n\\). Como consecuencia, el valor de los estimadores \\(\\mathbf{\\hat{\\beta}} = (\\hat{\\beta}_0, \\hat{\\beta}_1, \\ldots, \\hat{\\beta}_k)\\) cambian si cambiamos la muestra. Desde el punto de vista formal, los estimadores \\(\\mathbf{\\hat{\\beta}}\\) cumplen con las siguientes propiedades: Los estimadores \\(\\mathbf{\\hat\\beta}\\) son insesgados. Esta propiedad implica que, al aumentar \\(n\\), el valor de los estimadores de \\(\\mathbf{\\beta}\\) se aproximan a los verdaderos valores de los parámetros. Matemáticamente se tiene que: \\[\\begin{eqnarray} E[\\mathbf{\\hat\\beta}] &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{y}]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime(\\mathbf{X\\beta} + \\mathbf{\\epsilon})]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{X\\beta} + (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{\\epsilon}]\\\\\\nonumber &amp;=&amp; E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{X\\beta}] + E[(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime\\mathbf{\\epsilon}]\\\\\\nonumber &amp;=&amp; E[\\mathbf\\beta] + \\mathbf{0} = \\mathbf{\\beta}\\nonumber \\end{eqnarray}\\] La varianza de \\(\\hat{\\beta}_j\\) y la covarianza entre \\(\\hat\\beta_{i}\\) y \\(\\hat\\beta_{j}\\) están dadas por: \\[\\begin{eqnarray*} V(\\hat\\beta_{j}) &amp;=&amp; \\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj} \\hspace{0.5cm} 0,1,2,\\ldots,p;\\\\ \\text{cov}(\\hat\\beta_{i}, \\hat\\beta_{j}) &amp;=&amp; \\sigma^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{ij} \\hspace{0.5cm} i\\neq j. \\end{eqnarray*}\\] Ahora, a partir de \\(E[\\hat{\\beta}_j]\\) y \\(V(\\hat\\beta_{j})\\), es posible hacer inferencia para el parámetro \\(\\beta_j\\), \\(j=1,2,\\ldots,k.\\) Sin embargo, observe que \\(V(\\hat\\beta_{j})\\) depende de \\(\\sigma^2\\), la varianza del modelo de RLM. A continuación se muestra cómo se estima dicha cantidad. 3.3 Estimación de \\(\\sigma^2\\) A partir \\(\\mathbf{\\hat{\\beta}}\\), se tiene que \\[\\begin{eqnarray*}\\label{L2} L &amp;=&amp;\\sum_{i=1}^n\\hat{\\epsilon}_i^2 = \\sum_{i=1}^n(Y_i-\\hat{Y}_i^2)^2 \\\\ &amp;=&amp;\\sum_{i=1}^n(Y_i-\\hat{\\beta}_0-\\hat{\\beta}_1X_{1,i} -\\hat{\\beta}_2X_{2,i} - \\cdots -\\hat{\\beta}_kX_{k,i})^2 \\\\ &amp;=&amp; SSE \\end{eqnarray*}\\] Similar a como se observó en RLS, para el caso de RLM se tiene que \\[\\hat{\\sigma}^2 = \\frac{SSE}{n-p} = MSE\\] donde \\(p = k+1\\) es el número de coeficientes del modelo ajustado. Este resulado indica que la varianza de los errores, también conocida como la varianza del modelo, puede estimarse utilizando el MSE. El MSE se obtiene de la tabla ANOVA que tiene la siguiente forma: En RLM, también se cumple la misma relación que en RLS en cuanto que \\[SST = SSR + SSE \\] donde \\[\\begin{eqnarray} SST = \\sum_{i=1}^ny_i^2 - \\frac{1}{n}\\left(\\sum_{i=1}^ny_i \\right)^2, \\hspace{1cm} SSE = \\sum_{i=1}^n\\hat{\\epsilon}_i^2 \\\\\\nonumber \\end{eqnarray}\\] Recordemos que, adicional al MSE, a partir de la tabla ANOVA es posible calcular el porcentaje de variabilidad de la respuesta explicado por el modelo de RLM, también conocido como coeficiente de determinación o, simplemente, como \\(R^2\\): \\[R^2 = SSR/SST = 1 - SSE/SST\\] Puesto que \\(R^2\\) incrementa a medida que el número de variables aumenta, en RLM es preferible usar \\[R^2_{\\text{ajustado}} = 1 - \\frac{SSE/(n-p)}{SST/(n-1)}\\] 3.4 Inferencia para \\(\\mathbf{\\beta}\\) Uno de los propósitos de la inferencia estadística es determinar el valor de los verdaderos parámetros de una población a partir de los resultados obtenidos en una muestra. En este caso, los parámetros poblacionales son \\(\\mathbf{{\\beta}} = (\\beta_0, \\beta_1, \\ldots, \\beta_k)\\), además de \\(\\sigma^2\\). Con los valores muestrales, podemos construir pruebas de hipótesis de dos tipos para los parámetros del modelo de RLM: 3.4.1 Prueba de significancia global Esta prueba se utiliza para determinar la significancia total del modelo, es decir, para detemrinar si incluir las variables controlables en el modelo de regresión es mejor que no incluirlas para explicar la respuesta \\(Y\\). La idea fundamental es determinar si, en la población, \\[\\begin{eqnarray} H_0&amp;:&amp; \\beta_1=\\beta_2=\\cdots\\beta_k=0 \\\\\\nonumber H_1&amp;:&amp; \\text{Al menos un $\\beta_j \\neq 0$}\\nonumber \\end{eqnarray}\\] Este procedimiento de prueba de hipótesis se realiza a través de la tabla de ANOVA utilizando el estadístico \\(F\\) dado por \\[F_0 = = \\frac{SSR/k}{SSE/(n-p)} = \\frac{MSR}{MSE} \\sim F_{k, n-p}\\] Rechazamos \\(H_0: \\beta_1=\\beta_2=\\cdots\\beta_k=0\\) si \\(F_0 &gt; F_{\\alpha,k,n-p}\\), donde \\(\\alpha\\in(0,1)\\) es un nivel de significancia predeterminado. Cuando esto ocurre, concluimos que al menos un \\(\\beta_j\\) es estad'isticamente significativo al \\(100(1-\\alpha)\\%\\). 3.4.2 Prueba de significancia marginal Esta prueba se realiza si rechazamos la prueba de significancia global. Lo que intentamos hacer es determinar si, a nivel poblacional, los coeficientes asociados a cada \\(x_j\\) son o diferentes de cero. Esto es equivalente a probar: \\[\\begin{eqnarray*} H_0&amp;:&amp; \\beta_j=0 \\\\\\nonumber H_1&amp;:&amp; \\beta_j \\neq 0\\nonumber \\end{eqnarray*}\\] Para \\(j\\) fijo, el estadístico de prueba es \\[t_j = \\frac{\\hat{\\beta}_j - 0}{\\text{s.e.}(\\hat\\beta_j)} = \\frac{\\hat{\\beta}_j}{\\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}}}\\sim t_{n-p}\\] Por lo tanto, rechazamos \\(H_0\\) con un nivel de significancia de \\(100\\times(1-\\alpha)\\%\\) si \\(|t_j| &gt; t_{\\alpha/2, n-p}\\). 3.4.3 Intervalos de confianza para \\(\\beta_j\\) Otra forma de realizar inferencia para \\(\\mathbf{\\beta}\\) es a través de la construcción de intervalos de confianza del \\(100\\times(1-\\alpha)100\\%\\). Es fácil mostrar que, para \\(j\\) fijo, \\[\\begin{equation}\\label{eq:icbeta} \\beta_j\\in \\left( \\hat{\\beta}_j - t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}}, \\hat{\\beta}_j + t_{\\alpha/2, n-p} \\sqrt{\\hat{\\sigma}^2(\\mathbf{X}^\\prime\\mathbf{X})^{-1}_{jj}} \\right) \\end{equation}\\] Otra alternativa para construir intervalos de confianza es es vía o . Finalmente concluimos, con un nivel de confianza del \\(100\\times(1-\\alpha)\\%\\), que \\(\\beta_j\\) está en el intervalo anterior. 3.5 Inferencia para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) El modelo de RLM ajustado puede utilizarse para predecir \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) sólo si se validan todos los supuestos. Para más detalles ver Análisis de Residuales 3.5.1 Intervalos de confianza para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) A partir del modelo ajustado y para valores fijos de las variables controlables, digamos \\(\\mathbf{x}_0\\), se tiene que \\[\\begin{eqnarray} \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0} &amp;=&amp; \\hat{E[\\mathbf{Y} | \\mathbf{x}_0]} = \\mathbf{x}_0^\\prime\\hat{\\mathbf{\\beta}} \\\\\\nonumber V[\\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}] &amp;=&amp; \\hat{\\sigma}^2\\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0 \\end{eqnarray}\\] Finalmente, el intervalo de confianza del \\(100\\times(1-\\alpha)\\%\\) puede calcularse como \\[\\begin{eqnarray} \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0} \\pm t_{\\alpha/2,n-p}\\sqrt{\\hat{\\sigma}^2\\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0} \\end{eqnarray}\\] 3.5.2 Intervalos de predicción para \\(E[\\mathbf{Y}|\\mathbf{x}_0]\\) Sea \\(\\hat{y}_0 = \\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}\\), donde \\(\\mathbf{x}_0\\) es el vector de covariables futuro. Un intervalo de predicción del \\(100\\times(1-\\alpha)\\%\\) para \\(Y_0\\) está dado por: \\[\\begin{eqnarray} \\hat{y}_0 \\pm t_{\\alpha/2,n-p}\\sqrt{\\hat{\\sigma}^2(1 + \\mathbf{x}_0^\\prime(\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{x}_0)} \\end{eqnarray}\\] Otra posibilidad para construir dicho intervalo es vía bootstrap. Observe que lo único que cambia en este intervalo en relación con el intervalo de confianza es la varianza de \\(Y_0\\) \\(-\\) existe más incertidumbre. Observe que en el intervalo de predicción estamos interesados en \\(Y_0 | \\mathbf{x}_0\\) y no \\(\\hat{\\mu}_{\\mathbf{Y} | \\mathbf{x}_0}\\). 3.6 Análisis de Residuales El análisis de residuales en RLM es fundamental para: Validar los supuestos del error; identicar observaciones ; e identificar observaciones influenciales. 3.6.1 Validación de supuestos La validación de los supestos del error en el modelo de RLM se realiza de manera similar a como se mostró para el modelo de RLS. Para más detalles, ver Análisis de Residuales. 3.6.2 Identificación de outliers Los outliers son también conocidos como observaciones atípicas en los datos. Con los resultados del modelo ajustado, procedemos a calcular: Residuales crudos \\[\\hat{\\epsilon}_i = y_i - \\hat{y}_i\\] Residuales estandarizados \\[d_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\hat{\\sigma}^2}} = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{MSE}}} \\] Residuales estudentizados \\[r_i = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\hat{\\sigma}^2(1-h_{ii})}} = \\frac{\\hat{\\epsilon}_i}{\\sqrt{\\text{MSE}(1-h_{ii})}}\\] donde \\(h_{ii} = \\mathbf{X}(\\mathbf{X^\\prime\\mathbf{X}})^{-1}\\mathbf{X}^\\prime_{ii}\\) y \\(\\mathbf{X}(\\mathbf{X^\\prime\\mathbf{X}})^{-1}\\mathbf{X}\\) denominada la matriz hat. La mejor manera de identificar outliers es a partir del cálculo de los residuales estudentizados. Decimos que la \\(i\\)-ésima observación es un outlier si \\(r_i\\notin (-3, 3)\\). 3.6.3 Identificación de observaciones influenciales En ciertas ocasiones encontramos observaciones que lucen algo ‘anormales.’ Por lo tanto, es importante determinar si estas son influenciales o no. A diferencia de los outliers, las observaciones influenciales ‘controlan’ el modelo y por ello es importate determinar si el modelo ajustado es consistente cuando estas se remueven. La identificación de este tipo de observaciones se realiza utilizando la Distancia de Cook. Para la \\(i\\)-ésima observación, esta distancia se calcula como \\[ D_i = \\frac{r_i^2}{p}\\frac{h_{ii}}{(1-h_{ii})} = \\frac{\\hat{\\epsilon}_i^2 \\,h_{ii}}{p\\,\\hat{\\sigma}^2(1-h_{ii})^2}\\] donde \\(p\\) es el número de variables controlables incluídas en el modelo. Usualmente, observaciones en las que \\(D_i&gt;1\\) decimos que la \\(i\\)-ésima observacion es influencial. Sin embargo, se también se recomienda revisar las observaciones con \\(D_i&gt;\\frac{4}{n}\\) y \\(D_i&gt;\\frac{4}{n-k-1}\\). Éste último criterio es el utilizado por la función cooks.distance() del R. 3.7 Análisis de Multicolinealidad Cuando se utiliza el modelo de RLM, se asume que las variables controlables \\(X_1, X_2,\\ldots,X_k\\) son independientes. Desde el punto de vista práctico, esto tiene consideraciones importantes puesto que permite evaluar la magnitud del efecto de sobre \\(\\widehat{E[Y]}\\) cuando modificamos, en una unidad, digamos \\(x_j\\), mientras se mantienen el resto de ellas constantes. Este efecto corresponde, sin duda, a \\(\\hat{\\beta}_j\\). Sin embargo, cuando estas variables controlables no son independientes, este efecto no puede calcularse de la misma forma. Buscamos modelos en los que las covariables estén altamente correlacionadas con la respuesta, pero mínimamente entre ellas. Desde el punto de vista teórico, la existencia de no independencia en las variables controlables tiene consecuencias importantes sobre los estimadores de los parámetros del modelo dados por \\(\\mathbf{\\beta} = (\\beta_0, \\beta_1, \\beta_2, \\ldots, \\beta_k)\\). Cuando se usa el método de mínimos cuadrados, los estimadores de \\(\\mathbf{\\beta}\\) están dados por: \\[ \\hat{\\mathbf{\\beta}}_{\\text{OLS}} = (\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\mathbf{X}^\\prime \\mathbf{y} \\] La existencia de multicolinealidad es sinónimo de que no existe independencia en las variables controlables del modelo. Si esto es cierto, las columnas de la matriz de diseño \\(\\mathbf{X}\\) no son independientes, es decir, que la columna \\(x_j\\) puede expresarse como una combinación lineal de las demás. Matemáticamente, esto es equivalente a escribir \\(x_j \\sim x_{-j}\\) para algún \\(j\\). Por ejemplo, para \\(j=1\\) tendríamos \\[ x_1 \\sim x_2 + x_3 + \\cdots + x_k. \\] Esta expresión indica que la variable independiente/controlable \\(x_1\\) puede escribirse como una combinación lineal de las demás variables controlables. O, en otras palabras, que la información contenida en \\(x_1\\) puede explicarse por las demás variables controlables medidas en el proceso durante la etapa de muestreo. Una manera de interpretar la multicolinealidad es como sinónimo de redundancia. Esta redundancia se refiere a que existen variables controlables en el proceso de producción que contienen la misma información que las demás. Por lo tanto, basta con medir sólo aquellas que realmente determinan dicho proceso. En la expresión de \\(\\hat{\\mathbf{\\beta}}_{\\text{OLS}}\\), el término \\((\\mathbf{X}^\\prime\\mathbf{X})^{-1}\\) se refiere a la inversa de \\(\\mathbf{X}^\\prime\\mathbf{X}\\). Si existe multicolinealidad, \\[ \\text{det}(\\mathbf{X}^\\prime \\mathbf{X}) \\approx 0 \\quad \\rightarrow \\quad \\frac{1}{|\\mathbf{X}^\\prime \\mathbf{X}|} \\rightarrow\\infty \\] Por lo tanto, \\[ \\hat{\\mathbf{\\beta}}_{\\text{OLS}} \\rightarrow \\infty \\] 3.7.1 Cómo determinar que existe multicolinealidad? Existen varios indicadores para sospechar que existe multicolinealidad: Una alta correlación en las variables independientes. Esto es posible determinarlo gráficamente a través de una matriz de dispersión (ver por ejemplo ?pairs en la consola del R) o utilizando una prueba de independencia completa. Que se rechace la prueba de significancia global pero no todas las pruebas de significancia marginal. Que ocurran ambios considerables en \\(\\hat{\\mathbf{\\beta}}\\) cuando se agrega o elimina una variable predictora. Para probar que efectivamente existe, podemos usar dos aproximaciones: El número de condición de la matriz \\(\\mathbf{X}^\\prime \\mathbf{X}\\). También conocido como I-ll condicion number o ICN, este número mide qué tan “enferma” se encuentra la matriz que debe ser invertida para calcular \\[\\hat{\\mathbf{\\beta}}_{\\text{OLS}}\\]. El ICN se calcula como \\[\\text{ICN}(\\mathbf{X}^\\prime\\mathbf{X}) = \\sqrt{\\frac{\\lambda_\\text{máx}}{\\lambda_\\text{min}}} \\] con \\(\\lambda_\\text{máx}\\) y \\(\\lambda_\\text{min}\\) los valores propios máximos y mínimos de \\(\\mathbf{X}^\\prime \\mathbf{X}\\), obtenidos a partir de la descomposición espectral de dicha matriz. Decimos que existe multicolinealidad cuando \\(\\text{ICN}(\\mathbf{X}^\\prime \\mathbf{X}) &gt; 30\\). El inconveniente con el ICN es que no nos da información acerca de cuál de las variables independentes es la más multicolineal (o redundante) en el sistema. El factor de inflación de varianza (VIF). A través de este indicador podemos detectar cuál de las variables independientes es la más colineal de las \\(k\\) medidas. Para la \\(j\\)-ésima variable independiente, \\[\\text{VIF}_j = \\frac{1}{1-R_j^2}\\] donde \\(R_j^2\\) es el \\(R^2_\\text{adjusted}\\) del modelo \\(x_j\\sim x_{-j}\\). Decimos que la variable \\(x_j\\) es responsable por la multicolineal en el sistema si \\(\\text{VIF}_j &gt; 5\\). 3.8 Selección de Modelos 3.8.1 Método de Todas las Regresiones Posibles 3.8.2 Selección secuencial Método Método Método 3.9 Ejercicios "],["glm.html", "Capítulo 4 Modelos de Regresión Avanzados 4.1 Regresión No Lineal 4.2 Regresión Logística 4.3 Regresión Poisson", " Capítulo 4 Modelos de Regresión Avanzados Here is a review of existing methods. 4.1 Regresión No Lineal 4.1.1 Ejercicios 4.2 Regresión Logística 4.2.1 Ejercicios 4.3 Regresión Poisson 4.3.1 Variaciones Aunque no hace parte del contenido del curso, el modelo de Regresión Poisson tiene algunas variaciones. El tipo de model 4.3.2 Ejercicios "],["series.html", "Capítulo 5 Introducción a Series de Tiempo 5.1 Qué es una Serie de Tiempo? 5.2 Definiciones básicas 5.3 Por qué y para qué? 5.4 Modelos básicos 5.5 Validación de supuestos 5.6 Pronósticos 5.7 Ejercicios", " Capítulo 5 Introducción a Series de Tiempo 5.1 Qué es una Serie de Tiempo? 5.2 Definiciones básicas 5.3 Por qué y para qué? 5.4 Modelos básicos 5.4.1 Método de Descomposición 5.4.2 Métodos de Suavizamiento 5.4.3 Metodología Box-Jenkins 5.5 Validación de supuestos 5.6 Pronósticos 5.7 Ejercicios "],["enp.html", "Capítulo 6 Estadística No Paramétrica 6.1 Por qué y para qué? 6.2 Modelos básicos 6.3 Ejercicios", " Capítulo 6 Estadística No Paramétrica 6.1 Por qué y para qué? 6.2 Modelos básicos 6.2.1 Prueba de signos 6.2.1.1 Ejemplos 6.2.2 Prueba de Rangos con Signos 6.2.2.1 Ejemplos 6.2.3 Prueba de Mann-Whitney-Wilcoxon 6.2.3.1 Ejemplos 6.2.4 Prueba de Kruskal-Wallis 6.2.4.1 Ejemplos 6.3 Ejercicios "],["palabras-finales.html", "Capítulo 7 Palabras Finales", " Capítulo 7 Palabras Finales We have finished a nice book. "],["referencias.html", "Referencias", " Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Capítulo 3 Regresión Lineal Múltiple | Modelos de Regresión: Una aproximación práctica con R</title>
  <meta name="description" content="Este libro recopila mi años de experiencia en el aprendizaje de R (R Core Team 2019), y está escrito como una guía para aquellos que recién comienzan." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Capítulo 3 Regresión Lineal Múltiple | Modelos de Regresión: Una aproximación práctica con R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Este libro recopila mi años de experiencia en el aprendizaje de R (R Core Team 2019), y está escrito como una guía para aquellos que recién comienzan." />
  <meta name="github-repo" content="https://github.com/jivelez/book-adii" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Capítulo 3 Regresión Lineal Múltiple | Modelos de Regresión: Una aproximación práctica con R" />
  
  <meta name="twitter:description" content="Este libro recopila mi años de experiencia en el aprendizaje de R (R Core Team 2019), y está escrito como una guía para aquellos que recién comienzan." />
  

<meta name="author" content="Jorge I. Vélez" />


<meta name="date" content="2021-09-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="rls.html"/>
<link rel="next" href="glm.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Modelos de Regresión con R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bienvenido</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#estructura-del-libro"><i class="fa fa-check"></i>Estructura del libro</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#software-y-convenciones"><i class="fa fa-check"></i>Software y convenciones</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#bloques-informativos"><i class="fa fa-check"></i>Bloques informativos</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#dedicatoria"><i class="fa fa-check"></i>Dedicatoria</a></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#sobre-el-autor"><i class="fa fa-check"></i>Sobre el autor</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introducción</a>
<ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#analítica-de-datos"><i class="fa fa-check"></i><b>1.1</b> Analítica de Datos</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="intro.html"><a href="intro.html#conceptos-básicos"><i class="fa fa-check"></i><b>1.1.1</b> Conceptos básicos</a></li>
<li class="chapter" data-level="1.1.2" data-path="intro.html"><a href="intro.html#gráficos-básicos"><i class="fa fa-check"></i><b>1.1.2</b> Gráficos básicos</a></li>
<li class="chapter" data-level="1.1.3" data-path="intro.html"><a href="intro.html#tópicos-avanzados"><i class="fa fa-check"></i><b>1.1.3</b> Tópicos avanzados</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#r"><i class="fa fa-check"></i><b>1.2</b> <code>R</code></a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="intro.html"><a href="intro.html#por-qué-r"><i class="fa fa-check"></i><b>1.2.1</b> Por qué <code>R</code>?</a></li>
<li class="chapter" data-level="1.2.2" data-path="intro.html"><a href="intro.html#cran"><i class="fa fa-check"></i><b>1.2.2</b> <code>CRAN</code></a></li>
<li class="chapter" data-level="1.2.3" data-path="intro.html"><a href="intro.html#descarga-e-instalación"><i class="fa fa-check"></i><b>1.2.3</b> Descarga e instalación</a></li>
<li class="chapter" data-level="1.2.4" data-path="intro.html"><a href="intro.html#paquetes"><i class="fa fa-check"></i><b>1.2.4</b> Paquetes</a></li>
<li class="chapter" data-level="1.2.5" data-path="intro.html"><a href="intro.html#operadores-básicos"><i class="fa fa-check"></i><b>1.2.5</b> Operadores básicos</a></li>
<li class="chapter" data-level="1.2.6" data-path="intro.html"><a href="intro.html#creación-de-funciones"><i class="fa fa-check"></i><b>1.2.6</b> Creación de funciones</a></li>
<li class="chapter" data-level="1.2.7" data-path="intro.html"><a href="intro.html#obteniendo-ayuda"><i class="fa fa-check"></i><b>1.2.7</b> Obteniendo ayuda</a></li>
<li class="chapter" data-level="1.2.8" data-path="intro.html"><a href="intro.html#lectura-de-datos"><i class="fa fa-check"></i><b>1.2.8</b> Lectura de datos</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#rstudio"><i class="fa fa-check"></i><b>1.3</b> RStudio</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="rls.html"><a href="rls.html"><i class="fa fa-check"></i><b>2</b> Regresión Lineal Simple</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rls.html"><a href="rls.html#formulación-básica-del-modelo-de-rls"><i class="fa fa-check"></i><b>2.1</b> Formulación básica del modelo de RLS</a></li>
<li class="chapter" data-level="2.2" data-path="rls.html"><a href="rls.html#minimos"><i class="fa fa-check"></i><b>2.2</b> Estimación</a></li>
<li class="chapter" data-level="2.3" data-path="rls.html"><a href="rls.html#tabla-anova-y-medidas-de-desempeño"><i class="fa fa-check"></i><b>2.3</b> Tabla ANOVA y medidas de desempeño</a>
<ul>
<li><a href="rls.html#estimación-de-sigma2">Estimación de <span class="math inline">\(\sigma^2\)</span></a></li>
<li><a href="rls.html#inferencia-para-sigma2">Inferencia para <span class="math inline">\(\sigma^2\)</span></a></li>
<li><a href="rls.html#coeficiente-de-determinación-r2">Coeficiente de determinación <span class="math inline">\(R^2\)</span></a></li>
<li class="chapter" data-level="" data-path="rls.html"><a href="rls.html#validación-del-modelo-de-rls"><i class="fa fa-check"></i>Validación del modelo de RLS</a></li>
<li><a href="rls.html#inferencia-para-beta_0-y-beta_1">Inferencia para <span class="math inline">\(\beta_0\)</span> y <span class="math inline">\(\beta_1\)</span></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="rls.html"><a href="rls.html#residuales"><i class="fa fa-check"></i><b>2.4</b> Análisis de Residuales</a>
<ul>
<li class="chapter" data-level="" data-path="rls.html"><a href="rls.html#validación-de-supuestos"><i class="fa fa-check"></i>Validación de Supuestos</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="rls.html"><a href="rls.html#predicción"><i class="fa fa-check"></i><b>2.5</b> Predicción</a>
<ul>
<li><a href="rls.html#intervalo-de-confianza-para-eyxx_0">Intervalo de confianza para <span class="math inline">\(E[Y|X=x_0]\)</span></a></li>
<li><a href="rls.html#intervalo-de-predicción-para-eyxx_0">Intervalo de predicción para <span class="math inline">\(E[Y|X=x_0]\)</span></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>3</b> Regresión Lineal Múltiple</a>
<ul>
<li class="chapter" data-level="3.1" data-path="rlm.html"><a href="rlm.html#formulación-básica-del-modelo-de-rlm"><i class="fa fa-check"></i><b>3.1</b> Formulación básica del modelo de RLM</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="rlm.html"><a href="rlm.html#estimacion"><i class="fa fa-check"></i><b>3.1.1</b> Estimación</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="rlm.html"><a href="rlm.html#propiedades-de-los-estimadores-de-mathbfbeta"><i class="fa fa-check"></i><b>3.2</b> Propiedades de los estimadores de <span class="math inline">\({\mathbf{\beta}}\)</span></a></li>
<li class="chapter" data-level="3.3" data-path="rlm.html"><a href="rlm.html#estimación-de-sigma2-1"><i class="fa fa-check"></i><b>3.3</b> Estimación de <span class="math inline">\(\sigma^2\)</span></a></li>
<li class="chapter" data-level="3.4" data-path="rlm.html"><a href="rlm.html#inferencia-para-mathbfbeta"><i class="fa fa-check"></i><b>3.4</b> Inferencia para <span class="math inline">\(\mathbf{\beta}\)</span></a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="rlm.html"><a href="rlm.html#prueba-de-significancia-global"><i class="fa fa-check"></i><b>3.4.1</b> Prueba de significancia global</a></li>
<li class="chapter" data-level="3.4.2" data-path="rlm.html"><a href="rlm.html#prueba-de-significancia-marginal"><i class="fa fa-check"></i><b>3.4.2</b> Prueba de significancia marginal</a></li>
<li class="chapter" data-level="3.4.3" data-path="rlm.html"><a href="rlm.html#intervalos-de-confianza-para-beta_j"><i class="fa fa-check"></i><b>3.4.3</b> Intervalos de confianza para <span class="math inline">\(\beta_j\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="rlm.html"><a href="rlm.html#inferencia-para-la-respuesta"><i class="fa fa-check"></i><b>3.5</b> Inferencia para la respuesta</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="rlm.html"><a href="rlm.html#intervalos-de-confianza-para-emathbfymathbfx_0"><i class="fa fa-check"></i><b>3.5.1</b> Intervalos de confianza para <span class="math inline">\(E[\mathbf{Y}|\mathbf{x}_0]\)</span></a></li>
<li class="chapter" data-level="3.5.2" data-path="rlm.html"><a href="rlm.html#intervalos-de-predicción-para-emathbfymathbfx_0"><i class="fa fa-check"></i><b>3.5.2</b> Intervalos de predicción para <span class="math inline">\(E[\mathbf{Y}|\mathbf{x}_0]\)</span></a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="rlm.html"><a href="rlm.html#análisis-de-residuales"><i class="fa fa-check"></i><b>3.6</b> Análisis de Residuales</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="rlm.html"><a href="rlm.html#validación-de-supuestos-1"><i class="fa fa-check"></i><b>3.6.1</b> Validación de supuestos</a></li>
<li class="chapter" data-level="3.6.2" data-path="rlm.html"><a href="rlm.html#identificación-de-outliers"><i class="fa fa-check"></i><b>3.6.2</b> Identificación de <em>outliers</em></a></li>
<li class="chapter" data-level="3.6.3" data-path="rlm.html"><a href="rlm.html#identificación-de-observaciones-influenciales"><i class="fa fa-check"></i><b>3.6.3</b> Identificación de observaciones influenciales</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="rlm.html"><a href="rlm.html#análisis-de-multicolinealidad"><i class="fa fa-check"></i><b>3.7</b> Análisis de Multicolinealidad</a>
<ul>
<li class="chapter" data-level="3.7.1" data-path="rlm.html"><a href="rlm.html#cómo-detectarla"><i class="fa fa-check"></i><b>3.7.1</b> Cómo detectarla?</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="rlm.html"><a href="rlm.html#selección-de-modelos"><i class="fa fa-check"></i><b>3.8</b> Selección de Modelos</a>
<ul>
<li class="chapter" data-level="3.8.1" data-path="rlm.html"><a href="rlm.html#método-de-todas-las-regresiones-posibles"><i class="fa fa-check"></i><b>3.8.1</b> Método de Todas las Regresiones Posibles</a></li>
<li class="chapter" data-level="3.8.2" data-path="rlm.html"><a href="rlm.html#selección-secuencial"><i class="fa fa-check"></i><b>3.8.2</b> Selección secuencial</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="glm.html"><a href="glm.html"><i class="fa fa-check"></i><b>4</b> Modelos de Regresión Avanzados</a>
<ul>
<li class="chapter" data-level="4.1" data-path="glm.html"><a href="glm.html#regresión-no-lineal"><i class="fa fa-check"></i><b>4.1</b> Regresión No Lineal</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="glm.html"><a href="glm.html#introducción"><i class="fa fa-check"></i><b>4.1.1</b> Introducción</a></li>
<li class="chapter" data-level="4.1.2" data-path="glm.html"><a href="glm.html#por-qué"><i class="fa fa-check"></i><b>4.1.2</b> Por qué?</a></li>
<li class="chapter" data-level="4.1.3" data-path="glm.html"><a href="glm.html#ejemplo"><i class="fa fa-check"></i><b>4.1.3</b> Ejemplo</a></li>
<li class="chapter" data-level="4.1.4" data-path="glm.html"><a href="glm.html#modelo-ajustado"><i class="fa fa-check"></i><b>4.1.4</b> Modelo ajustado</a></li>
<li class="chapter" data-level="4.1.5" data-path="glm.html"><a href="glm.html#inferencia"><i class="fa fa-check"></i><b>4.1.5</b> Inferencia</a></li>
<li class="chapter" data-level="4.1.6" data-path="glm.html"><a href="glm.html#medidas-de-desempeño"><i class="fa fa-check"></i><b>4.1.6</b> Medidas de desempeño</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="glm.html"><a href="glm.html#regresión-logística"><i class="fa fa-check"></i><b>4.2</b> Regresión Logística</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="glm.html"><a href="glm.html#introducción-1"><i class="fa fa-check"></i><b>4.2.1</b> Introducción</a></li>
<li class="chapter" data-level="4.2.2" data-path="glm.html"><a href="glm.html#por-qué-1"><i class="fa fa-check"></i><b>4.2.2</b> Por qué?</a></li>
<li class="chapter" data-level="4.2.3" data-path="glm.html"><a href="glm.html#ejemplo-1"><i class="fa fa-check"></i><b>4.2.3</b> Ejemplo</a></li>
<li class="chapter" data-level="4.2.4" data-path="glm.html"><a href="glm.html#modelo-ajustado-1"><i class="fa fa-check"></i><b>4.2.4</b> Modelo ajustado</a></li>
<li class="chapter" data-level="4.2.5" data-path="glm.html"><a href="glm.html#inferencia-1"><i class="fa fa-check"></i><b>4.2.5</b> Inferencia</a></li>
<li class="chapter" data-level="4.2.6" data-path="glm.html"><a href="glm.html#medidas-de-desempeño-1"><i class="fa fa-check"></i><b>4.2.6</b> Medidas de desempeño</a></li>
<li class="chapter" data-level="" data-path="glm.html"><a href="glm.html#matriz-de-confusión"><i class="fa fa-check"></i>Matriz de confusión</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="glm.html"><a href="glm.html#regresión-poisson"><i class="fa fa-check"></i><b>4.3</b> Regresión Poisson</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="glm.html"><a href="glm.html#introducción-2"><i class="fa fa-check"></i><b>4.3.1</b> Introducción</a></li>
<li class="chapter" data-level="4.3.2" data-path="glm.html"><a href="glm.html#por-qué-2"><i class="fa fa-check"></i><b>4.3.2</b> Por qué?</a></li>
<li class="chapter" data-level="4.3.3" data-path="glm.html"><a href="glm.html#ejemplo-2"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo</a></li>
<li class="chapter" data-level="4.3.4" data-path="glm.html"><a href="glm.html#modelo-ajustado-2"><i class="fa fa-check"></i><b>4.3.4</b> Modelo ajustado</a></li>
<li class="chapter" data-level="4.3.5" data-path="glm.html"><a href="glm.html#inferencia-para-lambda"><i class="fa fa-check"></i><b>4.3.5</b> Inferencia para <span class="math inline">\(\lambda\)</span></a></li>
<li class="chapter" data-level="4.3.6" data-path="glm.html"><a href="glm.html#estimación-del-número-de-errores"><i class="fa fa-check"></i><b>4.3.6</b> Estimación del número de errores</a></li>
<li class="chapter" data-level="4.3.7" data-path="glm.html"><a href="glm.html#cálculo-de-probabilidades"><i class="fa fa-check"></i><b>4.3.7</b> Cálculo de probabilidades</a></li>
<li class="chapter" data-level="4.3.8" data-path="glm.html"><a href="glm.html#variaciones"><i class="fa fa-check"></i><b>4.3.8</b> Variaciones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="series.html"><a href="series.html"><i class="fa fa-check"></i><b>5</b> Introducción a Series de Tiempo</a>
<ul>
<li class="chapter" data-level="5.1" data-path="series.html"><a href="series.html#qué-es-una-serie-de-tiempo"><i class="fa fa-check"></i><b>5.1</b> Qué es una Serie de Tiempo?</a></li>
<li class="chapter" data-level="5.2" data-path="series.html"><a href="series.html#definiciones-básicas"><i class="fa fa-check"></i><b>5.2</b> Definiciones básicas</a></li>
<li class="chapter" data-level="5.3" data-path="series.html"><a href="series.html#por-qué-y-para-qué"><i class="fa fa-check"></i><b>5.3</b> Por qué y para qué?</a></li>
<li class="chapter" data-level="5.4" data-path="series.html"><a href="series.html#modelos-básicos"><i class="fa fa-check"></i><b>5.4</b> Modelos básicos</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="series.html"><a href="series.html#método-de-descomposición"><i class="fa fa-check"></i><b>5.4.1</b> Método de Descomposición</a></li>
<li class="chapter" data-level="5.4.2" data-path="series.html"><a href="series.html#métodos-de-suavizamiento"><i class="fa fa-check"></i><b>5.4.2</b> Métodos de Suavizamiento</a></li>
<li class="chapter" data-level="5.4.3" data-path="series.html"><a href="series.html#metodología-box-jenkins"><i class="fa fa-check"></i><b>5.4.3</b> Metodología Box-Jenkins</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="series.html"><a href="series.html#validación-de-supuestos-2"><i class="fa fa-check"></i><b>5.5</b> Validación de supuestos</a></li>
<li class="chapter" data-level="5.6" data-path="series.html"><a href="series.html#pronósticos"><i class="fa fa-check"></i><b>5.6</b> Pronósticos</a></li>
<li class="chapter" data-level="5.7" data-path="series.html"><a href="series.html#ejercicios"><i class="fa fa-check"></i><b>5.7</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="enp.html"><a href="enp.html"><i class="fa fa-check"></i><b>6</b> Estadística No Paramétrica</a>
<ul>
<li class="chapter" data-level="6.1" data-path="enp.html"><a href="enp.html#por-qué-y-para-qué-1"><i class="fa fa-check"></i><b>6.1</b> Por qué y para qué?</a></li>
<li class="chapter" data-level="6.2" data-path="enp.html"><a href="enp.html#modelos-básicos-1"><i class="fa fa-check"></i><b>6.2</b> Modelos básicos</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="enp.html"><a href="enp.html#prueba-de-signos"><i class="fa fa-check"></i><b>6.2.1</b> Prueba de signos</a></li>
<li class="chapter" data-level="6.2.2" data-path="enp.html"><a href="enp.html#prueba-de-rangos-con-signos"><i class="fa fa-check"></i><b>6.2.2</b> Prueba de Rangos con Signos</a></li>
<li class="chapter" data-level="6.2.3" data-path="enp.html"><a href="enp.html#prueba-de-mann-whitney-wilcoxon"><i class="fa fa-check"></i><b>6.2.3</b> Prueba de Mann-Whitney-Wilcoxon</a></li>
<li class="chapter" data-level="6.2.4" data-path="enp.html"><a href="enp.html#prueba-de-kruskal-wallis"><i class="fa fa-check"></i><b>6.2.4</b> Prueba de Kruskal-Wallis</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="enp.html"><a href="enp.html#ejercicios-1"><i class="fa fa-check"></i><b>6.3</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="palabras-finales.html"><a href="palabras-finales.html"><i class="fa fa-check"></i><b>7</b> Palabras Finales</a></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a></li>
<li class="divider"></li>
<li><a href="https://github.com/jivelez/book-adii" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Modelos de Regresión: Una aproximación práctica con R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="rlm" class="section level1" number="3">
<h1><span class="header-section-number">Capítulo 3</span> Regresión Lineal Múltiple</h1>
<p>Como se mencionó en el Capítulo <a href="rls.html#rls">2</a>, los modelos de regresión lineal pueden utilizarse para predecir futuros valores de una variable respuesta continua a partir de valores <em>específicos</em> de las variables <em>controlables</em> del proceso.</p>
<p>En la práctica, pueden existir múltiples variables controlables en un proceso de producción o de servicios. Por ejemplo, en un proceso de pintura electrostática, puede ser de interés determinar el espesor de la capa de pintura (variable respuesta <span class="math inline">\(y\)</span>, en micrones) con la que se recubre una lámina de área determinada, a partir de valores conocidos de la presión de aire (variable <span class="math inline">\(x_1\)</span> en <em>psi</em>) y la velocidad de la banda transportadora (variable <span class="math inline">\(x_2\)</span> en m/s) en la que se desplaza dicha lámina. En este caso, el interés es:</p>
<ol style="list-style-type: decimal">
<li>Determinar la <em>magnitud de la influencia</em> de las variables <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> sobre el espesor de capa esperado;</li>
<li>construir una función <span class="math inline">\(f(x_1,x_2)\)</span> que permita predecir el espesor de capa esperado; y</li>
<li>construir intervalos de confianza y predicción para dicho valor.</li>
</ol>
<p>Si la variable respuesta <span class="math inline">\(y\)</span> es continua y aproximadamente simétrica, podemos desarrollar 1, 2 y 3 a partir de la estimación de un modelo de regresión lineal. Puesto que el número de variables controlables es <span class="math inline">\(k&gt;1\)</span>, una posibilidad es utilizar el modelo de Regresión Lineal Múltiple (RLM).</p>

<div class="rmdnote">
El modelo de RLM es una <em>extensión</em> del modelo de RLS cuando se tiene más de una variable controlable.
</div>
<div id="formulación-básica-del-modelo-de-rlm" class="section level2" number="3.1">
<h2><span class="header-section-number">3.1</span> Formulación básica del modelo de RLM</h2>
<p>Matemáticamente, el modelo de RLM puede expresarse como:</p>
<p><span class="math display">\[\begin{align} \label{mod1}
y_i &amp;= \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \cdots + \beta_1X_{ki} + \epsilon_i,\\ 
\epsilon_i &amp;\sim N(0, \sigma^2), \\
\sigma^2 &amp;= \text{constante}.
\end{align}\]</span></p>
<p>Este modelo es equivalente a</p>
<p><span class="math display">\[\begin{align} \label{mod2}
\mathbf{y} &amp;= \mathbf{X}\mathbf{\beta} + \mathbf{\epsilon}
\end{align}\]</span></p>
<p>donde <span class="math inline">\(\mathbf{y} = (y_1,y_2,\ldots,y_n)\)</span> es el vector respuesta, <span class="math inline">\(\mathbf{X} = (\mathbf{1}, \mathbf{x}_1, \mathbf{x}_2,\ldots,\mathbf{x}_k)_{n\times p}\)</span> es la matriz de diseño y <span class="math inline">\(\mathbf{\epsilon} = (\epsilon_1,\epsilon_2,\ldots,\epsilon_n)\)</span> es el error aleatorio.</p>
<div id="estimacion" class="section level3" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Estimación</h3>
<p>Similar a RLS, la estimación del modelo de RLM se realiza utilizando el método de mínimos cuadrados ordinarios (MCO).</p>
<p>A partir de una muestra aleatoria de tamaño <span class="math inline">\(n\)</span> del proceso de producción, los datos se registran en una estructura rectangular similar a:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:str"></span>
<img src="images/datastructure.png" alt="Estructura de datos en RLM." width="40%" />
<p class="caption">
Figura 3.1: Estructura de datos en RLM.
</p>
</div>
<p>De esta forma, se tienen <span class="math inline">\(n\)</span> unidades experimentales para cada una de estas se determina el valor de la variable respuesta <span class="math inline">\(y_i\)</span> para condiciones fijas <span class="math inline">\(\mathbf{X}_i\)</span>. Por ejemplo, para la quinta unidad experimental, se obtuvo el valor <span class="math inline">\(y_6\)</span> cuando las variables controlables tomaron los valores fijos <span class="math inline">\((x_{1,6}, x_{2,6}, \ldots, x_{k,6})\)</span>.</p>
<p>Al igual que en RLS, la estimación del modelo de RLM realiza utilizando minimos cuadrados La idea fundamental consiste en minimizar</p>
<p><span class="math display">\[\begin{eqnarray*}\label{L}
L &amp;=&amp;\sum_{i=1}^n\epsilon_i^2 = \sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{1,i} -\beta_2X_{2,i} - \ldots -\beta_kX_{k,i} )^2.
\end{eqnarray*}\]</span></p>
<p>Los estimadores de mínimos cuadrados deben satisfacer las siguientes dos condiciones fundamentales:</p>
<p><span class="math display">\[\begin{eqnarray*}
  \frac{\partial L}{\partial \beta_0} | _{\hat{\beta}_0,\hat{\beta}_1,\ldots \hat{\beta}_k} &amp;=&amp; -2\sum_{i=1}^n \left(y_i-\hat{\beta}_0-\sum_{j=1}^k{\hat{\beta}_jx_{ij}}\right) = 0   \\ 
  \frac{\partial L}{\partial \beta_j} | _{\hat{\beta}_0,\hat{\beta}_1,\ldots \hat{\beta}_k} &amp;=&amp; -2\sum_{i=1}^n \left(y_i-\hat{\beta}_0-\sum_{j=1}^k{\hat{\beta}_jx_{ij}}\right)x_{ij} = 0 
  \end{eqnarray*}\]</span></p>
<p>La solución al sistema de ecuaciones de condiciones fundamentales da origen al sistema de ecuaciones normales de mínimos cuadrados dado por</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ecuaciones"></span>
<img src="images/ecuaciones.png" alt="Ecuaciones normales de mínimos cuadrados." width="70%" />
<p class="caption">
Figura 3.2: Ecuaciones normales de mínimos cuadrados.
</p>
</div>
<p>Las solución de estas ecuaciones permite determinar <span class="math inline">\(\hat{\mathbf{\beta}}\)</span>. Es fácil llegar a que el vector de coeficientes estimado para el modelo de RLM puede obtenerse como</p>
<p><span class="math display">\[
\hat{\mathbf{\beta}} = (\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime \mathbf{y}
\]</span>
<!-- donde $\mathbf{X}$ es la matriz de diseño. --></p>
<p>Finalmente, el modelo estimado es <span class="math display">\[\hat{y}_i =  \hat{\beta}_0+\sum_{j=1}^k\hat{\beta}_jx_{ij},\]</span></p>
<p>que, matricialmente, puede representarse como</p>
<p><span class="math display">\[\hat{\mathbf{y}} =  \mathbf{X}\hat{\mathbf{\beta}}\]</span></p>
<p>A partir del modelo ajustado, un valor específico <span class="math inline">\(y_i\)</span> puede calculase como:</p>
<p><span class="math display">\[\hat{y}_i =  \hat{\beta}_0 + \hat{\beta}_1x_1 + \hat{\beta}_2x_2+\cdots+\hat{\beta}_kx_k\]</span></p>
<p>Como ilustración, consideremos los siguientes datos provenientes de un proceso de pintura electrostática:</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="rlm.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="do">## lectura de datos</span></span>
<span id="cb65-2"><a href="rlm.html#cb65-2" aria-hidden="true" tabindex="-1"></a>url <span class="ot">&lt;-</span> <span class="st">&#39;https://www.dropbox.com/s/st5xk1prkxg1pj4/datalab2.txt?dl=1&#39;</span></span>
<span id="cb65-3"><a href="rlm.html#cb65-3" aria-hidden="true" tabindex="-1"></a>datos <span class="ot">&lt;-</span> <span class="fu">read.table</span>(url, <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb65-4"><a href="rlm.html#cb65-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(datos)</span></code></pre></div>
<pre><code>##          y        x1 x2
## 1 91.88308 18.722091  6
## 2 94.37544 19.056131  3
## 3 74.63026  9.292093  3
## 4 93.66348 17.456714  4
## 5 76.84981 14.626183  9
## 6 76.05198 12.786439  9</code></pre>
<p>En este caso, la variable respuesta <span class="math inline">\(y\)</span> representa el espesor de la capa de pintura en micrones, <span class="math inline">\(x_1\)</span> es la presión de aire en <em>psi</em> y <span class="math inline">\(x_2\)</span> es la velocidad de la banda transportadora.</p>
<p>Una forma de comenzar a establecer si existe relación lineal entre <span class="math inline">\(y\)</span> y las variables <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> es a través de una red, como se muestra a continuación:</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="rlm.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="do">## matriz de correlación</span></span>
<span id="cb67-2"><a href="rlm.html#cb67-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(IsingSampler)</span>
<span id="cb67-3"><a href="rlm.html#cb67-3" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(qgraph)</span>
<span id="cb67-4"><a href="rlm.html#cb67-4" aria-hidden="true" tabindex="-1"></a>corMat <span class="ot">&lt;-</span> <span class="fu">cor</span>(datos)</span>
<span id="cb67-5"><a href="rlm.html#cb67-5" aria-hidden="true" tabindex="-1"></a><span class="fu">par</span>(<span class="at">mfrow =</span> <span class="fu">c</span>(<span class="dv">1</span>, <span class="dv">1</span>), <span class="at">mar =</span> <span class="fu">c</span>(<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">0.1</span>))</span>
<span id="cb67-6"><a href="rlm.html#cb67-6" aria-hidden="true" tabindex="-1"></a><span class="fu">qgraph</span>(corMat, <span class="at">graph =</span> <span class="st">&quot;cor&quot;</span>, <span class="at">layout =</span> <span class="st">&quot;spring&quot;</span>, <span class="at">sampleSize =</span> <span class="fu">nrow</span>(datos),</span>
<span id="cb67-7"><a href="rlm.html#cb67-7" aria-hidden="true" tabindex="-1"></a>    <span class="at">legend.cex =</span> <span class="dv">1</span>, <span class="at">alpha =</span> <span class="fl">0.05</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-71"></span>
<img src="book-adii_files/figure-html/unnamed-chunk-71-1.png" alt="Red de correlación para $x_1$, $x_2$ y $y$. Correlaciones positivas se muestran en verde, y las negativas en rojo." width="480" />
<p class="caption">
Figura 3.3: Red de correlación para <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> y <span class="math inline">\(y\)</span>. Correlaciones positivas se muestran en verde, y las negativas en rojo.
</p>
</div>
<p>O utilizando una matriz de dispersión:</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="rlm.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="do">## pairs plot</span></span>
<span id="cb68-2"><a href="rlm.html#cb68-2" aria-hidden="true" tabindex="-1"></a>panel.cor <span class="ot">&lt;-</span> <span class="cf">function</span>(x, y, <span class="at">digits =</span> <span class="dv">2</span>, <span class="at">prefix =</span> <span class="st">&quot;&quot;</span>, cex.cor, ...) {</span>
<span id="cb68-3"><a href="rlm.html#cb68-3" aria-hidden="true" tabindex="-1"></a>    usr <span class="ot">&lt;-</span> <span class="fu">par</span>(<span class="st">&quot;usr&quot;</span>)</span>
<span id="cb68-4"><a href="rlm.html#cb68-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">on.exit</span>(<span class="fu">par</span>(usr))</span>
<span id="cb68-5"><a href="rlm.html#cb68-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">par</span>(<span class="at">usr =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">1</span>))</span>
<span id="cb68-6"><a href="rlm.html#cb68-6" aria-hidden="true" tabindex="-1"></a>    r <span class="ot">&lt;-</span> <span class="fu">cor</span>(x, y)</span>
<span id="cb68-7"><a href="rlm.html#cb68-7" aria-hidden="true" tabindex="-1"></a>    txt <span class="ot">&lt;-</span> <span class="fu">format</span>(<span class="fu">c</span>(r, <span class="fl">0.123456789</span>), <span class="at">digits =</span> digits)[<span class="dv">1</span>]</span>
<span id="cb68-8"><a href="rlm.html#cb68-8" aria-hidden="true" tabindex="-1"></a>    txt <span class="ot">&lt;-</span> <span class="fu">paste0</span>(prefix, txt)</span>
<span id="cb68-9"><a href="rlm.html#cb68-9" aria-hidden="true" tabindex="-1"></a>    <span class="fu">text</span>(<span class="fl">0.5</span>, <span class="fl">0.5</span>, txt, <span class="at">cex =</span> <span class="fl">1.5</span>)</span>
<span id="cb68-10"><a href="rlm.html#cb68-10" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb68-11"><a href="rlm.html#cb68-11" aria-hidden="true" tabindex="-1"></a><span class="fu">pairs</span>(datos, <span class="at">lower.panel =</span> panel.smooth, <span class="at">upper.panel =</span> panel.cor, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-72"></span>
<img src="book-adii_files/figure-html/unnamed-chunk-72-1.png" alt="Matriz de dispersión para $x_1$, $x_2$ y $y$." width="528" />
<p class="caption">
Figura 3.4: Matriz de dispersión para <span class="math inline">\(x_1\)</span>, <span class="math inline">\(x_2\)</span> y <span class="math inline">\(y\)</span>.
</p>
</div>
<p>Este último gráfico indica que <span class="math inline">\(x_1\)</span> e <span class="math inline">\(y\)</span> están linealmente relacionados y la correlación es <span class="math inline">\(\hat\rho = 0.89\)</span>, mientras la correlación de <span class="math inline">\(y\)</span> y <span class="math inline">\(x_2\)</span> es <span class="math inline">\(\hat\rho = -0.32\)</span>.</p>

<div class="rmdtip">
Para otras posibilidades relacionadas con correlogramas, se recomienda revisar las alternativas en <a href="https://www.r-graph-gallery.com/correlogram.html">R Graph Gallery</a>.
</div>
<p>Para estimar el modelo de RLM, utilizamos la función <code>lm</code> como se muestra a continuación:</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb69-1"><a href="rlm.html#cb69-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ajuste del modelo de RLM</span></span>
<span id="cb69-2"><a href="rlm.html#cb69-2" aria-hidden="true" tabindex="-1"></a>fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> x1 <span class="sc">+</span> x2, <span class="at">data =</span> datos)</span>
<span id="cb69-3"><a href="rlm.html#cb69-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ x1 + x2, data = datos)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -5.0571 -1.6610 -0.1362  1.5409  8.7008 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 66.28483    1.10350  60.068  &lt; 2e-16 ***
## x1           1.48275    0.06144  24.132  &lt; 2e-16 ***
## x2          -1.05521    0.13660  -7.725 1.03e-11 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.767 on 97 degrees of freedom
## Multiple R-squared:  0.8716, Adjusted R-squared:  0.8689 
## F-statistic: 329.1 on 2 and 97 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>A partir de estos resultados podemos realizar las pruebas de significancia global y marginales tal y como se mostró en el Capítulo <a href="rls.html#rls">2</a>.</p>
</div>
</div>
<div id="propiedades-de-los-estimadores-de-mathbfbeta" class="section level2" number="3.2">
<h2><span class="header-section-number">3.2</span> Propiedades de los estimadores de <span class="math inline">\({\mathbf{\beta}}\)</span></h2>
<p>Cuando estimamos <span class="math inline">\(\hat{\mathbf{\beta}}\)</span>, los cálculos están basados en los resultados obtenidos al tomar una muestra aleatoria de tamaño <span class="math inline">\(n\)</span>. Como consecuencia, el valor de los estimadores <span class="math inline">\(\mathbf{\hat{\beta}} = (\hat{\beta}_0, \hat{\beta}_1, \ldots, \hat{\beta}_k)\)</span> cambian si cambiamos la muestra.</p>
<p>Desde el punto de vista formal, los estimadores <span class="math inline">\(\mathbf{\hat{\beta}}\)</span> cumplen con las siguientes propiedades:</p>
<ol style="list-style-type: decimal">
<li>Los estimadores <span class="math inline">\(\mathbf{\hat\beta}\)</span> son insesgados. Esta propiedad implica que, al aumentar <span class="math inline">\(n\)</span>, el valor de los estimadores de <span class="math inline">\(\mathbf{\beta}\)</span> se aproximan a los verdaderos valores de los parámetros. Matemáticamente se tiene que:</li>
</ol>
<p><span class="math display">\[\begin{eqnarray}
E[\mathbf{\hat\beta}] &amp;=&amp; E[(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime\mathbf{y}]\\\nonumber
                  &amp;=&amp; E[(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime(\mathbf{X\beta} + \mathbf{\epsilon})]\\\nonumber
                  &amp;=&amp; E[(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime\mathbf{X\beta} + (\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime\mathbf{\epsilon}]\\\nonumber
                  &amp;=&amp; E[(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime\mathbf{X\beta}] + E[(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime\mathbf{\epsilon}]\\\nonumber
                  &amp;=&amp; E[\mathbf\beta] + \mathbf{0} = \mathbf{\beta}\nonumber
\end{eqnarray}\]</span></p>
<ol start="2" style="list-style-type: decimal">
<li>La varianza de <span class="math inline">\(\hat{\beta}_j\)</span> y la covarianza entre <span class="math inline">\(\hat\beta_{i}\)</span> y <span class="math inline">\(\hat\beta_{j}\)</span> están dadas por:</li>
</ol>
<p><span class="math display">\[\begin{eqnarray*}
V(\hat\beta_{j}) &amp;=&amp; \sigma^2(\mathbf{X}^\prime\mathbf{X})^{-1}_{jj} \hspace{0.5cm} 0,1,2,\ldots,p;\\
\text{cov}(\hat\beta_{i}, \hat\beta_{j}) &amp;=&amp; \sigma^2(\mathbf{X}^\prime\mathbf{X})^{-1}_{ij} \hspace{0.5cm} i\neq j.
\end{eqnarray*}\]</span></p>
<p>Ahora, a partir de <span class="math inline">\(E[\hat{\beta}_j]\)</span> y <span class="math inline">\(V(\hat\beta_{j})\)</span>, es posible hacer <strong>inferencia</strong> para el parámetro <span class="math inline">\(\beta_j\)</span>, <span class="math inline">\(j=1,2,\ldots,k.\)</span> Sin embargo, observe que <span class="math inline">\(V(\hat\beta_{j})\)</span> depende de <span class="math inline">\(\sigma^2\)</span>, la varianza del modelo de RLM, que se estima a través del MSE.</p>
<p>Los coeficientes del modelo de RLM ajustado para los datos provenientes del proceso de pintura electrostática se obtienen haciendo:</p>
<div class="sourceCode" id="cb71"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb71-1"><a href="rlm.html#cb71-1" aria-hidden="true" tabindex="-1"></a><span class="do">## coeficientes estimados</span></span>
<span id="cb71-2"><a href="rlm.html#cb71-2" aria-hidden="true" tabindex="-1"></a><span class="fu">coefficients</span>(fit)</span></code></pre></div>
<pre><code>## (Intercept)          x1          x2 
##   66.284831    1.482753   -1.055206</code></pre>
<p>A partir de estos coeficientes, el modelo ajustado será</p>
<p><span class="math display">\[\hat{y} = 66.285 + 1.483x_1 - 1.055x_2\]</span>
Note que esta es la ecuación de un plano en el espacio <span class="math inline">\((x_1, x_2, y)\)</span>.</p>

<div class="rmdnote">
Los coeficientes <em>estimados</em> del modelo de RLM se interpretan en términos de una <em>derivada</em> parcial.
</div>
<p>En particular, si incrementamos <span class="math inline">\(x_1\)</span> en una unidad y mantenemos <em>constante</em> <span class="math inline">\(x_2\)</span>, esperaríamos que el espesor de la capa de pintura aumentara, en promedio, <span class="math inline">\(\hat{\beta}_1 = 1.483\)</span> micrones. Por otro lado, si incrementamos <span class="math inline">\(x_2\)</span> en una unidad y mantenemos <em>constante</em> <span class="math inline">\(x_1\)</span>, se espera que el espesor de la capa de pintura disminuya, en promedio, <span class="math inline">\(\hat{\beta}_2 = -1.055\)</span> micrones.</p>
</div>
<div id="estimación-de-sigma2-1" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Estimación de <span class="math inline">\(\sigma^2\)</span></h2>
<p>Similar a como se observó en RLS, en RLM, también se cumple la misma relación que en RLS en cuanto que</p>
<p><span class="math display">\[SST = SSR + SSE \]</span></p>
<p>Adicionalmente, <span class="math inline">\(\mathbf{\hat{\beta}}\)</span>, se tiene que</p>
<p><span class="math display">\[\begin{eqnarray*}\label{L2}
L &amp;=&amp;\sum_{i=1}^n\hat{\epsilon}_i^2 = \sum_{i=1}^n(Y_i-\hat{Y}_i^2)^2  \\
  &amp;=&amp;\sum_{i=1}^n(Y_i-\hat{\beta}_0-\hat{\beta}_1X_{1,i} -\hat{\beta}_2X_{2,i} - \cdots -\hat{\beta}_kX_{k,i})^2 \\
  &amp;=&amp; SSE
\end{eqnarray*}\]</span></p>
<p>Por lo tanto,</p>
<p><span class="math display">\[\hat{\sigma}^2 = \frac{SSE}{n-p} = MSE\]</span></p>
<p>donde <span class="math inline">\(p = k+1\)</span> es el número de coeficientes del modelo ajustado.</p>
<p>Este resulado indica que la varianza de los errores, también conocida como la <strong>varianza del modelo</strong>, puede estimarse utilizando el MSE. El MSE se obtiene de la tabla ANOVA que tiene la siguiente forma:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:tablaanova"></span>
<img src="images/tablaanova.png" alt="Tabla ANOVA en RLM." width="90%" />
<p class="caption">
Figura 3.5: Tabla ANOVA en RLM.
</p>
</div>
<p>En el caso del modelo de RLM para el proceso de pintura electrostática, podemos obtener el valor de <span class="math inline">\(\hat{\sigma}^2\)</span> haciendo:</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="rlm.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="do">## estimación de sigma^2</span></span>
<span id="cb73-2"><a href="rlm.html#cb73-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>sigma<span class="sc">^</span><span class="dv">2</span></span></code></pre></div>
<pre><code>## [1] 7.65454</code></pre>
<p>Por lo tanto, el modelo estimado será:</p>
<p><span class="math display">\[
\begin{eqnarray} 
\hat{y} &amp;=&amp; 66.285 + 1.483x_1 - 1.055x_2 \\\nonumber
\epsilon &amp;\sim&amp; N(0, \sigma^2) \\\nonumber
\hat{\sigma}^2 &amp;=&amp; 7.655.\nonumber
\end{eqnarray}
\]</span></p>
<p><!-- donde    --></p>
<p><!-- \begin{eqnarray} -->
<!-- SST = \sum_{i=1}^ny_i^2 - \frac{1}{n}\left(\sum_{i=1}^ny_i \right)^2, \hspace{1cm} SSE = \sum_{i=1}^n\hat{\epsilon}_i^2 \\\nonumber -->
<!-- \end{eqnarray} --></p>
<p>Recordemos que, adicional al MSE, a partir de la tabla ANOVA es posible calcular el porcentaje de variabilidad de la respuesta explicado por el modelo de RLM, también conocido como <strong>coeficiente de determinación</strong> o, simplemente, como <span class="math inline">\(R^2\)</span>:</p>
<p><span class="math display">\[R^2 = SSR/SST = 1 - SSE/SST\]</span></p>
<p>Puesto que <span class="math inline">\(R^2\)</span> incrementa a medida que el número de variables aumenta, se recomienda utilizar el <span class="math inline">\(R^2\)</span> ajustado en RLM:</p>
<p><span class="math display">\[R^2_{\text{ajustado}} = 1 - \frac{SSE/(n-p)}{SST/(n-1)}\]</span></p>
<p>La interpretación de esta medida de desempeño es similar a la interpretación de <span class="math inline">\(R^2\)</span> discutida en el Capítulo <a href="rls.html#rls">2</a>.</p>
<p>El ejemplo de la pintura electrostática, <span class="math inline">\(R^2_{\text{adj}}=0.8689\)</span>, por lo que podemos afirmar que incluir las variables <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> en un modelo de regresión permite explica cerca del 87% de la variabilidad de la respuesta.</p>
</div>
<div id="inferencia-para-mathbfbeta" class="section level2" number="3.4">
<h2><span class="header-section-number">3.4</span> Inferencia para <span class="math inline">\(\mathbf{\beta}\)</span></h2>
<p>Uno de los propósitos de la <strong>inferencia estadística</strong> es determinar el valor de los verdaderos parámetros de una población a partir de los resultados obtenidos en una muestra. En este caso, los parámetros poblacionales son <span class="math inline">\(\mathbf{{\beta}} = (\beta_0, \beta_1, \ldots, \beta_k)\)</span>, además de <span class="math inline">\(\sigma^2\)</span>.</p>
<p>Con los valores muestrales, podemos construir pruebas de hipótesis de dos tipos para los parámetros del modelo de RLM: la prueba de significancia total y las pruebas de significancia marginal.</p>
<div id="prueba-de-significancia-global" class="section level3" number="3.4.1">
<h3><span class="header-section-number">3.4.1</span> Prueba de significancia global</h3>
<p>Esta prueba se utiliza para determinar la <strong>significancia total</strong> del modelo, es decir, para detemrinar si incluir las variables controlables en el modelo de regresión es mejor que no incluirlas para explicar la respuesta <span class="math inline">\(Y\)</span>. La idea fundamental es determinar si, en la población,</p>
<p><span class="math display">\[\begin{eqnarray}
  H_0&amp;:&amp; \beta_1=\beta_2=\cdots\beta_k=0 \\\nonumber
  H_1&amp;:&amp; \text{Al menos un $\beta_j \neq 0$}\nonumber
  \end{eqnarray}\]</span></p>
<p>Este procedimiento de prueba de hipótesis se realiza a
través de la tabla de ANOVA utilizando el estadístico <span class="math inline">\(F\)</span> dado por</p>
<p><span class="math display">\[F_0 =  \frac{SSR/k}{SSE/(n-p)} = \frac{MSR}{MSE} \sim F_{k, n-p}\]</span></p>
<p>Rechazamos <span class="math inline">\(H_0: \beta_1=\beta_2=\cdots\beta_k=0\)</span> si <span class="math inline">\(F_0 &gt; F_{\alpha,k,n-p}\)</span>, donde <span class="math inline">\(\alpha\in(0,1)\)</span> es un nivel de significancia predeterminado. Cuando esto ocurre, concluimos que al menos un <span class="math inline">\(\beta_j\)</span> es estad'isticamente significativo al <span class="math inline">\(100(1-\alpha)\%\)</span>.</p>
<p>Observe que en la prueba de significancia global del modelo, <span class="math inline">\(F_{\text{calc}} = 329.1\)</span> y <span class="math inline">\(p &lt; 2.2\times 10^{-16}\)</span>. Esto indica que tener este modelo de RLM para explicar el espesor de la capa de pintura es mejor que no tenerlo.</p>
</div>
<div id="prueba-de-significancia-marginal" class="section level3" number="3.4.2">
<h3><span class="header-section-number">3.4.2</span> Prueba de significancia marginal</h3>
<p>Esta prueba se realiza si rechazamos la prueba de significancia global. Lo que intentamos hacer es determinar si, a nivel poblacional, los coeficientes asociados a cada <span class="math inline">\(x_j\)</span> son o diferentes de cero. Esto es equivalente a probar:</p>
<p><span class="math display">\[\begin{eqnarray*}
  H_0&amp;:&amp; \beta_j=0 \\\nonumber
  H_1&amp;:&amp; \beta_j \neq 0\nonumber
  \end{eqnarray*}\]</span></p>
<p>Para <span class="math inline">\(j\)</span> fijo, el estadístico de prueba es</p>
<p><span class="math display">\[t_j = \frac{\hat{\beta}_j - 0}{\text{s.e.}(\hat\beta_j)} = \frac{\hat{\beta}_j}{\sqrt{\hat{\sigma}^2(\mathbf{X}^\prime\mathbf{X})^{-1}_{jj}}}\sim t_{n-p}\]</span></p>
<p>Por lo tanto, rechazamos <span class="math inline">\(H_0\)</span> con un nivel de significancia de <span class="math inline">\(100\times(1-\alpha)\%\)</span> si <span class="math inline">\(|t_j| &gt; t_{\alpha/2, n-p}\)</span>.</p>
<p>Para el problema de la pintura electrostática, tendríamos:</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb75-1"><a href="rlm.html#cb75-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ajuste del modelo de RLM</span></span>
<span id="cb75-2"><a href="rlm.html#cb75-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit)<span class="sc">$</span>coefficients</span></code></pre></div>
<pre><code>##              Estimate Std. Error   t value     Pr(&gt;|t|)
## (Intercept) 66.284831 1.10349995 60.067815 1.529216e-78
## x1           1.482753 0.06144364 24.131927 8.750958e-43
## x2          -1.055206 0.13659758 -7.724922 1.026117e-11</code></pre>
<p>Según estos resultados, todas las pruebas marginales del tipo <span class="math inline">\(H_0: \beta_j = 0\)</span> vs. <span class="math inline">\(H_1: \beta_j \neq 0\)</span>, el valor <span class="math inline">\(p\)</span> es <span class="math inline">\(&lt; 0.05\)</span>. Por lo tanto, la magnitud de la influencia de cada variable controlable sobre <span class="math inline">\(E[y|x_1, x_2]\)</span> es estadísticamente diferente de cero a nivel poblacional. En otras palabras, controlando las variables <span class="math inline">\(x_1\)</span> y <span class="math inline">\(x_2\)</span> en el proceso, permitiría modificar satisfactoriamente el espesor de la capa de pintura.</p>
</div>
<div id="intervalos-de-confianza-para-beta_j" class="section level3" number="3.4.3">
<h3><span class="header-section-number">3.4.3</span> Intervalos de confianza para <span class="math inline">\(\beta_j\)</span></h3>
<p>Otra forma de realizar inferencia para <span class="math inline">\(\mathbf{\beta}\)</span> es a través de la construcción de intervalos de confianza del <span class="math inline">\(100\times(1-\alpha)100\%\)</span>. Es fácil mostrar que, para <span class="math inline">\(j\)</span> fijo,</p>
<p><span class="math display">\[\begin{equation}\label{eq:icbeta}
  \beta_j\in \left( \hat{\beta}_j - t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2(\mathbf{X}^\prime\mathbf{X})^{-1}_{jj}}, \hat{\beta}_j + t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2(\mathbf{X}^\prime\mathbf{X})^{-1}_{jj}} \right)
  \end{equation}\]</span></p>
<p>Otra alternativa para construir intervalos de confianza es vía <em>bootstrap</em> o <em>likelihood profiling</em>. Finalmente concluimos, con un nivel de confianza del <span class="math inline">\(100\times(1-\alpha)\%\)</span>, que <span class="math inline">\(\beta_j\)</span> está en el intervalo anterior.</p>
<p>Uan forma de construir los intervalos de confianza para los coeficientes del modelo es utilizando la función <code>confint.default</code>:</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb77-1"><a href="rlm.html#cb77-1" aria-hidden="true" tabindex="-1"></a><span class="do">## intervalos de confianza del 95% para los coeficientes</span></span>
<span id="cb77-2"><a href="rlm.html#cb77-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confint.default</span>(fit)</span></code></pre></div>
<pre><code>##                 2.5 %     97.5 %
## (Intercept) 64.122011 68.4476513
## x1           1.362326  1.6031807
## x2          -1.322932 -0.7874793</code></pre>
<p>Por lo tanto, con una confianza del 95%,</p>
<p><span class="math display">\[\begin{align}
\beta_0 &amp;\in(64.12, 68.45) \\
\beta_1 &amp;\in(1.36, 1.60) \\
\beta_2 &amp;\in(-1.323, -0.787).
\end{align}\]</span></p>
</div>
</div>
<div id="inferencia-para-la-respuesta" class="section level2" number="3.5">
<h2><span class="header-section-number">3.5</span> Inferencia para la respuesta</h2>

<div class="rmdwarning">
El modelo de RLM ajustado puede utilizarse para predecir <span class="math inline">\(E[\mathbf{Y}|\mathbf{x}_0]\)</span> sólo si se validan <strong>todos los supuestos</strong>. Para más detalles ver <a href="rls.html#residuales">Análisis de Residuales</a>.
</div>
<div id="intervalos-de-confianza-para-emathbfymathbfx_0" class="section level3" number="3.5.1">
<h3><span class="header-section-number">3.5.1</span> Intervalos de confianza para <span class="math inline">\(E[\mathbf{Y}|\mathbf{x}_0]\)</span></h3>
<p>A partir del modelo ajustado y para valores fijos de las variables controlables, digamos <span class="math inline">\(\mathbf{x}_0\)</span>, se tiene que
<span class="math display">\[\begin{eqnarray}
 \hat{\mu}_{\mathbf{Y} | \mathbf{x}_0} &amp;=&amp; \hat{E[\mathbf{Y} | \mathbf{x}_0]} = \mathbf{x}_0^\prime\hat{\mathbf{\beta}} \\\nonumber
 V[\hat{\mu}_{\mathbf{Y} | \mathbf{x}_0}] &amp;=&amp; \hat{\sigma}^2\mathbf{x}_0^\prime(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{x}_0
 \end{eqnarray}\]</span></p>
<p>Finalmente, el intervalo de confianza del <span class="math inline">\(100\times(1-\alpha)\%\)</span> puede calcularse como</p>
<p><span class="math display">\[\begin{eqnarray}
 \hat{\mu}_{\mathbf{Y} | \mathbf{x}_0} \pm t_{\alpha/2,n-p}\sqrt{\hat{\sigma}^2\mathbf{x}_0^\prime(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{x}_0}
 \end{eqnarray}\]</span></p>
<p>Supongamos que queremos determinar el espesor de la capa de pintura para las condiciones <span class="math inline">\(\mathbf{x}_0 = (10, 8)\)</span>, es decir, cuando <span class="math inline">\(x_1 = 10\)</span> y <span class="math inline">\(x_2 = 8\)</span>.</p>
<p>En <code>R</code> procedemos de la siguiente manera:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb79-1"><a href="rlm.html#cb79-1" aria-hidden="true" tabindex="-1"></a><span class="do">## cálculo de E[y|x_1 = 10, x_2 = 8]</span></span>
<span id="cb79-2"><a href="rlm.html#cb79-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="dv">10</span>, <span class="at">x2 =</span> <span class="dv">8</span>))</span></code></pre></div>
<pre><code>##        1 
## 72.67072</code></pre>
<p>Si trabajamos bajo las condiciones <span class="math inline">\(\mathbf{x}_0\)</span>, se espera que, en promedio, el espesor de la capa de pintura sea 72.67 micrones.</p>

<div class="rmdtip">
Para otros argumentos y opciones, se sugiere al lector escribir
<code>?predict.lm</code> en la consola de <code>R</code>.
</div>
<p>Ahora, si es de interés calcular un intervalo de confianza del 95%, agregamos el argumento <code>interval = 'confidence'</code> a la instrucción anterior:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb81-1"><a href="rlm.html#cb81-1" aria-hidden="true" tabindex="-1"></a><span class="do">## cálculo de E[y|x_1 = 10, x_2 = 8]</span></span>
<span id="cb81-2"><a href="rlm.html#cb81-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="dv">10</span>, <span class="at">x2 =</span> <span class="dv">8</span>), <span class="at">interval =</span> <span class="st">&#39;confidence&#39;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 72.67072 71.64637 73.69507</code></pre>
<p>Por lo tanto, se espera que, si continuamos trabajando bajo las condiciones <span class="math inline">\(x_1 = 10\)</span> y <span class="math inline">\(x_2 = 8\)</span>, el espesor de capa <em>promedio</em> sea 72.67 micrones. A nivel <em>poblacional</em>, dicho promedio se encontrará en el intervalo <span class="math inline">\((71.646, 73.695)\)</span> con una confianza del 95%.</p>
</div>
<div id="intervalos-de-predicción-para-emathbfymathbfx_0" class="section level3" number="3.5.2">
<h3><span class="header-section-number">3.5.2</span> Intervalos de predicción para <span class="math inline">\(E[\mathbf{Y}|\mathbf{x}_0]\)</span></h3>
<p>Sea <span class="math inline">\(\hat{y}_0 = \hat{\mu}_{\mathbf{Y} | \mathbf{x}_0}\)</span>, donde <span class="math inline">\(\mathbf{x}_0\)</span> es el vector de covariables <em>futuro</em>. Un intervalo de predicción del <span class="math inline">\(100\times(1-\alpha)\%\)</span> para <span class="math inline">\(Y_0\)</span> está dado por:</p>
<p><span class="math display">\[\begin{eqnarray}
 \hat{y}_0 \pm t_{\alpha/2,n-p}\sqrt{\hat{\sigma}^2(1 + \mathbf{x}_0^\prime(\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{x}_0)}
 \end{eqnarray}\]</span></p>
<p>Otra posibilidad para construir dicho intervalo es vía <a href="https://es.wikipedia.org/wiki/Bootstrapping_(estad%C3%ADstica)"><em>bootstrap</em></a>. Observe que lo único que cambia en este intervalo en relación con el intervalo de confianza es la varianza de <span class="math inline">\(Y_0\)</span> <span class="math inline">\(-\)</span> existe más incertidumbre. Observe que en el intervalo de predicción estamos interesados en <span class="math inline">\(Y_0 | \mathbf{x}_0\)</span> y no <span class="math inline">\(\hat{\mu}_{\mathbf{Y} | \mathbf{x}_0}\)</span>.</p>
<p>Para calcular un intervalo de predicción del 95%, agregamos el argumento <code>interval = 'prediction'</code> a la instrucción anterior:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb83-1"><a href="rlm.html#cb83-1" aria-hidden="true" tabindex="-1"></a><span class="do">## intervalo de predicción para E[y|x_1 = 10, x_2 = 8]</span></span>
<span id="cb83-2"><a href="rlm.html#cb83-2" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(fit, <span class="at">newdata =</span> <span class="fu">data.frame</span>(<span class="at">x1 =</span> <span class="dv">10</span>, <span class="at">x2 =</span> <span class="dv">8</span>), <span class="at">interval =</span> <span class="st">&#39;prediction&#39;</span>)</span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 72.67072 67.08489 78.25655</code></pre>
<p>Este resultado indica que, bajo las condiciones <span class="math inline">\(x_1 = 10\)</span> y <span class="math inline">\(x_2 = 8\)</span>, el valor del espesor de la capa de pintura para la <em>próxima</em> unidad experimental será 72.67 micrones. A nivel <em>poblacional</em>, dicho valor se encontrará en el intervalo <span class="math inline">\((67.085, 78.257)\)</span> con una confianza del 95%.</p>
</div>
</div>
<div id="análisis-de-residuales" class="section level2" number="3.6">
<h2><span class="header-section-number">3.6</span> Análisis de Residuales</h2>
<p>El análisis de residuales en RLM es fundamental para:</p>
<ol style="list-style-type: decimal">
<li>Validar los supuestos del error;</li>
<li>identicar observaciones <em>outlier</em>; e</li>
<li>identificar observaciones <em>influenciales</em>.</li>
</ol>
<div id="validación-de-supuestos-1" class="section level3" number="3.6.1">
<h3><span class="header-section-number">3.6.1</span> Validación de supuestos</h3>
<p>La validación de los supestos del error en el modelo de RLM se realiza de manera similar a como se mostró para el modelo de RLS. Para más detalles, ver <a href="rls.html#residuales">Análisis de Residuales</a>.</p>
</div>
<div id="identificación-de-outliers" class="section level3" number="3.6.2">
<h3><span class="header-section-number">3.6.2</span> Identificación de <em>outliers</em></h3>
<p>Los <em>outliers</em> son también conocidos como observaciones <em>atípicas</em> en los datos. A partir del modelo ajustado, podemos calcular:</p>
<ol style="list-style-type: decimal">
<li><p>Residuales <em>crudos</em>
<span class="math display">\[\hat{\epsilon}_i = y_i - \hat{y}_i\]</span></p></li>
<li><p>Residuales <em>estandarizados</em>
<span class="math display">\[d_i = \frac{\hat{\epsilon}_i}{\sqrt{\hat{\sigma}^2}} = \frac{\hat{\epsilon}_i}{\sqrt{\text{MSE}}} \]</span></p></li>
<li><p>Residuales <em>estudentizados</em>
<span class="math display">\[r_i = \frac{\hat{\epsilon}_i}{\sqrt{\hat{\sigma}^2(1-h_{ii})}} = \frac{\hat{\epsilon}_i}{\sqrt{\text{MSE}(1-h_{ii})}}\]</span></p>
<p>donde <span class="math inline">\(h_{ii} = \mathbf{X}(\mathbf{X^\prime\mathbf{X}})^{-1}\mathbf{X}^\prime_{ii}\)</span> y <span class="math inline">\(\mathbf{X}(\mathbf{X^\prime\mathbf{X}})^{-1}\mathbf{X}\)</span> denominada la <a href="https://en.wikipedia.org/wiki/Projection_matrix">matriz <em>hat</em></a>.</p></li>
</ol>

<div class="rmdtip">
La mejor manera de identificar <em>outliers</em> es a partir del cálculo de los residuales estudentizados. Decimos que la <span class="math inline">\(i\)</span>-ésima observación <strong>es</strong> un <em>outlier</em> si <span class="math inline">\(r_i\notin (-3, 3)\)</span>.
</div>
<p>En <code>R</code> las funciones clave son:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb85-1"><a href="rlm.html#cb85-1" aria-hidden="true" tabindex="-1"></a><span class="do">## residuales crudos</span></span>
<span id="cb85-2"><a href="rlm.html#cb85-2" aria-hidden="true" tabindex="-1"></a>r_crudo <span class="ot">&lt;-</span> <span class="fu">residuals</span>(fit)</span>
<span id="cb85-3"><a href="rlm.html#cb85-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-4"><a href="rlm.html#cb85-4" aria-hidden="true" tabindex="-1"></a><span class="do">## residuales estudentizados</span></span>
<span id="cb85-5"><a href="rlm.html#cb85-5" aria-hidden="true" tabindex="-1"></a>r_est <span class="ot">&lt;-</span> <span class="fu">rstudent</span>(fit)</span>
<span id="cb85-6"><a href="rlm.html#cb85-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb85-7"><a href="rlm.html#cb85-7" aria-hidden="true" tabindex="-1"></a><span class="do">## residuales estandarizados</span></span>
<span id="cb85-8"><a href="rlm.html#cb85-8" aria-hidden="true" tabindex="-1"></a>r_normal <span class="ot">&lt;-</span> <span class="fu">rstandard</span>(fit)</span></code></pre></div>
<p>donde <code>fit</code> es el objeto <code>R</code> que contiene en modelo de RLM estimado. Para más detalles, se sugiere consultar la ayuda de cada función: <code>?residuals</code>, <code>?rstudent</code> y <code>?rstandard</code>.</p>
<p>En nuestro caso,</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb86-1"><a href="rlm.html#cb86-1" aria-hidden="true" tabindex="-1"></a><span class="do">## residuales estudentizados</span></span>
<span id="cb86-2"><a href="rlm.html#cb86-2" aria-hidden="true" tabindex="-1"></a>r <span class="ot">&lt;-</span> <span class="fu">rstudent</span>(fit)</span>
<span id="cb86-3"><a href="rlm.html#cb86-3" aria-hidden="true" tabindex="-1"></a><span class="fu">which</span>(r <span class="sc">&lt;</span> <span class="sc">-</span><span class="dv">3</span> <span class="sc">|</span> r <span class="sc">&gt;</span> <span class="dv">3</span>)</span></code></pre></div>
<pre><code>## 18 
## 18</code></pre>
<p>la observación <code>18</code> podría considerarse un <em>outlier</em>. En la práctica, el siguiente paso es evaluar la trazabilidad de esa observación y determinar si existen o no causas asignables para que esta sea un <em>outlier</em>. En caso de que exista una causa asignable, dicha observación <em>debería</em> removerse de la base de datos y, con los datos reducidos, estimar nuevamente el modelo de RLM.</p>
<p>Otra forma de detectar <em>outliers</em> es a través de la <strong>prueba de Bonferroni</strong>. Esta prueba está implementada en la función <code>outlierTest</code> del paquete <code>car</code>. En nuestro ejemplo, procedemos de la siguiente manera:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb88-1"><a href="rlm.html#cb88-1" aria-hidden="true" tabindex="-1"></a><span class="do">## prueba Bonferroni para outliers</span></span>
<span id="cb88-2"><a href="rlm.html#cb88-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(car)</span>
<span id="cb88-3"><a href="rlm.html#cb88-3" aria-hidden="true" tabindex="-1"></a><span class="fu">outlierTest</span>(fit, <span class="at">n.max =</span> <span class="dv">5</span>)</span></code></pre></div>
<pre><code>## No Studentized residuals with Bonferroni p &lt; 0.05
## Largest |rstudent|:
##    rstudent unadjusted p-value Bonferroni p
## 18 3.360964           0.001116       0.1116</code></pre>
<p>En la parte superior vemos que la observación <code>18</code> tiene el mayor valor del residual estudentizado. Sin embargo, el valor <span class="math inline">\(p\)</span> es <em>superior</em> a 0.05, por lo que no tenemos evidencia suficiente para concluir que dicha observación representa un <em>outlier</em>.</p>
<p>Gráficamente es posible identificar gráficamente cuáles son las observaciones influenciales utilizando la función <code>influenceIndexPlot</code> del paquete <code>car</code>:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb90-1"><a href="rlm.html#cb90-1" aria-hidden="true" tabindex="-1"></a><span class="do">## gráfico de variables influenciales</span></span>
<span id="cb90-2"><a href="rlm.html#cb90-2" aria-hidden="true" tabindex="-1"></a><span class="fu">influenceIndexPlot</span>(fit, <span class="at">vars =</span> <span class="st">&quot;Bonf&quot;</span>, <span class="at">las =</span> <span class="dv">1</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-88"></span>
<img src="book-adii_files/figure-html/unnamed-chunk-88-1.png" alt="Gráfico de observaciones influenciales. El eje $y$ corresponde al valor $p$ de la prueba de Bonferroni." width="528" />
<p class="caption">
Figura 3.6: Gráfico de observaciones influenciales. El eje <span class="math inline">\(y\)</span> corresponde al valor <span class="math inline">\(p\)</span> de la prueba de Bonferroni.
</p>
</div>
<p>Para más detalles, se recomienda escribir <code>?viainfluenceIndexPlot</code> en la consola del <code>R</code>.</p>
</div>
<div id="identificación-de-observaciones-influenciales" class="section level3" number="3.6.3">
<h3><span class="header-section-number">3.6.3</span> Identificación de observaciones influenciales</h3>
<p>En ciertas ocasiones encontramos observaciones que lucen algo <em>anormales</em> y es importante determinar si estas son <em>influenciales</em> o no.</p>
<p>A diferencia de los <em>outliers</em>, las observaciones <em>influenciales</em> <strong>controlan</strong> el modelo y por ello es importante determinar si el modelo ajustado es consistente cuando estas se remueven.</p>
<p>La identificación de este tipo de observaciones se realiza utilizando, principalmente, la <a href="https://www.jstor.org/stable/1268249?seq=1#metadata_info_tab_contents"><em>Distancia de Cook</em></a>.</p>
<p>Para la <span class="math inline">\(i\)</span>-ésima observación, esta
distancia se calcula como</p>
<p><span class="math display">\[D_i = \frac{r_i^2}{p}\frac{h_{ii}}{(1-h_{ii})} = \frac{\hat{\epsilon}_i^2 \,h_{ii}}{p\,\hat{\sigma}^2(1-h_{ii})^2}\]</span></p>
<p>donde <span class="math inline">\(p\)</span> es el número de variables controlables incluídas en el modelo.</p>

<div class="rmdtip">
Usualmente, cuando <span class="math inline">\(D_i&gt;\frac{4}{n-p-2}\)</span> decimos que la <span class="math inline">\(i\)</span>-ésima observacion <strong>es influencial</strong>. Este criterio es el utilizado por la función <code>cooks.distance()</code> del <code>R</code>.
</div>
</div>
</div>
<div id="análisis-de-multicolinealidad" class="section level2" number="3.7">
<h2><span class="header-section-number">3.7</span> Análisis de Multicolinealidad</h2>
<p>Cuando se utiliza el modelo de RLM, se asume que las variables controlables <span class="math inline">\(X_1, X_2,\ldots,X_k\)</span> son independientes. Desde el punto de vista práctico, esto tiene consideraciones importantes puesto que permite evaluar la magnitud del efecto de sobre <span class="math inline">\(\widehat{E[Y]}\)</span> cuando modificamos, en una unidad, digamos <span class="math inline">\(x_j\)</span>, mientras se mantienen el resto de ellas constantes. Este efecto corresponde, sin duda, a <span class="math inline">\(\hat{\beta}_j\)</span>. Sin embargo, cuando estas variables controlables <strong>no</strong> son independientes, este efecto <em>no</em> puede calcularse de la misma forma.</p>

<div class="rmdtip">
Buscamos modelos en los que las covariables estén altamente correlacionadas con la respuesta, pero mínimamente entre ellas.
</div>
<p>Desde el punto de vista teórico, la existencia de <em>no</em> independencia en las variables controlables tiene consecuencias importantes sobre los estimadores de los parámetros del modelo dados por <span class="math inline">\(\mathbf{\beta} = (\beta_0, \beta_1, \beta_2, \ldots, \beta_k)\)</span>. Cuando se usa el <a href="rlm.html#estimacion">método de mínimos cuadrados</a>, los estimadores de <span class="math inline">\(\mathbf{\beta}\)</span> están dados por:</p>
<p><span class="math display">\[
\hat{\mathbf{\beta}}_{\text{OLS}} = (\mathbf{X}^\prime\mathbf{X})^{-1}\mathbf{X}^\prime \mathbf{y}
\]</span></p>
<p>La existencia de multicolinealidad es sinónimo de que no existe independencia en las variables controlables del modelo. Si esto es cierto, las columnas de la matriz de diseño <span class="math inline">\(\mathbf{X}\)</span> no son independientes, es decir, que la columna <span class="math inline">\(x_j\)</span> puede expresarse como una combinación lineal de las demás. Matemáticamente, esto es equivalente a escribir <span class="math inline">\(x_j \sim x_{-j}\)</span> para algún <span class="math inline">\(j\)</span>. Por ejemplo, para <span class="math inline">\(j=1\)</span> tendríamos</p>
<p><span class="math display">\[
x_1 \sim x_2 + x_3 + \cdots + x_k.
\]</span></p>
<p>Esta expresión indica que la variable independiente/controlable <span class="math inline">\(x_1\)</span> puede escribirse como una combinación lineal de las demás variables controlables. O, en otras palabras, que la información contenida en <span class="math inline">\(x_1\)</span> puede explicarse por las demás variables controlables medidas en el proceso durante la etapa de muestreo.</p>

<div class="rmdtip">
Una manera de interpretar la multicolinealidad es como sinónimo de <strong>redundancia</strong>. Esta redundancia se refiere a que existen variables controlables en el proceso de producción que contienen la misma <em>información</em> que las demás. Por lo tanto, basta con medir sólo aquellas que realmente determinan dicho proceso.
</div>
<p>En la expresión de <span class="math inline">\(\hat{\mathbf{\beta}}_{\text{OLS}}\)</span>, el término <span class="math inline">\((\mathbf{X}^\prime\mathbf{X})^{-1}\)</span> se refiere a la inversa de <span class="math inline">\(\mathbf{X}^\prime\mathbf{X}\)</span>. Si existe multicolinealidad,</p>
<p><span class="math display">\[
\text{det}(\mathbf{X}^\prime \mathbf{X}) \approx 0 \quad \Rightarrow \quad  \frac{1}{|\mathbf{X}^\prime \mathbf{X}|} \rightarrow\infty
\]</span></p>
<p>Por lo tanto,</p>
<p><span class="math display">\[
\hat{\mathbf{\beta}}_{\text{OLS}} \rightarrow \infty.
\]</span></p>
<div id="cómo-detectarla" class="section level3" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Cómo detectarla?</h3>
<p>Existen varios indicadores para <em>sospechar</em> que existe multicolinealidad:</p>
<ol style="list-style-type: decimal">
<li>Una alta correlación en las variables independientes. Esto es posible determinarlo gráficamente a través de una matriz de dispersión (ver por ejemplo <code>?pairs</code> en la consola del <code>R</code>) o utilizando una <a href="https://revistas.usantotomas.edu.co/index.php/estadistica/article/view/1097/1332">prueba de independencia completa</a>.</li>
<li>Que se rechace la prueba de significancia global pero no todas las pruebas de significancia marginal.</li>
<li>Que ocurran ambios considerables en <span class="math inline">\(\hat{\mathbf{\beta}}\)</span> cuando se agrega o elimina una variable predictora.</li>
</ol>
<p>Para <em>probar que efectivamente</em> existe, podemos usar tres aproximaciones:</p>
<ol style="list-style-type: decimal">
<li>El número de condición de la matriz <span class="math inline">\(\mathbf{X}^\prime \mathbf{X}\)</span>. También conocido como <em>I-ll condicion number</em> o ICN, este número mide qué tan “enferma” se encuentra la matriz que debe ser invertida para poder calcular <span class="math inline">\(\hat{\mathbf{\beta}}_{\text{OLS}}\)</span>. El ICN se calcula como</li>
</ol>
<p><span class="math display">\[\text{ICN}(\mathbf{X}^\prime\mathbf{X}) = \sqrt{\frac{\lambda_\text{máx}}{\lambda_\text{min}}}
\]</span></p>
<p>con <span class="math inline">\(\lambda_\text{máx}\)</span> y <span class="math inline">\(\lambda_\text{min}\)</span> los valores propios máximos y mínimos de <span class="math inline">\(\mathbf{X}^\prime \mathbf{X}\)</span>, obtenidos a partir de la descomposición espectral de dicha matriz.</p>

<div class="rmdtip">
Decimos que existe multicolinealidad cuando <span class="math inline">\(\text{ICN}(\mathbf{X}^\prime \mathbf{X}) &gt; 30\)</span>. El inconveniente con el ICN es que no nos da información acerca de cuál de las variables independentes es la más multicolineal (o redundante) en el sistema.
</div>
<p>En <code>R</code>, la función clave para calcular el ICN es <code>kappa</code>. Para más detalles, se recomienda escribir <code>?kappa</code> en la consola. En el caso del ejemplo del espesor de pintura, tendríamos los siguientes resultados:</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb91-1"><a href="rlm.html#cb91-1" aria-hidden="true" tabindex="-1"></a><span class="do">## ICN para el modelo ajustado</span></span>
<span id="cb91-2"><a href="rlm.html#cb91-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kappa</span>(fit)</span></code></pre></div>
<pre><code>## [1] 48.62071</code></pre>
<p>Puesto que <span class="math inline">\(\text{ICN}(\mathbf{X}^\prime \mathbf{X}) &gt; 30\)</span>, podemos concluir que existe evidencia para sospechar que, efectivamente, <strong>existe</strong> multicolinealidad.</p>
<ol start="2" style="list-style-type: decimal">
<li>El factor de inflación de varianza (VIF). A través de este indicador podemos detectar <strong>cuál</strong> de las variables independientes es la más colineal de las <span class="math inline">\(k\)</span> medidas. Para la <span class="math inline">\(j\)</span>-ésima variable independiente,</li>
</ol>
<p><span class="math display">\[\text{VIF}_j = \frac{1}{1-R_j^2}\]</span></p>
<p>donde <span class="math inline">\(R_j^2\)</span> es el <span class="math inline">\(R^2_\text{adjusted}\)</span> del modelo <span class="math inline">\(x_j\sim x_{-j}\)</span>.</p>

<div class="rmdtip">
Decimos que la variable <span class="math inline">\(x_j\)</span> es <em>responsable</em> por la multicolineal en el sistema si <span class="math inline">\(\text{VIF}_j &gt; 5\)</span>.
</div>
<p>En <code>R</code>, el VIF puede calcularse a través de la función <code>vif</code> del paquete <code>car</code>. Para más detalles, se recomienda escribir <code>?vif</code> en la consola. En nuestro ejemplo tendríamos los siguientes resultados:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb93-1"><a href="rlm.html#cb93-1" aria-hidden="true" tabindex="-1"></a><span class="do">## cálculo del VIF</span></span>
<span id="cb93-2"><a href="rlm.html#cb93-2" aria-hidden="true" tabindex="-1"></a>car<span class="sc">:::</span><span class="fu">vif</span>(fit)</span></code></pre></div>
<pre><code>##       x1       x2 
## 1.001645 1.001645</code></pre>
<p>Puesto que en ninguna de las dos variables controlables el <span class="math inline">\(\text{VIF}&gt;5\)</span>, concluimos que <strong>no</strong> existe multicolinealidad.</p>
<ol start="3" style="list-style-type: decimal">
<li>Pruebas complementarias. En algunos casos, el ICN indica que <em>existe</em> multicolinealidad, pero para ninguno de los predictores el <span class="math inline">\(\text{VIF}&gt;5\)</span>. Cuando esto ocurre, lo mejor es utilizar pruebas complementaria, más robustas, que permitan decidir si efectivamente existe multicolinealidad en el modelo de RLM.</li>
</ol>
<p>Las pruebas complementarias pueden realizarse en <code>R</code> con la función <code>mctest</code> del paquete <code>mctest</code>. En nuestro ejemplo tendríamos los siguientes resultados:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb95-1"><a href="rlm.html#cb95-1" aria-hidden="true" tabindex="-1"></a><span class="do">## pruebas complementarias de multicolinealidad</span></span>
<span id="cb95-2"><a href="rlm.html#cb95-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(mctest)</span>
<span id="cb95-3"><a href="rlm.html#cb95-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mctest</span>(fit)</span></code></pre></div>
<pre><code>## 
## Call:
## omcdiag(mod = mod, Inter = TRUE, detr = detr, red = red, conf = conf, 
##     theil = theil, cn = cn)
## 
## 
## Overall Multicollinearity Diagnostics
## 
##                        MC Results detection
## Determinant |X&#39;X|:         0.9984         0
## Farrar Chi-Square:         0.1602         0
## Red Indicator:             0.0405         0
## Sum of Lambda Inverse:     2.0033         0
## Theil&#39;s Method:           -0.8683         0
## Condition Number:          8.2787         0
## 
## 1 --&gt; COLLINEARITY is detected by the test 
## 0 --&gt; COLLINEARITY is not detected by the test</code></pre>
<p>De acuerdo con estos resultados, podemos concluir que <strong>no existe</strong> multicolinealidad.</p>
</div>
</div>
<div id="selección-de-modelos" class="section level2" number="3.8">
<h2><span class="header-section-number">3.8</span> Selección de Modelos</h2>
<div id="método-de-todas-las-regresiones-posibles" class="section level3" number="3.8.1">
<h3><span class="header-section-number">3.8.1</span> Método de Todas las Regresiones Posibles</h3>
<p>El Método de Todas las Regresiones Posibles permite, a partir de un conjunto de variables independientes <span class="math inline">\(X_1, X_2, \ldots, X_k\)</span> que potencialmente podrían explicar una respuesta continua <span class="math inline">\(Y\)</span>, ajustar hasta <span class="math inline">\(2^{k}-1\)</span> modelos de regresión y seleccionar <em>el mejor</em> de estos utilizando algún criterio.</p>
<p>En la práctica, algunos de los criterios más utilizados incluyen <span class="math inline">\(R^2\)</span>, <span class="math inline">\(R^2_\text{adj}\)</span>, <span class="math inline">\(\sqrt{MSE}\)</span>, AIC, BIC, PRESS y</p>
<p><span class="math display">\[\begin{equation}
C_p = p+\frac{\text{SSE}_p}{\text{MSE}_\text{todos}} - (n-2p) = \begin{cases}
= p   &amp; \text{ para el modelo completo } \\
\approx p   &amp; \text{ el sesgo es pequeño $\rightarrow$  ideal!} \\
&gt; p   &amp; \text{ sesgo es alto }\\
&lt; p   &amp; \text{ no hay sesgo }\\
\end{cases}
\end{equation}\]</span></p>
<p>también conocido como el <a href="https://es.wikiqube.net/wiki/Mallows%27s_Cp">estadístico de Mallows</a>. En la expresión anterior, <span class="math inline">\(\text{MSE}_\text{todos}\)</span> es <span class="math inline">\(\hat{\sigma}^2\)</span> usando todas las covariables, y <span class="math inline">\(\text{SSE}_p\)</span> es el SSE del modelo con sólo <span class="math inline">\(p^\prime &lt; p\)</span> de ellas.</p>
<p>Como ejemplo, consideraremos los siguientes datos que corresponden al peso del producto terminado en gramos (variable <span class="math inline">\(y\)</span>) cuando se controlan los parámetros <span class="math inline">\(x_1, x_2, \ldots, x_{10}\)</span> de una inyectora:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb97-1"><a href="rlm.html#cb97-1" aria-hidden="true" tabindex="-1"></a><span class="do">## datos inyectora</span></span>
<span id="cb97-2"><a href="rlm.html#cb97-2" aria-hidden="true" tabindex="-1"></a>d <span class="ot">&lt;-</span> <span class="fu">read.table</span>(<span class="st">&quot;https://www.dropbox.com/s/a9gzu54luabtubo/inyectora.txt?dl=1&quot;</span>, </span>
<span id="cb97-3"><a href="rlm.html#cb97-3" aria-hidden="true" tabindex="-1"></a>                <span class="at">header =</span> <span class="cn">TRUE</span>)</span>
<span id="cb97-4"><a href="rlm.html#cb97-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb97-5"><a href="rlm.html#cb97-5" aria-hidden="true" tabindex="-1"></a><span class="do">## primeras 3 filas</span></span>
<span id="cb97-6"><a href="rlm.html#cb97-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(d, <span class="dv">3</span>)</span></code></pre></div>
<pre><code>##          y         x1          x2         x3         x4         x5         x6         x7
## 1 22.49879 -0.4248450  0.19997792 -0.5225479  0.5691505  0.9721086 -0.2927878 -0.5255406
## 2 17.60173  0.5766103 -0.33435292  0.9247179 -0.9811402 -0.7258651 -0.2671171  0.3729807
## 3 22.50649 -0.1820462 -0.02277393  0.2027315  0.5581318  0.8106192 -0.4257997 -0.5483632
##           x8          x9        x10
## 1  0.6898671 -0.05863633 0.84739843
## 2 -0.4797351 -0.26830905 0.08519674
## 3 -0.9537110 -0.75745589 0.70472920</code></pre>
<p>En total se tienen 100 unidades experimentales. Dado que en los datos sólo existen la variable respuesta y las covariables, podemos utilizar la siguiente sintaxis para ajustar el modelo de RLM:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb99-1"><a href="rlm.html#cb99-1" aria-hidden="true" tabindex="-1"></a><span class="do">## modelo de RLM ajustado</span></span>
<span id="cb99-2"><a href="rlm.html#cb99-2" aria-hidden="true" tabindex="-1"></a>fit_inyectora <span class="ot">&lt;-</span> <span class="fu">lm</span>(y <span class="sc">~</span> ., <span class="at">data =</span> d)</span>
<span id="cb99-3"><a href="rlm.html#cb99-3" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(fit_inyectora)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = y ~ ., data = d)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -2.09900 -0.61503 -0.04698  0.49843  2.35473 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 19.97501    0.09496 210.353  &lt; 2e-16 ***
## x1           3.04897    0.17692  17.234  &lt; 2e-16 ***
## x2           0.85418    0.18412   4.639 1.20e-05 ***
## x3           4.98623    0.17088  29.179  &lt; 2e-16 ***
## x4           4.97033    0.17161  28.963  &lt; 2e-16 ***
## x5           2.99030    0.17537  17.052  &lt; 2e-16 ***
## x6           4.03875    0.16979  23.786  &lt; 2e-16 ***
## x7           2.08153    0.16505  12.612  &lt; 2e-16 ***
## x8           0.89152    0.16997   5.245 1.05e-06 ***
## x9           2.90108    0.18030  16.090  &lt; 2e-16 ***
## x10          2.89888    0.17135  16.918  &lt; 2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.9418 on 89 degrees of freedom
## Multiple R-squared:  0.9778, Adjusted R-squared:  0.9753 
## F-statistic: 391.2 on 10 and 89 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>Como el número de variables <em>independientes</em> es <span class="math inline">\(k=10\)</span>, debemos estimar <span class="math inline">\(2^k-1= 1023\)</span> modelos diferentes al utilizar el Método de Todas las Regresiones Posibles. Para encontrar <em>el mejor</em> modelo, usamos la función <code>ols_step_all_possible</code> del paquete <code>olsrr</code>. Para más detalles, se recomienda escribir <code>?ols_step_all_possible</code> en la consola del <code>R</code>.</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb101-1"><a href="rlm.html#cb101-1" aria-hidden="true" tabindex="-1"></a><span class="do">## método de todas las regresiones posibles</span></span>
<span id="cb101-2"><a href="rlm.html#cb101-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(olsrr)</span>
<span id="cb101-3"><a href="rlm.html#cb101-3" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">ols_step_all_possible</span>(fit_inyectora)</span>
<span id="cb101-4"><a href="rlm.html#cb101-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(k, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##    Index N Predictors    R-Square Adj. R-Square Mallow&#39;s Cp
## 4      1 1         x4 0.381431483   0.375119559    2379.065
## 10     2 1        x10 0.221939980   0.214000593    3017.235
## 5      3 1         x5 0.150758161   0.142092428    3302.054
## 3      4 1         x3 0.122800863   0.113849851    3413.919
## 9      5 1         x9 0.065647890   0.056113685    3642.604
## 6      6 1         x6 0.061473423   0.051896621    3659.307
## 7      7 1         x7 0.049953135   0.040258779    3705.403
## 8      8 1         x8 0.037568650   0.027747921    3754.957
## 1      9 1         x1 0.005957422  -0.004185870    3881.442
## 2     10 1         x2 0.005662949  -0.004483347    3882.620</code></pre>
<p>En el objeto <code>k</code> se encuentran todos los resultados. La función <code>ols_step_all_possible</code> permite estimar 11 indicadores diferentes que facilitan la elección del <em>mejor</em> modelo. Por faciidad, sólo se muestran 3 de ellos.</p>
<p>Por ejemplo, si queremos seleccionar el <em>mejor</em> modelo utilizando el <span class="math inline">\(R^2_\text{adj}\)</span>, basta con escribir</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb103-1"><a href="rlm.html#cb103-1" aria-hidden="true" tabindex="-1"></a><span class="do">## mejor modelo basado en el R^2_{adj}</span></span>
<span id="cb103-2"><a href="rlm.html#cb103-2" aria-hidden="true" tabindex="-1"></a>k[<span class="fu">which.max</span>(k[,<span class="st">&#39;adjr&#39;</span>]), ]</span></code></pre></div>
<pre><code>##      Index  N                     Predictors  R-Square Adj. R-Square Mallow&#39;s Cp
## 1023  1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571     0.9752579          11</code></pre>
<p>Si usamos el <a href="https://es.wikipedia.org/wiki/Criterio_de_informaci%C3%B3n_de_Akaike">AIC</a>, el <em>mejor</em> modelo será:</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb105-1"><a href="rlm.html#cb105-1" aria-hidden="true" tabindex="-1"></a><span class="do">## mejor modelo basado en el Cp</span></span>
<span id="cb105-2"><a href="rlm.html#cb105-2" aria-hidden="true" tabindex="-1"></a>k[<span class="fu">which.min</span>(<span class="fu">with</span>(k, (cp<span class="sc">-</span>n)<span class="sc">^</span><span class="dv">2</span>)), ]</span></code></pre></div>
<pre><code>##      Index  N                     Predictors  R-Square Adj. R-Square Mallow&#39;s Cp
## 1023  1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571     0.9752579          11</code></pre>
<p>Otra posibilidad es utilizar <strong>dos criterios</strong> selección a la vez:</p>
<div class="sourceCode" id="cb107"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb107-1"><a href="rlm.html#cb107-1" aria-hidden="true" tabindex="-1"></a><span class="do">## selección usando el R^2_{adj} y el AIC</span></span>
<span id="cb107-2"><a href="rlm.html#cb107-2" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(ggplot2)</span>
<span id="cb107-3"><a href="rlm.html#cb107-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(k, <span class="fu">aes</span>(<span class="at">x =</span> adjr, <span class="at">y =</span> aic)) <span class="sc">+</span> <span class="fu">geom_point</span>() <span class="sc">+</span> <span class="fu">theme_minimal</span>() <span class="sc">+</span> <span class="fu">xlab</span>(<span class="fu">expression</span>(R<span class="sc">^</span><span class="dv">2</span> <span class="sc">*</span></span>
<span id="cb107-4"><a href="rlm.html#cb107-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">&quot; ajustado&quot;</span>)) <span class="sc">+</span> <span class="fu">ylab</span>(<span class="st">&quot;AIC&quot;</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-102"></span>
<img src="book-adii_files/figure-html/unnamed-chunk-102-1.png" alt="AIC vs. $R^2$ ajustado en todas las regresiones posibles." width="480" />
<p class="caption">
Figura 3.7: AIC vs. <span class="math inline">\(R^2\)</span> ajustado en todas las regresiones posibles.
</p>
</div>
<p>De acuerdo con la definición de cada criterio, el <em>mejor</em> modelo debe tener un <span class="math inline">\(R^2_{\text{adj}}\rightarrow1\)</span> y <span class="math inline">\(\text{AIC}\rightarrow 0\)</span>. De la gráfica, es posible observar que hay cuatro modelos que sobresalen:</p>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb108-1"><a href="rlm.html#cb108-1" aria-hidden="true" tabindex="-1"></a><span class="do">## selección de modelos competitivos</span></span>
<span id="cb108-2"><a href="rlm.html#cb108-2" aria-hidden="true" tabindex="-1"></a>k <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(k)</span>
<span id="cb108-3"><a href="rlm.html#cb108-3" aria-hidden="true" tabindex="-1"></a><span class="fu">subset</span>(k, aic <span class="sc">&lt;</span> <span class="dv">350</span> <span class="sc">&amp;</span> adjr <span class="sc">&gt;</span> <span class="fl">0.8</span>, <span class="at">select =</span> n<span class="sc">:</span>aic)</span></code></pre></div>
<pre><code>##       n                     predictors   rsquare      adjr   predrsq       cp      aic
## 998   8       x1 x3 x4 x5 x6 x7 x9 x10 0.9650304 0.9619562 0.9577212 57.92307 325.3860
## 1021  9    x1 x3 x4 x5 x6 x7 x8 x9 x10 0.9723781 0.9696159 0.9653333 30.52284 303.7989
## 1015  9    x1 x2 x3 x4 x5 x6 x7 x9 x10 0.9708818 0.9679700 0.9640209 36.51001 309.0744
## 1023 10 x1 x2 x3 x4 x5 x6 x7 x8 x9 x10 0.9777571 0.9752579 0.9715036 11.00000 284.1403</code></pre>
<p>Por ejemplo, el modelo <code>968</code> no incluye <code>x2</code> ni <code>x8</code> y tiene un <span class="math inline">\(R^2_{\text{adj}}=0.9619\)</span>, mientras el modelo <code>1023</code> tiene el <span class="math inline">\(R^2_{\text{adj}}\)</span> más alto e incluye 10 variables predictoras. A pesar de que este modelo es <em>levemente</em> mejor el <code>968</code>, la ganancia en <span class="math inline">\(R^2_{\text{adj}}\)</span> es ínfima y podría no justificar la medición en proceso de dos factores controlables más. En términos del AIC, se tiene que los modelos <code>1021</code> y <code>1015</code> son similares.</p>
</div>
<div id="selección-secuencial" class="section level3" number="3.8.2">
<h3><span class="header-section-number">3.8.2</span> Selección secuencial</h3>
<p>Need to add some introductory remarks</p>
<div id="método-stepwise" class="section level4 unnumbered">
<h4>Método <em>stepwise</em></h4>
<p>To be completed</p>
</div>
<div id="método-forward" class="section level4 unnumbered">
<h4>Método <em>forward</em></h4>
<p>To be completed</p>
</div>
<div id="método-backward" class="section level4 unnumbered">
<h4>Método <em>backward</em></h4>
<p>To be completed</p>

</div>
</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="rls.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="glm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/jivelez/book-adii/edit/master/03-rlm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/jivelez/book-adii/blob/master/03-rlm.Rmd",
"text": null
},
"download": ["book-adii.pdf", "book-adii.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
